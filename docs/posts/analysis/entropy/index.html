<!doctype html><html lang=zh-cn><head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#"><meta charset=utf-8><meta name=generator content="Hugo 0.89.4"><meta name=theme-color content="#fff"><meta name=color-scheme content="light dark"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no, date=no, address=no, email=no"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><title>熵与信息 | 主页</title><link rel=stylesheet href=../../../css/meme.min.a5562c0d2764ee14eaa5f7ccddf34ddd6133b651c33a45e417a84ba7b441a17f.css><script src=../../../js/meme.min.4c8facfc8134c52bd7bf6bbfa3a7e68b06b47ae04968222e28e7831f5b1a7592.js></script>
<link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&family=Noto+Serif+SC:wght@400;500;700&family=Source+Code+Pro:ital,wght@0,400;0,700;1,400;1,700&display=swap" media=print onload="this.media='all'"><noscript><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&family=Noto+Serif+SC:wght@400;500;700&family=Source+Code+Pro:ital,wght@0,400;0,700;1,400;1,700&display=swap"></noscript><meta name=author content><meta name=description content="熵 熵，在统计学或信息论里，我们称其为信息熵；在物理学领域，一般指热力学熵。信息熵和热……"><link rel="shortcut icon" href=../../../favicon.ico type=image/x-icon><link rel=mask-icon href=../../../icons/safari-pinned-tab.svg color=#2a6df4><link rel=apple-touch-icon sizes=180x180 href=../../../icons/apple-touch-icon.png><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-title content="主页"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=mobile-web-app-capable content="yes"><meta name=application-name content="主页"><meta name=msapplication-starturl content="../../../"><meta name=msapplication-TileColor content="#fff"><meta name=msapplication-TileImage content="../../../icons/mstile-150x150.png"><link rel=manifest href=../../../manifest.json><link rel=canonical href=https://allenz-me.github.io/posts/analysis/entropy/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","datePublished":"2021-02-07T00:00:00+00:00","dateModified":"2022-07-05T20:26:54+08:00","url":"https://allenz-me.github.io/posts/analysis/entropy/","headline":"熵与信息","description":"熵 熵，在统计学或信息论里，我们称其为信息熵；在物理学领域，一般指热力学熵。信息熵和热……","inLanguage":"zh-CN","articleSection":"posts","wordCount":3141,"image":"https://allenz-me.github.io/icons/apple-touch-icon.png","author":{"@type":"Person","description":"找不到工作QAQ","email":"allenz.me@qq.com","image":"https://allenz-me.github.io/icons/apple-touch-icon.png","url":"https://io-oi.me/"},"license":"[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)","publisher":{"@type":"Organization","name":"主页","logo":{"@type":"ImageObject","url":"https://allenz-me.github.io/icons/apple-touch-icon.png"},"url":"https://allenz-me.github.io/"},"mainEntityOfPage":{"@type":"WebSite","@id":"https://allenz-me.github.io/"}}</script><meta property="og:title" content="熵与信息"><meta property="og:description" content="熵 熵，在统计学或信息论里，我们称其为信息熵；在物理学领域，一般指热力学熵。信息熵和热……"><meta property="og:url" content="https://allenz-me.github.io/posts/analysis/entropy/"><meta property="og:site_name" content="主页"><meta property="og:locale" content="zh"><meta property="og:image" content="https://allenz-me.github.io/icons/apple-touch-icon.png"><meta property="og:type" content="article"><meta property="article:published_time" content="2021-02-07T00:00:00+00:00"><meta property="article:modified_time" content="2022-07-05T20:26:54+08:00"><meta property="article:section" content="posts"></head><body><div class=container><header class=header><div class=header-wrapper><div class="header-inner single"><div class=site-brand><a href=../../../ class=brand>主页</a></div><nav class=nav><ul class=menu id=menu><li class="menu-item active"><a href=../../../posts/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon archive"><path d="M32 448c0 17.7 14.3 32 32 32h384c17.7.0 32-14.3 32-32V160H32v288zm160-212c0-6.6 5.4-12 12-12h104c6.6.0 12 5.4 12 12v8c0 6.6-5.4 12-12 12H204c-6.6.0-12-5.4-12-12v-8zM480 32H32C14.3 32 0 46.3.0 64v48c0 8.8 7.2 16 16 16h480c8.8.0 16-7.2 16-16V64c0-17.7-14.3-32-32-32z"/></svg><span class=menu-item-name>文章</span></a></li><li class=menu-item><a href=../../../categories/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon th"><path d="M149.333 56v80c0 13.255-10.745 24-24 24H24c-13.255.0-24-10.745-24-24V56c0-13.255 10.745-24 24-24h101.333c13.255.0 24 10.745 24 24zm181.334 240v-80c0-13.255-10.745-24-24-24H205.333c-13.255.0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256.0 24.001-10.745 24.001-24zm32-240v80c0 13.255 10.745 24 24 24H488c13.255.0 24-10.745 24-24V56c0-13.255-10.745-24-24-24H386.667c-13.255.0-24 10.745-24 24zm-32 80V56c0-13.255-10.745-24-24-24H205.333c-13.255.0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256.0 24.001-10.745 24.001-24zm-205.334 56H24c-13.255.0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.255.0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24zM0 376v80c0 13.255 10.745 24 24 24h101.333c13.255.0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H24c-13.255.0-24 10.745-24 24zm386.667-56H488c13.255.0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255.0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zm0 160H488c13.255.0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255.0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zM181.333 376v80c0 13.255 10.745 24 24 24h101.333c13.255.0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H205.333c-13.255.0-24 10.745-24 24z"/></svg><span class=menu-item-name>分类</span></a></li><li class=menu-item><a href=../../../tags/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="icon tags"><path d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a48 48 0 0014.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM112 160c-26.51.0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882.0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397.0h48.721a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882z"/></svg><span class=menu-item-name>标签</span></a></li><li class=menu-item><a href><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon user-circle"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6.0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7.0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4.0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9s28-2.7 40.9-6.9c2.3-.7 4.7-1.1 7.1-1.1 42.9.0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><span class=menu-item-name>关于</span></a></li><li class=menu-item><a id=theme-switcher href=#><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5 242 7a18 18 0 0128 0l48.8 97.5L422.2 70A18 18 0 01442 89.8l-34.5 103.4L505 242a18 18 0 010 28l-97.5 48.8L442 422.2A18 18 0 01422.2 442l-103.4-34.5L270 505a18 18 0 01-28 0l-48.8-97.5L89.8 442A18 18 0 0170 422.2l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8L70 89.8A18 18 0 0189.8 70zM256 128a128 128 0 10.01.0M256 160a96 96 0 10.01.0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412A256 256 0 10181 5a11.5 11.5.0 00-5 20A201.5 201.5.0 0142 399a11.5 11.5.0 00-15 13"/></svg></a></li></ul></nav></div></div><input type=checkbox id=nav-toggle aria-hidden=true>
<label for=nav-toggle class=nav-toggle></label>
<label for=nav-toggle class=nav-curtain></label></header><main class="main single" id=main><div class=main-inner><article class="content post h-entry" data-align=default data-type=posts data-toc-num=true><h1 class="post-title p-name">熵与信息</h1><div class=post-meta><time datetime=2021-02-07T00:00:00+00:00 class="post-meta-item published dt-published"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M148 288h-40c-6.6.0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6.0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6.0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5.0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6.0 12 5.4 12 12v52h48c26.5.0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3.0 6-2.7 6-6z"/></svg>&nbsp;2021/2/7</time>
<span class="post-meta-item category"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>&nbsp;<a href=../../../categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/ class="category-link p-category">分析与概率</a></span>
<span class="post-meta-item wordcount"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3.0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9.0l60.1 60.1c18.8 18.7 18.8 49.1.0 67.9zM284.2 99.8 21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3.0-17l-111-111c-4.8-4.7-12.4-4.7-17.1.0zM124.1 339.9c-5.5-5.5-5.5-14.3.0-19.8l154-154c5.5-5.5 14.3-5.5 19.8.0s5.5 14.3.0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8.0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;3141</span></div><nav class=contents><h2 id=contents class=contents-title>目录</h2><ol class=toc><li><a id=contents:熵 href=#熵>熵</a></li><li><a id=contents:事件的自信息 href=#事件的自信息>事件的自信息</a></li><li><a id=contents:信息熵information-entropy href=#信息熵information-entropy>信息熵（information entropy）</a></li><li><a id=contents:联合熵joint-entropy href=#联合熵joint-entropy>联合熵（joint entropy）</a></li><li><a id=contents:条件熵conditional-entropy href=#条件熵conditional-entropy>条件熵（conditional entropy）</a></li><li><a id=contents:互信息mutual-information href=#互信息mutual-information>互信息（mutual information）</a></li><li><a id=contents:信息散度information-divergence href=#信息散度information-divergence>信息散度（information divergence）</a></li><li><a id=contents:交叉熵cross-entropy href=#交叉熵cross-entropy>交叉熵（cross entropy）</a></li><li><a id=contents:微分熵 href=#微分熵>微分熵</a></li><li><a id=contents:熵的优化 href=#熵的优化>熵的优化</a></li><li><a id=contents:fano-不等式 href=#fano-不等式>Fano 不等式</a></li></ol></nav><div class="post-body e-content"><h2 id=熵><a href=#熵 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:熵 class=headings>熵</a></h2><p>熵，在统计学或信息论里，我们称其为信息熵；在物理学领域，一般指热力学熵。信息熵和热力学熵是统一的。熵是体系混乱度的度量，熵越大，说明这个体系越混乱。</p><p>在统计学和信息论中，熵被用来度量不确定性，熵的意义是信息量的多少。对于一个随机试验（不同事件的可能的概率为$p_1,p_2, ..., p_n$），希望找到一个函数 $H$ 来度量这个随机试验的不确定性。香农认为，熵 $H$ 需要满足三个条件：</p><ol><li>是 $p_i$ 的连续函数。</li><li>对于 $n$ 个等概率结果的试验，$H$ 是 $n$ 的单调上升函数。</li><li>一个试验分成两个相继试验，未分之前的 $H$ 是既分之后的 $H$ 的加权和。（可加性）</li></ol><p>可以证明满足以上三个条件的 $H$ 具有下列形式：</p><p>$$
H=-C\sum^n_{i=1}p_i\log p_i
$$</p><p>其中 $C$ 是正的常数。</p><h2 id=事件的自信息><a href=#事件的自信息 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:事件的自信息 class=headings>事件的自信息</a></h2><p>自信息表示某一事件发生时所带来的信息量的多少，当事件发生的概率越大，则自信息越小。七月飘雪这一事件所包含的信息要高于腊月下雪。</p><p>一个随机事件的自信息等于其概率的负对数（$-\log p$）。自信息满足可加性，即两个独立事件的自信息等于两个事件单独的自信息。</p><h2 id=信息熵information-entropy><a href=#信息熵information-entropy class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:信息熵information-entropy class=headings>信息熵（information entropy）</a></h2><p>自信息一般用来描述一个随机事件的信息量大小。<strong>信息熵则是用来描述整个随机分布所带来的信息量平均值。</strong> 离散型随机变量 $X$ 取不同值的可能概率为 $p_1,p_2, ..., p_n$，那么 $X$ 的信息熵为：</p><p>$$
H(X)=-\sum_{x \in \mathcal{X}} p(x) \log {p(x)}= - \sum_{i=1}^n p_i \log p_i
$$</p><p>信息熵只与随机变量取值的概率有关，而与它具体的取值无关。而诸如随机变量的方差、变异系数等，是与随机变量的具体取值有关的。</p><p>容易看出，$H(X)$ 在 $p_1=p_2=...=p_n=\frac{1}{n}$ 时取最大值，且其最大值等于 $\log |\mathcal{X}|$，其中$|\mathcal{X}|=n$ 是 $X$ 取值的数量。</p><p>随机变量的信息熵的数值，在信息论中，实际反映的是平均的二进制编码长度，也反映了信息的极限压缩程度。
比如说，如果 $X$ 有 $4$ 种可能的取值，我们就可以用 00、01、10、11 去分别表示；如果 $X$ 有 $3$ 种可能的取值，我们就可以用 00、01、1 去分别表示，我们会发现，后者的平均编码长度更短。</p><h2 id=联合熵joint-entropy><a href=#联合熵joint-entropy class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:联合熵joint-entropy class=headings>联合熵（joint entropy）</a></h2><p>离散型随机变量 $X, Y$ 的联合熵定义如下：</p><p>$$
H(X, Y)=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y)
$$</p><p>它反映了描述一对随机变量平均所需要的信息量。以上定义容易推广到多个随机变量的情况。事实上，这也是随机向量的熵的定义。</p><p>联合信息量小于等于独立观察信息量之和：</p><p>$$
H(X, Y) \leqslant H(X) + H(Y)
$$</p><h2 id=条件熵conditional-entropy><a href=#条件熵conditional-entropy class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:条件熵conditional-entropy class=headings>条件熵（conditional entropy）</a></h2><p>若 $(X, Y) \sim p(x, y)$，则条件熵 $H(Y\mid X)$ 定义为：</p><p>$$
\begin{aligned}
H(Y \mid X) &=\sum_{x \in \mathcal{X}} p(x) H(Y \mid X=x) \\
&=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(y|x) \\
\end{aligned}
$$</p><p>可以认为是已知随机变量 $X$ 后，$Y$ 的信息量，所以有：$H(Y \mid X) \leqslant H(Y)$。当我们知道了统计相关性的变量之后，就可以减少不确定性。</p><blockquote><p>并不意味着 $H(Y \mid X=x) \leqslant H(Y)$。</p></blockquote><p>如果 $X, Y$ 统计独立，那么</p><p>$$
H(X\mid Y)=H(X), \quad H(Y\mid X) = H(Y)
$$</p><p>这时候知道 $X$ 不会给 $Y$ 提供任何信息，知道 $Y$ 也不会给 $X$ 提供任何信息。</p><p>$(X, Y)$ 的联合熵可以看作是两个连续实验得到的信息：先观察 $X$，在已知 $X$ 的基础上观察 $Y$。于是我们能够得到如下等式：</p><p>$$
H(X, Y)=H(X) + H(Y \mid X)
$$</p><p>由此我们可以得到：</p><p>$$
H(X)+H(Y) - H(X, Y) = H(Y)-H(Y\mid X) = H(X) - H(X\mid Y)
$$</p><p>从而，联合观察 $X, Y$ 的信息量，比起独立观察 $X, Y$ 的信息量之和，少的那一部分，就是 $X, Y$ 之间相关关系带来的信息损失。应用在随机事件上，容易理解，两个相关性很强的小概率事件同时发生的信息量，要少于某个小概率时间发生信息量的两倍。</p><blockquote><p>类似地，不难解释下面的等式：
$$
H(X, Y \mid Z) = H(X \mid Z) + H(Y \mid X, Z)
$$</p></blockquote><h2 id=互信息mutual-information><a href=#互信息mutual-information class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:互信息mutual-information class=headings>互信息（mutual information）</a></h2><p>定义离散型随机变量 $X$ 与 $Y$ 之间的互信息为：</p><p>$$
\begin{aligned}
I(X, Y)=& H(X)-H(X \mid Y)=H(Y)-H(Y \mid X) \\
=&H(X)+H(Y)-H(X, Y) \\
=& H(X, Y)-H(X \mid Y)-H(Y \mid X)
\end{aligned}
$$</p><p>也可以直接定义互信息：</p><p>$$
I(X, Y) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}
$$</p><p>互信息满足对称性。它表示，在知道 $Y$ 之后，$X$ 的信息量减少了多少；<strong>换言之，$Y$ 能为 $X$ 提供多少信息量</strong>。或者说，$X, Y$ 相关关系所蕴含的信息量。</p><p>易知，若 $X, Y$ 统计独立，那么 $I(X, Y)=0$。$X, Y$ 不能为对方提供任何信息。</p><blockquote><p>决策树ID3算法的训练规则：$g(D, A)=H(D)-H(D \mid A)$，本质上就是找到一个与类别的互信息最大的特征，这个特征与类别的相关关系包含了最多的信息。</p></blockquote><h2 id=信息散度information-divergence><a href=#信息散度information-divergence class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:信息散度information-divergence class=headings>信息散度（information divergence）</a></h2><p>信息散度被用来衡量两个分布之间的相似度，最常用的、也是与香农体系相容的，是KL散度（<em>Kullback-Leibler divergence</em>），也叫相对熵（<em>relative entropy</em>）、鉴别信息（<em>information for discrimination</em>）。KL散度用于刻画使用理论分布$q(x)$拟合真实分布$p(x)$时产生的信息损失：</p><p>$$
D_{KL}(p \lVert q)=\sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}
$$</p><p><strong>KL散度具有非负性。</strong> 两个分布越接近，那么它们的KL散度值越小。如果 $p=q$，那么 $D_{KL}(p \lVert q)=0$。值得注意的是，尽管KL散度能够衡量两个分布之间的相似度，但它并不是一个分布之间<em>距离</em>，<strong>KL散度不满足距离公理中的对称性和三角不等式</strong>。</p><p>离散型随机变量的熵与KL散度之间存在关系：</p><p>$$
H(X)=\log |\mathcal{X}| - D(p \lVert u)
$$</p><p>其中 $\mathcal{X}$ 是 $X$ 的值域集，$u$ 是 $\mathcal{X}$ 上的等概均匀分布，并且我们知道，等概分布 $u$ 的信息熵就是 $\log|\mathcal{X}|$，这个式子背后的意义是好理解的。</p><p>互信息与KL散度满足关系：</p><p>$$
I(X, Y) = D(p(x, y) \lVert p(x)p(y))
$$</p><p>它表示，假定 $X, Y$ 独立，这种假定与真实情况的差别有多大。</p><h2 id=交叉熵cross-entropy><a href=#交叉熵cross-entropy class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:交叉熵cross-entropy class=headings>交叉熵（cross entropy）</a></h2><p>交叉熵表示，如果用错误的编码方式 $q(x)$ 去编码真实分布 $p(x)$ 所需要的平均编码长度：</p><p>$$
H(p, q) = - \sum_{x \in \mathcal{X}} p(x) \log q(x)
$$</p><p>交叉熵常用作机器学习的损失函数，它也能够衡量两个分布的相似程度，交叉熵越小，分布越相似。</p><blockquote><p>按照我的理解，每个样本都是独立的随机变量，$X_1, X_2, ... X_n = 0 \text{ or } 1$ 是样本标签，$Y_1, Y_2, ... Y_n$ 是概率模型的输出，此时 $P(Y_i=1)=1-P(Y_i=0)=y_{pred_i}$，交叉熵损失函数就是 $\sum_{i=1}^n H(X_i, Y_i)$。</p></blockquote><p>交叉熵与KL散度之间存在关系：</p><p>$$
D_{KL}(p\lVert q) = H(p, q) - H(p)
$$</p><p>因此寻找最优的拟合分布 $q$ 既是优化交叉熵也是优化信息散度。由此我们还得到了信息散度的另一种解释：用非真实分布 $q$ 得到的平均码长比真实分布 $p$ 得到的平均码长多出的比特数。</p><h2 id=微分熵><a href=#微分熵 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:微分熵 class=headings>微分熵</a></h2><p>对于连续型随机变量，我们总能找到一列离散型随机变量去无限地逼近它，然后连续型随机变量的信息熵，就定义为这列离散型随机变量的极限：</p><p>$$
H(X) = - \int_{-\infty}^{+\infty}p(x) \log p(x) \mathrm{d} x
$$</p><blockquote><p>事实上，连续型随机变量所具有的信息是无穷大的，以上仅是一种形式上的定义。</p></blockquote><p>由此我们还能够类似地定义联合熵：</p><p>$$
H(X, Y) = - \iint_{\mathbf{R}^2} p(x, y) \log p(x, y) \mathrm{d} x \mathrm{d} y
$$</p><p>条件熵：</p><p>$$
H(X\mid Y) = -\iint _{\mathbf{R}^2}p(x, y)\log p(x| y) \mathrm{d} x \mathrm{d} y
$$</p><blockquote><p><strong>注意：微分熵可能为负值。</strong></p></blockquote><p>$$
\begin{array}{l}
X: p(x)=\left\{\begin{array}{cc}
\frac{1}{b-a} & a \leq x \leq b, & b-a&lt;1 \\
0 & x>b, & x&lt;a
\end{array}\right. \\
H(X)=-\int_{a}^{b} \frac{1}{b-a} \log \frac{1}{b-a} d x=\log (b-a)&lt;0
\end{array}
$$</p><p>对于正态分布 $\mathcal{N}(\mu, \sigma^2)$ 的随机变量 $X$，其微分熵为：$H(X)=\frac{1}{2} \log 2\pi e \sigma^2$，与均值无关。</p><p>若连续型随机变量的均值和方差均被给定，则当其服从正态分布的时候，微分熵最大！（这一点与热力学相同）</p><h2 id=熵的优化><a href=#熵的优化 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:熵的优化 class=headings>熵的优化</a></h2><p>信息熵关于 $p$ 是凹函数。因此最大化信息熵是一个凸的问题。</p><p>交叉熵 $H(p, q)$ 关于 $q$ 是凸的，因此给定分布 $p$，寻找 $q$ 最小化交叉熵是凸的问题。</p><p>KL散度 $D_{KL}(p\lVert q)=\sum_{i=1}^n p_i \log \frac{p_i}{q_i}$ 对 $(p, q)$ 是凸的。因此最小化与一个给定分布的KL散度这一问题是凸的。</p><h2 id=fano-不等式><a href=#fano-不等式 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:fano-不等式 class=headings>Fano 不等式</a></h2><p>假设随机变量 $X$ 和 $Y$ 分别是输入、输出信息，我们无法直接观测到 $X$，只能观察到与 $X$ 相关的随机变量 $Y$，根据随机变量 $Y$ 我们可以做出对原随机变量 $X$ 的一个估计 $\hat X=f(Y)$。</p><p>$$
X\rightarrow Y \rightarrow \hat X
$$</p><p>对于任意的估计 $\hat X$，定义错误概率 $P_e=P(\hat X \neq X)$，此时Fano不等式成立：</p><p>$$
H\left(P_{e}\right)+P_{e} \log |\mathrm{X}| \geqslant H(X \mid \hat{X}) \geqslant H(X \mid Y)
$$</p><p>Fano不等式给出了解码器错误概率的下界。</p><hr><p><strong>总结：本文阐述了三个重要的概念：信息熵、互信息、信息散度（相对熵）。信息熵衡量一个随机变量所具有的信息量，互信息衡量两个随机变量的相关关系所蕴含的信息，信息散度衡量两个分布之间的差异程度。</strong></p><p>参考资料</p><ul><li><a href="https://www.bilibili.com/video/BV1Fs411g7G7?seid=10007690006652926935" target=_blank rel=noopener>https://www.bilibili.com/video/BV1Fs411g7G7?seid=10007690006652926935</a></li><li><a href=https://blog.csdn.net/pipisorry/article/details/51695283 target=_blank rel=noopener>https://blog.csdn.net/pipisorry/article/details/51695283</a></li><li><a href=https://www.jianshu.com/p/31a683cddb94 target=_blank rel=noopener>机器学习中的熵、条件熵、相对熵(KL散度)和交叉熵</a></li><li><a href=https://en.wikipedia.org/wiki/Fano%27s_inequality target=_blank rel=noopener>https://en.wikipedia.org/wiki/Fano%27s_inequality</a></li></ul></div></article><div class=updated-badge-container><span title="Updated @ 2022-07-05 20:26:54 CST" style=cursor:help><svg xmlns="http://www.w3.org/2000/svg" width="130" height="20" class="updated-badge"><linearGradient id="b" x2="0" y2="100%"><stop offset="0" stop-color="#bbb" stop-opacity=".1"/><stop offset="1" stop-opacity=".1"/></linearGradient><clipPath id="a"><rect width="130" height="20" rx="3" fill="#fff"/></clipPath><g clip-path="url(#a)"><path class="updated-badge-left" d="M0 0h55v20H0z"/><path class="updated-badge-right" d="M55 0h75v20H55z"/><path fill="url(#b)" d="M0 0h130v20H0z"/></g><g fill="#fff" text-anchor="middle" font-size="110"><text x="285" y="150" fill="#010101" fill-opacity=".3" textLength="450" transform="scale(.1)">updated</text><text x="285" y="140" textLength="450" transform="scale(.1)">updated</text><text x="915" y="150" fill="#010101" fill-opacity=".3" textLength="650" transform="scale(.1)">2022-07-05</text><text x="915" y="140" textLength="650" transform="scale(.1)">2022-07-05</text></g></svg></span></div><div class=post-share><div class=share-items><div class="share-item weibo"><a href="https://service.weibo.com/share/share.php?&url=https://allenz-me.github.io/posts/analysis/entropy/&title=%e7%86%b5%e4%b8%8e%e4%bf%a1%e6%81%af&pic=https://allenz-me.github.io/icons/apple-touch-icon.png&searchPic=false" title=分享到「新浪微博」 target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon weibo-icon"><path d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7.0 395.3.0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"/></svg></a></div><div class="share-item douban"><a href="https://www.douban.com/share/service?href=https://allenz-me.github.io/posts/analysis/entropy/&name=%e7%86%b5%e4%b8%8e%e4%bf%a1%e6%81%af&text=%e7%86%b5%20%e7%86%b5%ef%bc%8c%e5%9c%a8%e7%bb%9f%e8%ae%a1%e5%ad%a6%e6%88%96%e4%bf%a1%e6%81%af%e8%ae%ba%e9%87%8c%ef%bc%8c%e6%88%91%e4%bb%ac%e7%a7%b0%e5%85%b6%e4%b8%ba%e4%bf%a1%e6%81%af%e7%86%b5%ef%bc%9b%e5%9c%a8%e7%89%a9%e7%90%86%e5%ad%a6%e9%a2%86%e5%9f%9f%ef%bc%8c%e4%b8%80%e8%88%ac%e6%8c%87%e7%83%ad%e5%8a%9b%e5%ad%a6%e7%86%b5%e3%80%82%e4%bf%a1%e6%81%af%e7%86%b5%e5%92%8c%e7%83%ad%e2%80%a6%e2%80%a6" title=分享到「豆瓣」 target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon douban-icon"><path d="M.643.92v2.412h22.714V.92H.643zm1.974 4.926v9.42h18.764v-9.42H2.617zm2.72 2.408H18.69v4.605H5.338V8.254zm1.657 7.412-2.512.938c1.037 1.461 1.87 2.825 2.512 4.091H0v2.385h24v-2.385h-6.678c.818-1.176 1.589-2.543 2.303-4.091l-2.73-.938a29.952 29.952.0 01-2.479 5.03h-4.75c-.786-1.962-1.677-3.641-2.672-5.03z"/></svg></a></div><div class="share-item qq"><a href="https://connect.qq.com/widget/shareqq/index.html?url=https://allenz-me.github.io/posts/analysis/entropy/&title=%e7%86%b5%e4%b8%8e%e4%bf%a1%e6%81%af&summary=%e7%86%b5%20%e7%86%b5%ef%bc%8c%e5%9c%a8%e7%bb%9f%e8%ae%a1%e5%ad%a6%e6%88%96%e4%bf%a1%e6%81%af%e8%ae%ba%e9%87%8c%ef%bc%8c%e6%88%91%e4%bb%ac%e7%a7%b0%e5%85%b6%e4%b8%ba%e4%bf%a1%e6%81%af%e7%86%b5%ef%bc%9b%e5%9c%a8%e7%89%a9%e7%90%86%e5%ad%a6%e9%a2%86%e5%9f%9f%ef%bc%8c%e4%b8%80%e8%88%ac%e6%8c%87%e7%83%ad%e5%8a%9b%e5%ad%a6%e7%86%b5%e3%80%82%e4%bf%a1%e6%81%af%e7%86%b5%e5%92%8c%e7%83%ad%e2%80%a6%e2%80%a6&pics=https://allenz-me.github.io/icons/apple-touch-icon.png&site=%e4%b8%bb%e9%a1%b5" title=分享到「QQ」 target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon qq-icon"><path d="M433.754 420.445c-11.526 1.393-44.86-52.741-44.86-52.741.0 31.345-16.136 72.247-51.051 101.786 16.842 5.192 54.843 19.167 45.803 34.421-7.316 12.343-125.51 7.881-159.632 4.037-34.122 3.844-152.316 8.306-159.632-4.037-9.045-15.25 28.918-29.214 45.783-34.415-34.92-29.539-51.059-70.445-51.059-101.792.0.0-33.334 54.134-44.859 52.741-5.37-.65-12.424-29.644 9.347-99.704 10.261-33.024 21.995-60.478 40.144-105.779C60.683 98.063 108.982.006 224 0c113.737.006 163.156 96.133 160.264 214.963 18.118 45.223 29.912 72.85 40.144 105.778 21.768 70.06 14.716 99.053 9.346 99.704z"/></svg></a></div><div class="share-item qzone"><a href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://allenz-me.github.io/posts/analysis/entropy/&title=%e7%86%b5%e4%b8%8e%e4%bf%a1%e6%81%af&summary=%e7%86%b5%20%e7%86%b5%ef%bc%8c%e5%9c%a8%e7%bb%9f%e8%ae%a1%e5%ad%a6%e6%88%96%e4%bf%a1%e6%81%af%e8%ae%ba%e9%87%8c%ef%bc%8c%e6%88%91%e4%bb%ac%e7%a7%b0%e5%85%b6%e4%b8%ba%e4%bf%a1%e6%81%af%e7%86%b5%ef%bc%9b%e5%9c%a8%e7%89%a9%e7%90%86%e5%ad%a6%e9%a2%86%e5%9f%9f%ef%bc%8c%e4%b8%80%e8%88%ac%e6%8c%87%e7%83%ad%e5%8a%9b%e5%ad%a6%e7%86%b5%e3%80%82%e4%bf%a1%e6%81%af%e7%86%b5%e5%92%8c%e7%83%ad%e2%80%a6%e2%80%a6&pics=https://allenz-me.github.io/icons/apple-touch-icon.png&site=%e4%b8%bb%e9%a1%b5" title="分享到「QQ 空间」" target=_blank rel=noopener><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon qzone-icon"><path d="M23.985 9.202c-.032-.099-.127-.223-.334-.258-.207-.036-7.351-1.406-7.351-1.406s-.105-.022-.198-.07c-.092-.047-.127-.167-.127-.167S12.447.956 12.349.77C12.25.583 12.104.532 12 .532s-.251.051-.349.238c-.098.186-3.626 6.531-3.626 6.531s-.035.12-.128.167c-.092.047-.197.07-.197.07S.556 8.908.348 8.943c-.208.036-.302.16-.333.258a.477.477.0 00.125.449l5.362 5.49s.072.08.119.172c.016.104.005.21.005.21s-1.189 7.242-1.22 7.45.075.369.159.43c.083.062.233.106.421.013.189-.093 6.812-3.261 6.812-3.261s.098-.044.201-.061.201.061.201.061 6.623 3.168 6.812 3.261c.188.094.338.049.421-.013a.463.463.0 00.159-.43c-.021-.14-.93-5.677-.93-5.677.876-.54 1.425-1.039 1.849-1.747-2.594.969-6.006 1.717-9.415 1.866-.915.041-2.41.097-3.473-.015-.678-.071-1.17-.144-1.243-.438-.053-.215.054-.46.545-.831a2640.5 2640.5.0 012.861-2.155c1.285-.968 3.559-2.47 3.559-2.731.0-.285-2.144-.781-4.037-.781-1.945.0-2.275.132-2.811.168-.488.034-.769.005-.804-.138-.06-.248.183-.389.588-.568.709-.314 1.86-.594 1.984-.626.194-.052 3.082-.805 5.618-.535 1.318.14 3.244.668 3.244 1.276.0.342-1.721 1.494-3.225 2.597-1.149.843-2.217 1.561-2.217 1.688.0.342 3.533 1.241 6.689 1.01l.003-.022c.048-.092.119-.172.119-.172l5.362-5.49a.477.477.0 00.127-.449z"/></svg></a></div><div class="share-item qrcode"><div class=qrcode-container title=通过「二维码」><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon qrcode-icon"><path d="M0 224h192V32H0v192zM64 96h64v64H64V96zm192-64v192h192V32H256zm128 128h-64V96h64v64zM0 480h192V288H0v192zm64-128h64v64H64v-64zm352-64h32v128h-96v-32h-32v96h-64V288h96v32h64v-32zm0 160h32v32h-32v-32zm-64 0h32v32h-32v-32z"/></svg><div id=qrcode-img></div></div><script src=https://cdn.jsdelivr.net/npm/qrcode-generator@1.4.4/qrcode.min.js></script>
<script>var typeNumber=0,errorCorrectionLevel='L',qr=qrcode(typeNumber,errorCorrectionLevel);qr.addData('https://allenz-me.github.io/posts/analysis/entropy/'),qr.make(),document.getElementById('qrcode-img').innerHTML=qr.createImgTag()</script></div></div></div><div class=related-posts><h2 class=related-title>相关文章：<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon related-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm144 276c0 6.6-5.4 12-12 12h-92v92c0 6.6-5.4 12-12 12h-56c-6.6.0-12-5.4-12-12v-92h-92c-6.6.0-12-5.4-12-12v-56c0-6.6 5.4-12 12-12h92v-92c0-6.6 5.4-12 12-12h56c6.6.0 12 5.4 12 12v92h92c6.6.0 12 5.4 12 12v56z"/></svg></h2><ul class=related-list><li class=related-item><a href=../../../posts/analysis/lecture4-cont/ class=related-link>Markov Chain</a></li><li class=related-item><a href=../../../posts/analysis/possion/ class=related-link>泊松过程及其模拟</a></li><li class=related-item><a href=../../../posts/analysis/hdp2/ class=related-link><ol start=2><li>Concentration of sums of independent random variables</li></ol></a></li><li class=related-item><a href=../../../posts/analysis/hdp3/ class=related-link><ol start=3><li>Random vectors in high dimensions</li></ol></a></li><li class=related-item><a href=../../../posts/analysis/hdp4/ class=related-link><ol start=4><li>Random matrices</li></ol></a></li></ul></div><div class=post-tags><a href=../../../tags/%E7%86%B5/ rel=tag class=post-tags-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49.0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882.0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>熵</a></div></div></main><div id=back-to-top class=back-to-top><a href=#><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6.0-33.9L207 39c9.4-9.4 24.6-9.4 33.9.0l194.3 194.3c9.4 9.4 9.4 24.6.0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3.0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a></div><footer id=footer class=footer><div class=footer-inner><div class=site-info>2022&nbsp;<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon footer-icon"><path d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3.0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"/></svg>&nbsp;</div><div class=custom-footer>祝您生活愉快！</div></div></footer></div><script>typeof MathJax=='undefined'?(window.MathJax={loader:{load:['[tex]/mhchem']},options:{renderActions:{addMenu:[0,'','']}},tex:{inlineMath:{'[+]':[['$','$']]},tags:'ams',packages:{'[+]':['mhchem']}}},function(){var a=document.createElement('script');a.src='https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js',a.defer=!0,document.head.appendChild(a)}()):(MathJax.texReset(),MathJax.typeset())</script><script src=https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js></script>
<script>let imgNodes=document.querySelectorAll('div.post-body img');imgNodes=Array.from(imgNodes).filter(a=>a.parentNode.tagName!=="A"),mediumZoom(imgNodes,{background:'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'})</script><script src=https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js type=module defer></script></body></html>