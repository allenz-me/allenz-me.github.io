<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-CN"><title type="text">主页</title><subtitle type="html"/><updated>2022-07-05T20:26:55+08:00</updated><id>https://allenz-me.github.io/</id><link rel="alternate" type="text/html" href="https://allenz-me.github.io/"/><link rel="self" type="application/atom+xml" href="https://allenz-me.github.io/atom.xml"/><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><generator uri="https://gohugo.io/" version="0.89.4">Hugo</generator><entry><title type="text">Reinforcement Learning and Optimal Control</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/rl-oc/"/><id>https://allenz-me.github.io/posts/operations/rl-oc/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-07-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Exact Dynamic Programming Deterministic Dynamic Programming A deterministic DP problem involves a discrete-time dynamic system of the form $$ x_{k+1} = f_k(x_k, u_k), \quad k = 0, 1, \dots, N-1 $$ where $k$ is the time index $x_k$ is the state of……</summary><content type="html">&lt;h2 id="exact-dynamic-programming">Exact Dynamic Programming&lt;/h2>
&lt;h3 id="deterministic-dynamic-programming">Deterministic Dynamic Programming&lt;/h3>
&lt;p>A deterministic DP problem involves a discrete-time dynamic system of the form
$$
x_{k+1} = f_k(x_k, u_k), \quad k = 0, 1, \dots, N-1
$$
where&lt;/p>
&lt;ul>
&lt;li>$k$ is the time index&lt;/li>
&lt;li>$x_k$ is the state of the system&lt;/li>
&lt;li>$u_k$ is the control or decision, to be selected from some given set $U_k(x_k)$&lt;/li>
&lt;li>$f_k$ is a function of $(x_k, u_k)$ that describes the mechanism by which the state is updated from time $k$ to time $k+1$&lt;/li>
&lt;li>$N$ is the horizon&lt;/li>
&lt;/ul>
&lt;p>A cost incurred at time $k$, denoted by $g_k(x_k, u_k)$, accumulates over time. For a given initial state $x_0$, &lt;strong>the total costs of a control sequence&lt;/strong> $\{u_0, \dots, u_{N-1}\}$ is
$$
J\left(x_{0} ; u_{0}, \ldots, u_{N-1}\right)=g_{N}\left(x_{N}\right)+\sum_{k=0}^{N-1} g_{k}\left(x_{k}, u_{k}\right)
$$
where $g_N(x_N)$ is a terminal cost incurred at the end of the process.&lt;/p>
&lt;p>We want to minimize the total cost over all sequences $\{u_0, \dots, u_{N-1}\}$ that satisfy the control constraints, thereby obtaining the optimal value
$$
J^{\ast}\left(x_{0}\right)=\min _{\substack{u_{k} \in U_{k}\left(x_{k}\right) \\ k=0, \ldots, N-1}} J\left(x_{0} ; u_{0}, \ldots, u_{N-1}\right)
$$
The picture below illustrates the main elements of the problem.&lt;/p>
&lt;img src="../../figures/RL-OC/image-20220701143017425.png" alt="image-20220701143017425" style="zoom:50%;" />
&lt;h4 id="dynamic-programming-algorithm">Dynamic Programming Algorithm&lt;/h4>
&lt;p>The DP algorithm rests on a simple idea, the &lt;em>principle of optimality&lt;/em>.&lt;/p>
&lt;h5 id="principle-of-optimality">Principle of Optimality&lt;/h5>
&lt;p>Let $\left\{u_{0}^{\ast}, \ldots, u_{N-1}^{\ast}\right\}$ be an optimal control sequence, which together with $x_{0}$ determines the corresponding state sequence $\left\{x_{1}^{\ast}, \ldots, x_{N}^{\ast}\right\}$ via the system equation. Consider the subproblem whereby we start at $x_{k}^{\ast}$ at time $k$ and wish to minimize the &amp;quot;cost-to-go&amp;quot; from time $k$ to time $N$
$$
g_{k}\left(x_{k}^{\ast}, u_{k}\right)+\sum_{m=k+1}^{N-1} g_{m}\left(x_{m}, u_{m}\right)+g_{N}\left(x_{N}\right)
$$
over $\left\{u_{k}, \ldots, u_{N-1}\right\}$ with $u_{m} \in U_{m}\left(x_{m}\right), m=k, \ldots, N-1$. Then the truncated optimal control sequence $\left\{u_{k}^{\ast}, \ldots, u_{N-1}^{\ast}\right\}$ is optimal for this subproblem.&lt;/p>
&lt;p>Stated succinctly, the principle of optimality says that &lt;em>the tail of an optimal sequence is optimal for the tail subproblem&lt;/em>.&lt;/p>
&lt;h5 id="dp-algorithm-for-deterministic-finite-horizon-problems">DP Algorithm for Deterministic Finite Horizon Problems&lt;/h5>
&lt;p>Start with
$$
J_N^\ast (x_N) = g_N(x_N), \quad \text{ for all } x_N
$$
and for $k=0, \dots, N-1$, let
$$
J_{k}^{\ast}\left(x_{k}\right)=\min _{u_{k} \in U_{k}\left(x_{k}\right)}\left[g_{k}\left(x_{k}, u_{k}\right)+J_{k+1}^{\ast}\left(f_{k}\left(x_{k}, u_{k}\right)\right)\right], \quad \text { for all } x_{k}
$$
The algorithm constructs functions $J_N^\ast(x_N), J_{N-1}^\ast(x_{N-1}), \dots, J_0^\ast(x_0)$ sequentially, starting from $J_N^\ast$, and proceeding backwards to $J_{N-1}^\ast, J_{N-2}^\ast, \dots$ .&lt;/p>
&lt;p>$J_k^\ast(x_k)$ is the &lt;em>optimal cost-to-go&lt;/em> at state $x_k$ and time $k$ . We refer $J_k^\ast$ as the &lt;em>optimal cost-to-go function&lt;/em>.&lt;/p>
&lt;h5 id="construction-of-optimal-control-sequence">Construction of Optimal Control Sequence&lt;/h5>
&lt;p>Set
$$
u_{0}^{\ast} \in \underset{u_{0} \in U_{0}{\left(x_{0}\right)} }{\arg\min}\left[g_{0}\left(x_{0}, u_{0}\right)+J_{1}^{\ast}\left(f_{0}\left(x_{0}, u_{0}\right)\right)\right]
$$
and
$$
x_{1}^{\ast}=f_{0}\left(x_{0}, u_{0}^{\ast}\right) .
$$
Sequentially, going forward, for $k=1,2, \ldots, N-1$, set
$$
u_{k}^{\ast} \in \underset{u_{k} \in U_{k}\left(x_{k}^{\ast}\right)}{\arg\min}\left[g_{k}\left(x_{k}^{\ast}, u_{k}\right)+J_{k+1}^{\ast}\left(f_{k}\left(x_{k}^{\ast}, u_{k}\right)\right)\right],
$$
and
$$
x_{k+1}^{\ast}=f_{k}\left(x_{k}^{\ast}, u_{k}^{\ast}\right) .
$$&lt;/p>
&lt;h4 id="approximation-in-value-space">Approximation in Value Space&lt;/h4>
&lt;p>In practice, exact DP is often prohibitively time-consuming, because the number of possible $x_k$ and $k$ can be very large. An alternative by &lt;em>approximation in value space&lt;/em> constructs a suboptimal solution $\{\tilde{u}_0, \dots, \tilde{u}_{N-1}\}$ in place of the optimal $\{u_0^\ast, \dots, u_{N-1}^\ast\}$ based on using $\tilde{J}_k$ in place of $J_k^\ast$ in the DP procedure.&lt;/p>
&lt;h3 id="stochastic-dynamic-programming">Stochastic Dynamic Programming&lt;/h3>
&lt;p>$$
x_{k+1}=f_{k}\left(x_{k}, u_{k}, w_{k}\right), \quad k=0,1, \ldots, N-1
$$&lt;/p>
&lt;p>In a&lt;/p>
&lt;img src="../../figures/RL-OC/image-20220627154726731.png" alt="image-20220627154726731" style="zoom:50%;" />
&lt;h2 id="approximation-in-value-space-1">Approximation in Value Space&lt;/h2>
&lt;p>Curse of di&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/></entry><entry><title type="text">Revenue Management and the Rise of the Algorithmic Economy</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/23/"/><id>https://allenz-me.github.io/posts/papers/23/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-06-19T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Management Science, 2021. DOI: https://doi.org/10.1287/mnsc.2020.3712. Special Section of Management Science: 65th Anniversary. 收益管理的历史 The Early Days: 1970s to Early 1990s 收益管理起源于美国的航空业……</summary><content type="html">&lt;p>发表在 Management Science, 2021. DOI: &lt;a href="https://doi.org/10.1287/mnsc.2020.3712">https://doi.org/10.1287/mnsc.2020.3712&lt;/a>.&lt;/p>
&lt;p>Special Section of Management Science: 65th Anniversary.&lt;/p>
&lt;hr>
&lt;h3 id="收益管理的历史">收益管理的历史&lt;/h3>
&lt;h4 id="the-early-days-1970s-to-early-1990s">The Early Days: 1970s to Early 1990s&lt;/h4>
&lt;p>收益管理起源于美国的航空业如何赚取更多利润这一问题。&lt;/p>
&lt;p>Littlewood (1972) 首先提出了一个分析这个商业问题的框架。假设商务客户和休闲客户可以被区分，那么航空公司可以向他们收取不同的价格。这是一种 quantity-based 方法，针对不同的产品等级设置不同的价格。&lt;/p>
&lt;h4 id="the-growth-of-the-discipline-mid-1990s-to-mid-2000s">The Growth of the Discipline: Mid-1990s to Mid-2000s&lt;/h4>
&lt;p>这个阶段，收益管理逐渐成为了一个成熟的研究领域。&lt;/p>
&lt;p>它开始应用于零售领域。零售（快消）行业面临的问题是如何在一段时间内以合适的价格销售完大部分的货物，这导致了 price-based 方法的兴起。Gallego 和 van Ryzin (1994) 使用 HJB 方程推导出了最优的随时间变化的定价策略。&lt;/p>
&lt;p>紧接着，消费市场的极大繁荣催生了 choice-based 方法。消费者面临的不再是单一的商品，而是琳琅满目的货架。要给消费者展示什么样的货品、定什么样的价格，这就归结到了 assortment optimization。&lt;/p>
&lt;p>总的来说，二十一世纪之前，收益管理成功发展为了一个与现实紧密结合、理论与实践并重的学科。&lt;/p>
&lt;h3 id="收益管理与在线市场">收益管理与在线市场&lt;/h3>
&lt;p>随着互联网的发展，大数据时代的来临，促使收益管理的研究范式从 problem-driven 变成了 data-driven。收益管理面临了新问题、新挑战。值得一提的有三个：线上购物、在线广告、手机打车。&lt;/p>
&lt;p>线上购物几乎彻底地改变了人们的消费方式，也颠覆了经典的收益管理方法。新产品层出不穷，需求无法精准预测；消费者越来越精明；机器学习大规模应用于营销，给消费者呈现出千人千面的选择；线上线下如何做好全渠道营销。&lt;/p>
&lt;p>当人们使用手机的时间越来越多的时候，广告投放的方式也在发生变化。一方面，广告定价数量大，且需要毫秒级响应，这就存在着广告拍卖机制设计与求解的问题；另一方面，个性化广告推荐与用户隐私的矛盾急需调和。&lt;/p>
&lt;p>至于手机打车？这就可以涉及到动态定价（大数据杀熟）了。至于怎么合理地匹配好司机与乘客，也是个重要的问题。&lt;/p>
&lt;h3 id="收益管理的未来">收益管理的未来？&lt;/h3>
&lt;p>在人工智能与大数据兴起的当下，很多时候已经不再是人在做决策了，而是算法在做决策。这是新时代收益管理面临的一大挑战，在未来，我们一定能看到机器学习、强化学习等被大量应用在收益管理中。&lt;/p>
&lt;p>现如今社会开始把公平和隐私摆在更加重要的位置。一些计算机技术，如差分隐私，是不是有可能引入到收益管理的研究中？&lt;/p>
&lt;p>另外，收益管理研究中时常被忽略的一点，是竞争下的收益管理。大多数收益管理的文章都做出了 monopolistic 的假定，而现实大都是 oligopolistic 或者是充分竞争的。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/ms/" term="MS" label="MS"/></entry><entry><title type="text">Inventory Pooling Under Heavy-Tailed Demand</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/20/"/><id>https://allenz-me.github.io/posts/papers/20/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-06-03T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表于 Management Science, 2016. DOI: http://doi.org/10.1287/mnsc.2015.2204. Keywords: inventory management; inventory pooling; demand uncertainty; heavy-tailed distributions Area of review: operations management This paper considers a classic multilocation newsvendor setting following Eppen (1979), where the demand distribution is heavy tailed rather than……</summary><content type="html">&lt;p>发表于 Management Science, 2016. DOI: &lt;a href="http://doi.org/10.1287/mnsc.2015.2204">http://doi.org/10.1287/mnsc.2015.2204&lt;/a>.&lt;/p>
&lt;p>Keywords: inventory management; inventory pooling; demand uncertainty; heavy-tailed distributions&lt;/p>
&lt;p>Area of review: operations management&lt;/p>
&lt;hr>
&lt;p>This paper considers a classic multilocation newsvendor setting following Eppen (1979), where the demand distribution is heavy tailed rather than normally distributed.&lt;/p>
&lt;p>Heavy-tail distributions refer to those whose tail decays at a subexponential rate. Prime examples, like power law distributions
$$
\bar{F}(x) \sim \frac{1}{x^\alpha}, \quad \alpha &amp;gt; 0
$$&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/ms/" term="MS" label="MS"/></entry><entry><title type="text">Transshipment Between Overconﬁdent Newsvendors</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/21/"/><id>https://allenz-me.github.io/posts/papers/21/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-06-02T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Production and operations management, 2021. DOI: https://doi.org/10.1111/poms.13424. Key words: overconfidence; transshipment; behavioral operations management; managerial bias 这篇文章研究了过度自信这一行为对带转运的报童模……</summary><content type="html">&lt;p>发表在 Production and operations management, 2021. DOI: &lt;a href="https://doi.org/10.1111/poms.13424">https://doi.org/10.1111/poms.13424&lt;/a>.&lt;/p>
&lt;p>Key words: overconfidence; transshipment; behavioral operations management; managerial bias&lt;/p>
&lt;hr>
&lt;p>这篇文章研究了过度自信这一行为对带转运的报童模型的影响。&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Consider . . . the most important concepts of behavioral economics: overconﬁdence . . .&lt;/em>&lt;/p>
&lt;p>—— Richard Thaler (2016)&lt;/p>
&lt;p>Of these numerous types of bias, overconﬁdence is “one of the most consistent, powerful, and widespread” cognitive biases.&lt;/p>
&lt;/blockquote>
&lt;h3 id="overconfident-newsvendor">Overconfident newsvendor&lt;/h3>
&lt;p>过度自信的 newsvendor，他们会更多的认为需求靠近均值，如果需求是 $D$，那么因为这种 cognitive bias，在过度自信的人眼里，需求是：
$$
X = \alpha \mu + (1-\alpha) D
$$
其中 $\mu = {E}[D]$。如果 $\alpha &amp;gt; 0$，说明有过度自信存在，$\alpha=1$ 是最大程度的自信。&lt;/p>
&lt;p>过度自信降低了需求估计的偏差，因为 ${E}[X] = {E}[D], \operatorname{Var}(X) = (1-\alpha)^2 \operatorname{Var}(D)$ .&lt;/p>
&lt;h3 id="transshipment">Transshipment&lt;/h3>
&lt;p>考虑两个&lt;strong>独立&lt;/strong>的newsvendor在两个不同的地点，它们面临同样的需求分布 $D_i\, (i=1,2)$，订货成本 $c$，单位利润 $r$，在这种 basic setting 下每个 newsvendor 的利润是：
$$
\pi_{i}^{N}:=E\left\{r \min \left(D_{i}, q_{i}\right)-c q_{i}\right\}
$$
假如 newsvendor $i$ 处有多余的库存，同时 newsvendor $j$ 处有未满足的需求，这时候可以从 $i$ 到 $j$ 转运货物，假设这种转运的成本（售价）是 $\tau \leq r$，
$$
\begin{aligned}
\pi_{i}^{T}\left(q_{i}, q_{j}\right) &amp;amp;= E\left\{r \min \left(D_{i}, q_{i}\right)-c q_{i}\right\} \\
&amp;amp;+ E\left\{(r-\tau) \min \left[\left(D_{i}-q_{i}\right)^{+},\left(q_{j}-D_{j}\right)^{+}\right]\right.\\
&amp;amp; + \left.\tau \min \left[\left(D_{j}-q_{j}\right)^{+},\left(q_{i}-D_{i}\right)^{+}\right]\right\}
\end{aligned}
$$
在均衡状态下，带转运的 $(q_1^T, q_2^T)$ 带来的利润 $\pi_T$ 不低于 $\pi_N$，这是文章的 lemma1。&lt;/p>
&lt;h3 id="symmetric-overconﬁdence">Symmetric Overconﬁdence&lt;/h3>
&lt;p>在相同的过度自信水平下，订货量 $\hat{q}^{N}:=\alpha \mu+(1-\alpha) q^{N}$，从而最终的利润是：
$$
\hat{\pi}_{N}(\alpha):=E\left\{r \min \left(D_{i}, \hat{q}^{N}\right)-c \hat{q}^{N}\right\}
$$
文章接着给出了一个 Observation，就是当 $0 &amp;lt; \alpha &amp;lt; 1$ 时，有可能过度自信会带来利润的降低。&lt;/p>
&lt;blockquote>
&lt;p>Taken together, when the transshipment price is low and the proﬁt margin is high, the second effect can dominate the ﬁrst effect so that the biased newsvendors eventually suffer from transshipment.&lt;/p>
&lt;/blockquote>
&lt;h3 id="asymmetric-overconﬁdence">Asymmetric Overconﬁdence&lt;/h3>
&lt;p>这一部分，文章研究不同过度自信水平造成的影响。不妨设 $\alpha_1 &amp;lt; \alpha_2$。&lt;/p>
&lt;p>给出的结论是：如果对方的过度自信水平更高，那么自己的利润越低！&lt;/p>
&lt;h3 id="empirical-evidence">Empirical Evidence&lt;/h3>
&lt;p>Using the experiment data from Zhao et al. (2020)，这一部分文章做了三步统计分析&lt;/p>
&lt;ol>
&lt;li>Use MLE to estimate the overconfidence parameter&lt;/li>
&lt;li>Use MLE to fit the reduced model&lt;/li>
&lt;li>Perform a likelihood ratio test to assert the existence of overconfidence by rejecting null hypothesis&lt;/li>
&lt;/ol>
&lt;p>Note their MLE for $\alpha$ is approximately 0.65.&lt;/p>
&lt;h3 id="robustness-check">Robustness Check&lt;/h3>
&lt;p>In this section, more behavioral factors are incorporated.&lt;/p>
&lt;p>&lt;strong>Fairness Concern&lt;/strong>&lt;/p>
&lt;p>With fairness taken into account, this paper models the utility of newsvendor $i$ by:
$$
U_{i}=\pi_{i}-\lambda\left(\pi_{j}-\pi_{i}\right)
$$
Where $\lambda \geq 0$ is a fairness parameter. Typically, $\lambda=0$ implies fairness-neutral newsvendor.&lt;/p>
&lt;p>This paper argues that transshipment may still hurt overconﬁdent newsvendors with fairness concerns when the overconﬁdence level is moderate $(0.25 &amp;lt; \alpha &amp;lt; 0.42)$&lt;/p>
&lt;p>&lt;strong>Demand- and Supply-side Thinking&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Loss Aversion&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Demand Anchoring&lt;/strong>&lt;/p>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>文章给出了一个不同寻常的结论：the biased newsvendor does not necessarily receive a higher expected proﬁt under transshipment, particularly when the transshipment price is low and proﬁt margin is high.&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/pom/" term="POM" label="POM"/><category scheme="https://allenz-me.github.io/tags/behavioral-om/" term="Behavioral OM" label="Behavioral OM"/></entry><entry><title type="text">Effects of Centralization on Expected Costs in a Multi-Location Newsboy Problem</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/20/"/><id>https://allenz-me.github.io/posts/papers/20/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-05-25T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Management Science, 1979. DOI: https://doi.org/10.1287/mnsc.25.5.498. Keywords: facilities/equipment planning; inventory/production operating characteristics; inventory/production-stochastic models 这篇文章在报童模型的设定下比较了中心化库存和分散化库……</summary><content type="html">&lt;p>发表在 Management Science, 1979. DOI: &lt;a href="https://doi.org/10.1287/mnsc.25.5.498">https://doi.org/10.1287/mnsc.25.5.498&lt;/a>.&lt;/p>
&lt;p>Keywords: facilities/equipment planning; inventory/production operating characteristics; inventory/production-stochastic models&lt;/p>
&lt;hr>
&lt;p>这篇文章在报童模型的设定下比较了中心化库存和分散化库存的总费用，说明了在一定条件下，中心化库存 (inventory pooling) 能带来费用的下降。&lt;/p>
&lt;p>假设在 $N$ 个 location 同时面临一个报童问题，需求 $\xi_i \;(1\leq i\leq N )$ 服从正态分布 $N(\mu_i, \sigma_i^2)$ ，另有协方差 $\sigma_{ij}$ 和相关系数 $\rho_{ij}$ 。此外 $\sigma_i / \mu_i$ 足够小使得正态分布负的部分足以忽略。&lt;/p>
&lt;p>如果单位库存费用是 $h$，缺货代价是 $p$，则期望的总费用关于订货量 $y$ 的关系是：
$$
H_i(y) = h\mathrm{E}(y - \xi)^{+} + p \mathrm{E}(\xi - y)^{+}
$$
记 $\phi_i(\cdot), \, \Phi_i(\cdot)$ 分别是概率密度和累计分布函数，则 $H_i(y)$ 可以得到解析表达式：
$$
\begin{aligned}
H_{i}(y) &amp;amp; = \int_{-\infty}^{y} h \cdot(y-\xi) \phi_{i}(\xi) d \xi+\int_{y}^{\infty} p \cdot(\xi-y) \phi_{i}(\xi) d \xi \\
&amp;amp; = h\cdot \left( \int_{-\infty} ^{\infty} (y - \xi) \phi_i(s) d \xi - \int_{y} ^{\infty} (y - \xi) \phi_i(s) d \xi \right) + \int_{y}^{\infty} p \cdot(\xi-y) \phi_{i}(\xi) d \xi \\
&amp;amp; = hy-h\mu_i + (h+p) \int_{y}^{\infty} (\xi -y) \phi_i(\xi) d\xi \\
&amp;amp; = h y-h \mu_{i}+(h+p) \sigma_{i} R\left(\frac{y-\mu_{i}}{\sigma_{i}}\right)\\
\end{aligned}
$$&lt;/p>
&lt;p>其中 $R(u)=\displaystyle\int_{u}^{\infty}(w-u) \frac{1}{\sqrt{2 \pi}} e^{-w^{2} / 2} d w$ .&lt;/p>
&lt;p>&lt;strong>关于报童问题有一个经典的结论就是，存在一个最优的订货量，且最优订货量是需求分布的某个分位数&lt;/strong>，在这里，这个分位数是 $p/(p+h)$，也就是说，如果 $\bar{y}_i$ 是最优订货量，那么
$$
\Phi_i(\bar{y}_i) = \frac{p}{p + h}
$$
在正态分布这种场景下，$\bar{y}_i = \mu_i + \bar{z} \sigma_i$，$\bar{z}$ 是正态分布 $p/(p+h)$ 的分位数。这是一个需求分布为 $N(\mu_i, \sigma_i^2)$ 的 location 需要备的库存。&lt;/p>
&lt;p>经计算可以发现，单个 location 最优订货量下的&lt;strong>期望费用是需求分布标准差的某个倍数&lt;/strong>！
$$
H_{i}\left(\bar{y}_{i}\right)=[h \bar{z}+(h+p) R(\bar{z})] \sigma_{i} = K\sigma_i
$$
记这个倍数为 $K$。&lt;/p>
&lt;p>如果这些 location 分散化 (decentralization) 库存的话，那么这时候的总费用是：
$$
TC_D = K \sum_{i=1}^N \sigma_i
$$
如果这些 location 把它们的库存集中起来，那么它们集体面对的需求总和是 $N\left(\displaystyle\sum_{i=1}^N \mu_i, \sum_{i=1}^N \sum_{j=1}^N \sigma_i \sigma_j\right)$，这也是一个正态分布，所以总费用是：
$$
TC_C = K \sqrt{\sum_{i=1}^N \sum_{j=1}^N \sigma_i \sigma_j}
$$
结合概率论的知识，我们能注意到两点结论：&lt;/p>
&lt;ol>
&lt;li>$TC_C \leqslant TC_D$，即中心化的库存能够节约成本&lt;/li>
&lt;li>当且仅当对任意 $i, j$ 都有 $\rho_{ij}=1$ 时，$TC_C = TC_D$&lt;/li>
&lt;/ol>
&lt;p>如果 $N$ 个 location 的需求独立同分布的话，那么 $TC_C = \sqrt{N} K\sigma = TC_D / \sqrt{N}$，也就是说，地点个数的增加，中心化带来的费用降低也越多。&lt;/p>
&lt;p>文章的结论是非常直接明了的，库存的本质就是对抗需求的不确定性，这种不确定性可以用变异系数（标准差除以均值）来衡量。在独立同分布的情况下，两家店把需求统一在一起，均值扩大为原来的两倍，但标准差只变为原来的 $\sqrt{2}$ 倍，这样做实质上降低了需求的不确定性。&lt;/p>
&lt;hr>
&lt;p>这个模型虽然简单，但也有着非凡的现实意义。开设一家中心化的网店，相比开设若干家线下门店，即便二者每天的客户需求相近，前者也能节约更多的库存成本。&lt;/p>
&lt;p>另外，将库存集中起来，既可以是物理上放置在同一个仓库，也可以是虚拟的、逻辑上的集中。&lt;/p>
&lt;hr>
&lt;p>Eppen (1979) 这篇开创性的文章说明了集中化库存最理想能有 $\sqrt{N}$ 程度的费用的节约，这是建立在正态需求分布和没有调货费用的前提下。&lt;/p>
&lt;p>只要把原问题变得更复杂，或是修正前提条件，就能得到一个值得研究的新问题！&lt;/p>
&lt;p>比如：On the Beneﬁts of Risk Pooling in Inventory Management, POM, 2011. 研究了其它参数分布和非参数分布需求下集中化库存带来的费用节约，它们发现需求分布的变异系数会很大地影响集中化库存的效果。&lt;/p>
&lt;p>Inventory Pooling Under Heavy-Tailed Demand, MS, 2016. 研究了重尾分布下的库存池。他们发现需求分布尾部概率越大的时候，集中化库存带来的效益越低。&lt;/p>
&lt;p>还有一大类文章研究供应链的转运（transshipment），它们把调货费用添加到问题背景中。&lt;/p>
&lt;p>较为经典的，如 A Two-Location Inventory Model with Transshipment and Local Decision Making, MS, 2001. 考虑了两个不同地点的报童，当一方有库存剩余而另一方有满足不了的需求时，就可以通过转运增加整体的利润。文章研究了公司内（两个报童是利益共同体）和公司间（两个报童进行博弈）的报童转运问题。&lt;/p>
&lt;p>后续的研究，有些将两个报童扩展为多个，研究供应链网络。如 Optimal Policies for Transshipping Inventory in a Retail Network 研究供应链网络的最优转运策略&lt;/p>
&lt;p>在 A Two-Location Inventory Model with Transshipment and Local Decision Making 这个基础模型之上，近年有较多的文章开始考虑行为（behavior）对这个系统造成的影响。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>如 Inventory Sharing and Demand-Side Underweighting, MSOM, 2021. 通过调查和实验分析了行为因素对多地点库存的影响，他们发现实践当中经理的订货量往往偏低，并建立了一套理论模型加以解释。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Transshipment Between Overconﬁdent Newsvendors, POM, 2021. 研究了过度自信的报童，他们建立理论模型发现，如果报童是过度自信的话，公司间的库存共享可能导致双方的利润降低。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>（后续的文献，大都包含关键词 inventory pooling, inventory sharing, transshipment。）&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/ms/" term="MS" label="MS"/></entry><entry><title type="text">Inverse of the sum of matrices</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/inverse-of-matrix-sum/"/><id>https://allenz-me.github.io/posts/analysis/inverse-of-matrix-sum/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-05-11T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">如果 $A, B$ 都是正定矩阵，那么： $$ (A + B)^{-1} = A^{-1}-A^{-1}(A^{-1} + B^{-1})^{-1}A^{-1} $$ 根据 $A^{-1}B^{-1}=(BA)^{-1}$ 可以得到： $$ (A + B)^{-1} = A^{-1} (A^{-1} + B^{-1})^{-1} B^{-1} $$……</summary><content type="html">&lt;p>如果 $A, B$ 都是正定矩阵，那么：
$$
(A + B)^{-1} = A^{-1}-A^{-1}(A^{-1} + B^{-1})^{-1}A^{-1}
$$&lt;/p>
&lt;p>根据 $A^{-1}B^{-1}=(BA)^{-1}$ 可以得到：
$$
(A + B)^{-1} = A^{-1} (A^{-1} + B^{-1})^{-1} B^{-1}
$$
此外
$$
\begin{aligned}
(A^{-1} + B^{-1})^{-1} B^{-1} &amp;amp; = (A^{-1} + B^{-1})^{-1} (A^{-1} + B^{-1} - A^{-1}) \\
&amp;amp; = I - (A^{-1} + B^{-1})^{-1}A^{-1}
\end{aligned}
$$
两式结合即得： $(A + B)^{-1} = A^{-1}-A^{-1}(A^{-1} + B^{-1})^{-1}A^{-1}$&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/></entry><entry><title type="text">The Optimizer’s Curse: Skepticism and Postdecision Surprise in Decision Analysis</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/archives/19/"/><id>https://allenz-me.github.io/posts/archives/19/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-04-29T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Management Science, 2006. DOI: https://doi.org/10.1287/mnsc.1050.0451. Key words: decision analysis; optimization; optimizer’s curse; Bayesian models; postdecision surprise; disappointment 一个分析团队刚刚将复杂分……</summary><content type="html">&lt;p>发表在 Management Science, 2006. DOI: &lt;a href="https://doi.org/10.1287/mnsc.1050.0451">https://doi.org/10.1287/mnsc.1050.0451&lt;/a>.&lt;/p>
&lt;p>Key words: decision analysis; optimization; optimizer’s curse; Bayesian models; postdecision surprise; disappointment&lt;/p>
&lt;hr>
&lt;p>一个分析团队刚刚将复杂分析的结果提交给负责决策的高管。分析人士建议做一项创新投资，并声称，尽管该投资并非没有风险，但它具有巨大的正预期净现值。这位高管倾向于遵循团队的建议，但她回忆，在过去遵循这些建议后，结果有些失望。虽然分析似乎公平公正，但她还是忍不住感到有点怀疑。她的怀疑有道理吗？&lt;/p>
&lt;p>这是文章开头抛出的问题。这个问题的背后，就是 optimizer's curse。&lt;/p>
&lt;h3 id="optimizers-curse">Optimizer’s Curse&lt;/h3>
&lt;p>假如决策者要考虑 $n$ 个 alternative，它们的真实效用值是 $\mu_1, \dots, \mu_n$，但是决策者是不知道的；决策者只能通过分析得到 $n$ 个效用的估计值 $V_1, \dots, V_n$，然后选择估计效用最大的那个选项。&lt;/p>
&lt;p>假设 $V_{i^\ast} = \displaystyle\max_i\, V_i$，那么平均来说 postdecision suprise 等于 $\mathrm{E}[\mu_{i^\ast} - V_{i^\ast}]$，这是实际效用与期望效用的差值。&lt;strong>即便 $V_i$ 都是 $\mu_i$ 的无偏估计，$\mathrm{E}[\mu_{i^\ast} - V_{i^\ast}]$ 仍然可能是负的。&lt;/strong>&lt;/p>
&lt;p>文章接着用一些例子来说明这个。&lt;/p>
&lt;p>假定 $\mu_1=\mu_2=\mu_3$，估计值 $V_i \sim \mathcal{N}(0, 1)$，那实际上我们期待的效用值 $V_{i^\ast}$ 是三个标准正态分布取最大，$\mathrm{E}[\mu_{i^\ast} - V_{i^\ast}]$ 是负的。&lt;/p>
&lt;img src="../../figures/19/image-20220429163255638.png" alt="image-20220429163255638" style="zoom:40%;" />
&lt;p>如下图，我们做决策的 disappointment 随着选择的增加而增加。&lt;/p>
&lt;img src="../../figures/19/image-20220429163516964.png" alt="image-20220429163516964" style="zoom:40%;" />
&lt;p>$\mu_i$ 的（正）相关性会增大这种影响，$V_i$ 的正相关性会减小这种影响。&lt;/p>
&lt;img src="../../figures/19/image-20220430120302276.png" alt="image-20220430120302276" style="zoom:40%;" />
&lt;p>此外，文章还谈论了 optimizer's curse 对 &amp;quot;value added&amp;quot; 的影响。&lt;/p>
&lt;blockquote>
&lt;p>Value added: the difference between the estimated value of the optimal alternative identiﬁed in the analysis and the estimated value of a default alternative (or current plan) that would have been chosen if no analysis were done.&lt;/p>
&lt;/blockquote>
&lt;h3 id="what-to-do">What to Do&lt;/h3>
&lt;p>文章用例子说明了 optimizer's curse 可能对决策产生实质性的影响，接着文章提出了两个问题：&lt;/p>
&lt;ol>
&lt;li>How should we adjust our value estimates to eliminate this effect?&lt;/li>
&lt;li>How should it affect decision making?&lt;/li>
&lt;/ol>
&lt;p>文章提出用贝叶斯统计的方式来处理 optimizer's curse。&lt;/p>
&lt;p>假设真实效用 $\boldsymbol{\mu} = (\mu_1, \dots, \mu_n)$，估计值 $\boldsymbol{V} = (V_1, \dots, V_n)$ 由 $\boldsymbol{V} \mid \boldsymbol{\mu}$ 来刻画，于是我们可以使用贝叶斯法则得到后验分布 $\boldsymbol{\mu} \mid \boldsymbol{V}$ . 令 $\boldsymbol{v} = \mathrm{E}[\boldsymbol{\mu \mid \boldsymbol{V}}]$，则 $\hat{v}_{i^\ast}$ 是一个好的估计。&lt;/p>
&lt;p>这一部分文章使用的很多贝叶斯统计的工具。主要假设了高斯先验。&lt;/p>
&lt;h3 id="related-biases-and-curses">Related Biases and “Curses”&lt;/h3>
&lt;p>文章在第四部分比较了其它的 &amp;quot;curses&amp;quot;。&lt;/p>
&lt;p>比如 winner’s curse，拍卖中出价最高的一方往往支付了超过拍品本身价值的钱。&lt;/p>
&lt;p>比如幸存者偏差，optimizer's curse 可以看成是只有一个幸存者的情况。&lt;/p>
&lt;h3 id="总结">总结&lt;/h3>
&lt;p>这篇文章只有12页。思想深刻，理论精炼。&lt;/p>
&lt;p>文章提到，虽然 optimizer's curse 在实践中经常遇到，但是用实际数据证明它可能是很难的。&lt;/p>
&lt;p>文章提炼了一下 optimizer's curse: If decision makers take the value estimates resulting from an analysis at face value and select according to these estimates, then they should expect to be disappointed on average, &lt;strong>not because of any inherent bias in the estimates themselves, but because of the selection process itself&lt;/strong>.&lt;/p>
&lt;p>解决问题的思路：treat the results of the analysis as uncertain and &lt;strong>combine these results with prior estimates of value&lt;/strong> using Bayes’ rule before choosing an alternative.&lt;/p>
&lt;p>最后，文章回答了一开始提出的问题。是的，怀疑非常有道理；要做出好的决策，必须要克服 optimizers' curse。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/ms/" term="MS" label="MS"/></entry><entry><title type="text">风险测度 — Risk Measure</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/risk-measure/"/><id>https://allenz-me.github.io/posts/operations/risk-measure/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-04-23T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">在金融数学中，risk measure 用来度量一组资产的风险。经典的 MPT 使用方差作为风险的度量， 令 $(\Omega,……</summary><content type="html">&lt;p>在金融数学中，risk measure 用来度量一组资产的风险。经典的 MPT 使用方差作为风险的度量，&lt;/p>
&lt;p>令 $(\Omega, \mathcal{F}, \mathrm{P})$ 是一个概率空间，$\mathcal{L}$ 是其上所有可积的随机变量，risk measure $\rho$ 指的是 $\mathcal{L} \to [-\infty, +\infty]$ 的泛函。&lt;/p>
&lt;p>Risk measure 经常提到如下的几个性质：&lt;/p>
&lt;ol>
&lt;li>Monotonicity: $\rho(X) \leqslant \rho(Y)$ if $X \leqslant Y$.&lt;/li>
&lt;li>Translation invariance: $\rho(X+c)=\rho(X)+c$ for any $c \in \mathbb{R}$.&lt;/li>
&lt;li>Positive homogeneity: $\rho(\lambda X)=\lambda \rho(X)$ for any $\lambda&amp;gt;0$.&lt;/li>
&lt;li>Subadditivity: $\rho(X+Y) \leqslant \rho(X)+\rho(Y)$.&lt;/li>
&lt;/ol>
&lt;p>满足以上四点的度量称为 coherent risk measure。&lt;/p>
&lt;h2 id="dispersion-measures">Dispersion Measures&lt;/h2>
&lt;h2 id="downside-risk-measures">Downside Risk Measures&lt;/h2>
&lt;h3 id="value-at-risk">Value at Risk&lt;/h3>
&lt;p>在险价值 VaR 是一种常用的风险测度。假设一个投资组合的损失 $L = -r^T w$，则它置信度为 $1-\alpha$ 的 VaR 指的是概率不超过 $\alpha$ 的最大损失。
$$
\begin{aligned}
\text{VaR}_{1-\alpha}(L) = &amp;amp; \inf\, \{x : \mathrm{P}(L \leqslant x) \geqslant 1-\alpha\} = \inf\, \{x : \mathrm{P}(L &amp;gt; x) \leqslant \alpha\} \\
= &amp;amp; \inf\, \{x: F_L(x) \geqslant 1- \alpha \}
\end{aligned}
$$&lt;/p>
&lt;!--所以 VaR 越小越好-->
&lt;p>如果 $L \sim \mathcal{N}(\mu, \sigma^2)$，则 $\text{VaR}_{1-\alpha}(L) = \mu + z_{1-\alpha} \sigma$ .&lt;/p>
&lt;p>VaR 不是一致性风险测度，因为它不满足次可加性 (subadditivity)。&lt;/p>
&lt;p>对于 VaR 和机会约束 (chance constraint)，有如下的等价关系：【待确定】
$$
\text{VaR}_{1-\alpha} (L) \leqslant t \,\Longleftrightarrow \, \mathrm{P}(L \leqslant t) \geqslant 1-\alpha
$$&lt;/p>
&lt;h4 id="var-的优化">VaR 的优化&lt;/h4>
&lt;p>对于一个投资组合 $r^Tw$ 来说，最小化它的 VaR，即 $\displaystyle\min_{w}\, \text{VaR}_{1-\alpha}(-r^T w)$ 等价于如下的优化问题：
$$
\begin{aligned}
\min_{\gamma, \,w} \; &amp;amp; \gamma \\
\text{s.t. } &amp;amp; \mathrm{P}(-r^T w &amp;gt; \gamma) \leqslant \alpha \\
&amp;amp; \sum_{i=1}^n w_i = 1
\end{aligned}
$$
假设我们只有 $S$ 个 sample/senario，记 $y_s = \begin{cases}0, &amp;amp; \text{otherwise} \\ 1, &amp;amp; -\hat{r}_s^T w &amp;gt; \gamma \end{cases}$，则有：
$$
\begin{aligned}
\min_{\gamma, \, \vec{y}, \, w} \; &amp;amp; \gamma \\
\text{s.t. } &amp;amp; \sum_{s=1}^S y_s \leqslant \lfloor \alpha \cdot S\rfloor \\
&amp;amp; \sum_{i=1}^n w_i = 1,\;\, y_s = \begin{cases}0, &amp;amp; \text{otherwise} \\ 1, &amp;amp; -\hat{r}_s^T w &amp;gt; \gamma \end{cases}
\end{aligned}
$$
于是该问题最终可以写成：
$$
\begin{aligned}
\min_{\gamma, \, \vec{y}, \, w} \; &amp;amp; \gamma \\
\text{s.t. } &amp;amp; -\hat{r}_s^T w \leqslant \gamma + \mathrm{M} \cdot y_s, \; s=1, \dots, S \\
&amp;amp; \sum_{s=1}^S y_s \leqslant \lfloor \alpha \cdot S\rfloor \\
&amp;amp; \sum_{i=1}^n w_i = 1, \;\, y_s \in \{0, 1\}
\end{aligned}
$$
这是一个混合整数优化问题！&lt;/p>
&lt;h3 id="conditional-value-at-risk">Conditional Value at Risk&lt;/h3>
&lt;p>CVaR 是一个一致的风险测度。
$$
\text{CVaR}_{1-\alpha}(L) = \frac{1}{\alpha} \int_\alpha^1 \text{VaR}_u(L) \mathrm{d} u = \mathbb{E} [L \mid L \geq \text{VaR}_{1-\alpha}(L)]
$$&lt;/p>
&lt;h4 id="portfolio-optimization-with-cvar">Portfolio Optimization with CVaR&lt;/h4>
&lt;p>CVaR 有一个计算表达式：
&lt;strong>(Rockfellar &amp;amp; Uryasev 2000)&lt;/strong>
$$
\text{CVaR}_{1-\alpha}(L) = \min_{\xi\in \mathrm{R}} \left[ \xi + \frac{1}{\alpha}\mathrm{E}(L-\xi)^+\right]
$$&lt;/p>
&lt;p>优化 CVaR 是一个线性非整数的问题，对于 $\displaystyle\min_w \text{CVaR}_{1-\alpha} (-r^T w)$，利用上述命题，可以得到它的等价转化：&lt;/p>
&lt;p>$$
\begin{aligned}
\min_{\xi, \, \vec{y}, \, w} \; &amp;amp; \xi+\frac{1}{\alpha \cdot S} \sum_{s=1}^{S} y_{s} \\
\text {s.t. } &amp;amp; y_{s} \geq -\hat{r}_s^T w -\xi, \; s=1, \ldots, S \\
&amp;amp; \sum_{i=1}^{n} w_{i}=1, \; y_{s} \geq 0,\, s=1, \ldots, S
\end{aligned}
$$&lt;/p>
&lt;p>对于一个特定的投资组合，即确定 $w$，解上述线性规划，可以得到这个投资组合的 CVaR。&lt;/p>
&lt;p>对于给定的 $\text{CVaR}_{1-\alpha}$ level $\beta$，找到期望收益最大的投资组合，这个问题是：
$$
\begin{aligned}
\min_{\xi, \, \vec{y}, \, w} \; &amp;amp; - \frac{1}{S} \sum_{s=1}^S \hat{r}_s^T w \\
\text{s.t. } &amp;amp; \xi + \frac{1}{\alpha \cdot S} \sum_{s=1}^{S}(-\hat{r}_s^T w-\xi)^+ \leqslant \beta\\
&amp;amp; \sum_{i=1}^n w_i = 1\\
\end{aligned}
$$
引入 $y_s = (-\hat{r}_s^T w -\xi)^+$，有：
$$
\begin{aligned}
\min_{\xi, \, \vec{y}, \, w} \; &amp;amp; - \frac{1}{S} \sum_{s=1}^S \hat{r}_s^T w \\
\text{s.t. } &amp;amp; \xi + \frac{1}{\alpha \cdot S} \sum_{s=1}^{S} y_s \leqslant \beta\\
&amp;amp; y_s \geqslant -\hat{r}_s^T w -\xi, \;y_s \geqslant 0, \; s=1,\dots, S\\
&amp;amp; \sum_{i=1}^n w_i = 1\\
\end{aligned}
$$&lt;/p>
&lt;ol>
&lt;li>Law determination: $\rho(X)=\rho(Y)$ if $X$ and $Y$ have the same distribution.&lt;/li>
&lt;/ol></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/></entry><entry><title type="text">图上的匹配</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/matching/"/><id>https://allenz-me.github.io/posts/operations/matching/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-04-22T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Matching 图 $G(V, E)$ 上的一个匹配 (matching) 指的是一个集合 $M\subseteq E$，$M$ 中任何两条边没有公共端点。如果一个……</summary><content type="html">&lt;h1 id="matching">Matching&lt;/h1>
&lt;p>图 $G(V, E)$ 上的一个匹配 (matching) 指的是一个集合 $M\subseteq E$，$M$ 中&lt;strong>任何两条边没有公共端点&lt;/strong>。如果一个匹配包含了图的每一个顶点，就说这个匹配是完美的 (perfect matching)。&lt;/p>
&lt;p>下图表示了一个图的匹配：&lt;/p>
&lt;img src="../../figures/Matching/image-20220422200807886.png" alt="image-20220422200807886" style="zoom:40%;" />
&lt;p>称一个匹配是极大的 (maximal)，如果增加一条边之后就不是一个匹配了。组合优化里一个很基础的问题就是最大基数匹配 (&lt;em>maximum&lt;/em> cardinality matching)，完美匹配自然是最大基数匹配，但是完美匹配可能是不存在的。&lt;/p>
&lt;p>用 $\delta(v)$ 表示与 $v\in V$ 相邻的边，对于图的一个 characteristic vector ${x}$，它是一个 matching vector 当且仅当：&lt;/p>
&lt;p>$$
\sum_{e \in \delta(v)} x_e \leq 1 \;\; \forall v \in V, \quad x\in \{0, 1\}^E
$$&lt;/p>
&lt;p>用 node-edge incidence matrix 来表示，就是 $\{x \in \{0, 1\}^E: A_G x \leq \mathbf{1}\}$，它的 linear relexation 是 $\{x: A_Gx \leq \mathbf{1}, x\geq \mathbf{0}\}$ 。&lt;/p>
&lt;h2 id="max-matching-and-min-vertex-cover">Max Matching and Min Vertex Cover&lt;/h2>
&lt;p>图的最大基数匹配 (max matching) 问题是：
$$
\begin{aligned}
\max \; &amp;amp; \mathbf{1}^T x \\
\text{s.t. } &amp;amp; A_G x \leq \mathbf{1} \\
&amp;amp; x \in \{0, 1\}^E
\end{aligned}
$$
图的 vertex cover 指的的 $V$ 的一个子集，使得每一条边都至少包含这个子集的一个点。图的最小顶点覆盖 (min vertex cover) 问题是：
$$
\begin{aligned}
\min \; &amp;amp; \mathbf{1}^T y \\
\text{s.t. } &amp;amp; y_i + y_j \geq 1 \quad \forall (i, j) \in E \\
&amp;amp; y \in \{0, 1\}^V
\end{aligned}
$$&lt;/p>
&lt;p>图的最大匹配小于等于其最小顶点覆盖。&lt;/p>
&lt;p>&lt;strong>(König Theorem)&lt;/strong> 二分图的最大基数匹配等于其最小顶点覆盖。&lt;/p>
&lt;p>Size of max matching = size of min vertex cover, in bipartite graphs.&lt;/p>
&lt;h2 id="matching-polytope">Matching polytope&lt;/h2>
&lt;p>定义 $G$ 的 matching polytope 为所有匹配的凸包，记：
$$
P_{\text{match}}(G) = \operatorname{conv} \{\chi_M: M \text{ is a matching of } G\}
$$
定义其 pefect matching polytope 为所有的完美匹配的凸包，记：
$$
P_{\text{perf-match}}(G) = \operatorname{conv} \{\chi_M: M \text{ is a perfect matching of } G\}
$$&lt;/p>
&lt;p>如果 $G$ 是二分的，则 $A_G$ 是 TUM，所以：&lt;/p>
&lt;p>$$
\begin{aligned}
P_{\text{match}}(G) &amp;amp; = \{x \in \mathbb{R}^E \mid A_G x \leq \mathbf{1}, \, x\geq 0 \} \\
P_{\text{perf-match}}(G) &amp;amp; = \{x \in \mathbb{R}^E \mid A_G x= \mathbf{1}, \, x \geq 0\}
\end{aligned}
$$&lt;/p>
&lt;p>对于一般图，Edmonds 给出了如下的刻画：&lt;/p>
&lt;h3 id="matching-polytope-theorem">Matching Polytope Theorem&lt;/h3>
&lt;p>$$
\begin{aligned}
P_{\text{match}}(G) &amp;amp;= \{x \in \mathbb{R}^E \mid A_G x \leq \mathbf{1}, \, x \geq 0, \sum_{e \in E[U]} x_{e} \leq \frac{|U|-1}{2},\; \forall U \subseteq V,|U| \text { odd }\} \\
P_{\text{perf-match}}(G) &amp;amp; = \{x \in \mathbb{R}^E \mid A_G x= \mathbf{1}, \, x \geq 0, \sum_{e \in \delta(U)} x_{e} \geq 1, \; \forall U \subseteq V,|U| \text { odd }\}
\end{aligned}
$$&lt;/p>
&lt;p>考虑一个三角形，约束条件
$$
\begin{aligned}
&amp;amp;x_{12}+x_{13} \leq 1\\
&amp;amp;x_{12}+x_{23} \leq 1\\
&amp;amp;x_{13}+x_{23} \leq 1\\
&amp;amp;x \geq 0
\end{aligned}
$$
注意到 $(1/2, 1/2, 1/2)$ 也是这组约束表示的多面体的一个顶点，但它并不代表一个合法的匹配。所以说，刻画 matching polytope 的核心就是处理 odd cycle。&lt;/p>
&lt;p>令 $E[U] = \{(i, j) \in E \mid i, j \in U\}$ 表示奇数个节点内部的匹配，对于三角形来说，$(1/2, 1/2, 1/2)$ 不满足 $\displaystyle\sum_{e \in E[U]} x_e \leq \frac{|U|-1}{2}=1, \, U=V$ 。添加这个条件可以刻画 matching polytope。&lt;/p>
&lt;p>对于完全匹配，由 $\sum_{e \in \delta(v)} x_{e}=1, \; v \in V$ 对所有的 $v \in U \subseteq V$ 求和得
$$
|U|=\sum_{v \in U} \sum_{e \in \delta(v)} x_{e}=\sum_{e \in \delta(U)} x_{e}+2 \sum_{e \in E[U]} x_{e}
$$&lt;/p>
&lt;p>结合 $\displaystyle\sum_{e \in E[U]} x_e \leq \frac{|U|-1}{2}$ ，有
$$
\sum_{e \in \delta(U)} x_{e} \geq 1, \; \forall U \subseteq V,|U| \text { odd }
$$
这个条件表示的是任何 odd set $U$，都有一个 $U$ 中的点与 $V \backslash U$ 中的点相匹配。这是关于 perfect matching 的限制。&lt;/p>
&lt;h2 id="matchings-in-bipartite-graphs">Matchings in Bipartite Graphs&lt;/h2>
&lt;p>二分图指的是图的点可以划分成不相交的两组点 $A, B$，使得每一条边都恰好连接 $A, B$ 的顶点；等价地，&lt;strong>二分图可以定义为无奇数圈 (odd-length circle) 的图&lt;/strong>。下图是一个二分图，三角形不是二分图。&lt;/p>
&lt;p>【二分图的图例】&lt;/p>
&lt;h3 id="halls-theorem">Hall’s Theorem&lt;/h3>
&lt;p>令 $G=(V, E)$ 是一个二分图且 $V=U\cup W$，对于 $S \subseteq U$，定义 $S$ 的 neibor $N(S) \subseteq W$ 为与 $S$ 相邻的点，则：
$$
G \text{ has a matching covering } U \Longleftrightarrow |N(S)| \geq |S| \quad \forall S\subseteq U
$$
特别地，如果 $|U|=|W|$，$G$ 存在一个完美匹配。&lt;/p>
&lt;blockquote>
&lt;p>见：https://zhuanlan.zhihu.com/p/77563002&lt;/p>
&lt;/blockquote>
&lt;p>Hall 定理的证明可以用到 König 定理。&lt;/p>
&lt;h3 id="cardinality-bipartite-matching-algorithm">Cardinality bipartite matching algorithm&lt;/h3>
&lt;h2 id="matching-in-non-bipartite-graphs">Matching in Non-Bipartite Graphs&lt;/h2>
&lt;h3 id="augmenting-paths">Augmenting Paths&lt;/h3>
&lt;p>给定一个匹配 $M$，称一条路为&lt;strong>交错路 ($M$-alternation path)&lt;/strong> ，如果这条路每两条相邻的边都有一条在 $M$ 中；称其为&lt;strong>增广路 ($M$-augmenting path)&lt;/strong>，如果它是一个交错路，并且路的两个端点不在 $M$ 中。&lt;/p>
&lt;p>如果 $P$ 是一条增广路，那么 $N=M \triangle P=(M \backslash P) \cup(P \backslash M)$ 也是一个匹配，注意到 $| P \backslash M | = | P \cap M | + 1$，所以 $| N | = | M | + | P \backslash M | − | P ∩ M | = | M | + 1$ 是一个基数更大的匹配。&lt;/p>
&lt;p>由此我们能得到一个最大基数匹配的充分必要条件：
$$
M \text{ is a maximum matching } \Leftrightarrow \text{ no } M\text{-augmenting path}
$$&lt;/p>
&lt;p>【证明】&lt;/p>
&lt;p>所以说，要找最大匹配，其实就是要找 M-augmenting path。&lt;/p>
&lt;h3 id="edmonds-algorithm">Edmonds’ Algorithm&lt;/h3>
&lt;p>If there are any vertices not covered by the current matching (“exposed” vertices), search for an $M$-augmenting path from each exposed vertex $v$. If an augmenting path $P$ is found, augment $M$ by updating $M$ to $M\Delta P$. If no $M$-augmenting paths can be found, $M$ is optimal.&lt;/p>
&lt;p>At any step in the algorithm, let M be the current matching and let X be the set of exposed vertices (vertices not covered by M).&lt;/p>
&lt;h3 id="tutte-berge-formula">Tutte-Berge Formula&lt;/h3>
&lt;p>这个公式是对图的最大匹配数的一个刻画。&lt;/p>
&lt;p>记&lt;/p>
&lt;ul>
&lt;li>$\nu(G): \text{size of maximum matching}$&lt;/li>
&lt;li>$o(G):\text{number of the connected components of the graph that have an odd number of vertices}$&lt;/li>
&lt;/ul>
&lt;p>则：
$$
\nu(G)=\min _{U \subseteq V} \frac{1}{2}(|V|+|U|-o(G-U))
$$&lt;/p>
&lt;p>推论(Tutte’s theorem)：$G$ 存在完美匹配当且仅当 $\forall\, U \subseteq V, o(G-U) \leq |U|$ . 这是 Hall 定理在一般图的推广。&lt;/p>
&lt;h3 id="gallais-theorem">Gallai’s theorem&lt;/h3>
&lt;p>记：&lt;/p>
&lt;p>$$
\begin{aligned}
\alpha(G):=&amp;amp;\max \{|C| \mid C \text{ is a stable set } \} \\
\tau(G):=&amp;amp;\min \{|W| \mid W \text{ is a vertex cover } \} \\
\nu(G):=&amp;amp;\max \{|M| \mid M \text{ is a matching} \} \\
\rho(G):=&amp;amp;\min \{|F| \mid F \text{ is an edge cover } \} \\
\end{aligned}
$$&lt;/p>
&lt;p>如果图 $G=(V, E)$ 没有孤立点，则：&lt;/p>
&lt;p>$$
\alpha(G)+\tau(G)=|V|=\nu(G)+\rho(G)
$$&lt;/p>
&lt;p>(1) $U$ is a stable set $\Longleftrightarrow V \backslash U$ is a vertex cover.&lt;/p>
&lt;p>(2) 给定一个最大匹配 $M$，对每一个没有匹配到的 $v \notin M$ 添加一条边连接至 $M$，再加上 $M$ 中的边，这样可以形成一个 vertex cover，这说明 $\rho(G) \leq |M| + (|V|-2|M|)=|V| - \nu(G)$； 另一方面，给定一个最小边覆盖 $|F|$，图 $(V, F)$ 由 $|V|-|F|$ 个不相交的 $G$ 的子图构成，用 $|V|-|F|$ 条边将它们连接起来，可以构成一个匹配，这说明 $\nu(G) \geq |V| - |F| = |V| - \rho(G)$&lt;/p>
&lt;p>&lt;strong>König's edge cover theorem&lt;/strong>&lt;/p>
&lt;p>二分图的最大独立集数等于其最小边覆盖。&lt;/p>
&lt;hr>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">def&lt;/span> &lt;span class="nf">isBipartite&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">graph&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="c1"># 用深度优先搜索的方式判断一个图是否是二分的&lt;/span>
&lt;span class="c1"># graph[i]: 与顶点 i 相邻的其它顶点&lt;/span>
&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">graph&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">visited&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">n&lt;/span>
&lt;span class="n">not_visited&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">n&lt;/span>
&lt;span class="n">color&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">n&lt;/span>
&lt;span class="n">st&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">st&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">v&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">st&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">color&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;span class="n">not_visited&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">graph&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">color&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kc">False&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">True&lt;/span>
&lt;span class="n">color&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">st&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">not_visited&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">True&lt;/span>
&lt;span class="k">break&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kc">True&lt;/span>
&lt;span class="c1"># example&lt;/span>
&lt;span class="n">graph&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">],[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">],[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">],[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]]&lt;/span>
&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">isBipartite&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">graph&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="c1"># True&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E6%95%B4%E6%95%B0%E5%92%8C%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/" term="整数和组合优化" label="整数和组合优化"/></entry><entry><title type="text">Ellipsoid Algorithm</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/ellipsoid-algo/"/><id>https://allenz-me.github.io/posts/operations/ellipsoid-algo/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-04-21T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">椭球算法最早于1977年由俄罗斯数学家Shor提出用于解决一般的凸优化问题，后在19……</summary><content type="html">&lt;p>椭球算法最早于1977年由俄罗斯数学家Shor提出用于解决一般的凸优化问题，后在1979年被 Leonid Khachiyan 用于线性规划。椭球算法是第一个解线性规划的多项式时间的算法，虽然效果不好，但是在组合优化中有理论价值。&lt;/p>
&lt;h2 id="ellipsoid">Ellipsoid&lt;/h2>
&lt;p>欧式空间中的一个椭球，可以用一个正定矩阵和一个向量来表示：
$$
\begin{aligned}
E(A, c) &amp;amp;= \{ x \in \mathbb{R}^n : (x-c)^T A^{-1} (x-c) \leq 1\} \\
&amp;amp;= A^{1/2}S(0, 1) + c
\end{aligned}
$$
任何一个椭球都可以看成是单位球 $S(0, 1)$ 在线性变换的作用下的像再平移的结果。&lt;/p>
&lt;p>椭球的体积：
$$
\operatorname{vol} (E(A, c)) = \sqrt{\det A} \cdot \operatorname{vol}(S(0, 1)) = \sqrt{\det A} \cdot\frac{\pi^{n/2}}{\Gamma(n/2+1)}
$$&lt;/p>
&lt;h2 id="ellipsoid-algorithm">Ellipsoid Algorithm&lt;/h2>
&lt;p>Ellipsoid algorithm 意在解决这样一个问题：
$$
\text{Given a bounded polytope } P \in \mathbb{R}^n \text{ find } x \in P \text{ or assert } P \text{ empty}
$$
假设我们有这样一个 &lt;em>separation oracle&lt;/em> :给定一个 $x$，oracle 返回结果：$x \in P$ 或者一个 violated constraint $c^T x \leq d$ .&lt;/p>
&lt;p>其实就是当 $x \notin P$ 的时候返回一个 weakly seperating hyperplane between $x$ and $P$ . 这个 Oracle 能在多项式时间内运行是椭球法多项式时间结论的基本要求。&lt;/p>
&lt;p>椭球算法的基本思想如下：&lt;/p>
&lt;ul>
&lt;li>Let $E_{0}$ be an ellipsoid containing $P$&lt;/li>
&lt;li>while center $a_{k}$ of $E_{k}$ is not in $P$ do:
&lt;ul>
&lt;li>Let $c^{T} x \leq c^{T} a_{k}$ be such that $\left\{x: c^{T} x \leq c^{T} a_{k}\right\} \supseteq P$&lt;/li>
&lt;li>Let $E_{k+1}$ be the minimum volume ellipsoid containing $E_{k} \cap\left\{x: c^{T} x \leq c^{T} a_{k}\right\} \supseteq P$&lt;/li>
&lt;li>$k \leftarrow k+1$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>一次迭代的过程如下图所示：&lt;/p>
&lt;img src="../../figures/ellipsoid/image-20220421160023125.png" alt="image-20220421160023125" style="zoom:50%;" />
&lt;p>能这样迭代地找到满足条件的椭球的理论依据是 &lt;strong>Löwner-John theorem&lt;/strong>&lt;/p>
&lt;p>对于 $\mathbb{R}^n$ 中的一个凸紧集 $S$，存在最小的椭球 $E(S) \supseteq S$，且 $E(S) \supseteq S \supseteq \displaystyle\frac{1}{n} E(S)$ .&lt;/p>
&lt;p>由 $E_k(A_k, a_k)$ 确定 $E_{k+1}(A_{k+1}, a_{k+1})$ 的过程如下：&lt;/p>
&lt;p>While $a_{k} \notin P$ do:&lt;/p>
&lt;ul>
&lt;li>Let $c^{T} x \leq d$ be an inequality that is valid for all $x \in P$ but $c^{T} a_{k}&amp;gt;d$.&lt;/li>
&lt;li>Let $b=\frac{A_{k} c}{\sqrt{c^{T} A_{k} c}}$.&lt;/li>
&lt;li>Let $a_{k+1}=a_{k}-\frac{1}{n+1} b$.&lt;/li>
&lt;li>Let $A_{k+1}=\frac{n^{2}}{n^{2}-1}\left(A_{k}-\frac{2}{n+1} b b^{T}\right)$.&lt;/li>
&lt;/ul>
&lt;p>每一轮迭代都能将椭球的体积缩小，成立关系：
$$
\frac{\operatorname{vol}\left(E_{k+1}\right)}{\operatorname{vol}\left(E_{k}\right)}&amp;lt;\exp \left(-\frac{1}{2(n+1)}\right) &amp;lt; 1
$$
缩小的比例只于维数有关，而与迭代步数是无关的。&lt;/p>
&lt;p>当算法的迭代步数很大的时候，便可以认为 $P$ 是空集了。因为，一般来说 $P=\{x: Cx \leq d\}$，$\displaystyle\frac{\operatorname{vol}(E_0)}{\operatorname{vol}(P)}$ 关于 encoding length ($\langle C, d \rangle$) 是多项式的。&lt;/p>
&lt;h3 id="from-feasibility-to-optimization">From feasibility to Optimization&lt;/h3>
&lt;p>上述算法能检验线性不等式组的可行性 (feasibility)，要求解 $\max \{c^T x : x \in P\}$，只需令 $Q_t=P\cap \{c^T x \leq t\}$，对 $t$ 进行二分查找即可。&lt;/p>
&lt;p>所以，总的来说，椭球算法的时间复杂度就是多项式的。(weakly polynomial)&lt;/p>
&lt;p>椭球法还能解决一般的凸优化问题。&lt;/p>
&lt;h3 id="oracles-and-computational-complexity">Oracles and Computational Complexity&lt;/h3>
&lt;p>椭球算法可以分解为：calls to a separation oracle + basic arithmetic operations.&lt;/p>
&lt;p>事实上，&lt;strong>separation 和 optimization 在计算上是等价的 (computationally equivalent)&lt;/strong>，给定凸集 $K$，对于：&lt;/p>
&lt;ul>
&lt;li>Separate $({K}):$ Given as input a vector $\vec{x}$, output yes if $\vec{x} \in \mathcal{K}$, or a hyperplane separating $\vec{x}$ from ${K}$.&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>Optimize $({K}):$ Given as input a vector $\vec{w}$, output $\arg \max _{\vec{x} \in {K}}\{\vec{x} \cdot \vec{w}\}$.&lt;/li>
&lt;/ul>
&lt;p>椭球算法实质上说明了，如果能在多项式时间内计算Separate $({K})$ ，那么也能在多项式时间内计算 Optimize $({K})$。&lt;/p>
&lt;p>除了 separation oracle，还有其它的 oracle，比如：&lt;/p>
&lt;ul>
&lt;li>Violation oracle&lt;/li>
&lt;li>Optimization oracle&lt;/li>
&lt;li>Validity oracle&lt;/li>
&lt;li>Membership Oracle&lt;/li>
&lt;/ul>
&lt;p>见：https://en.wikipedia.org/wiki/Separation_oracle&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E6%95%B4%E6%95%B0%E5%92%8C%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/" term="整数和组合优化" label="整数和组合优化"/><category scheme="https://allenz-me.github.io/tags/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/" term="线性规划" label="线性规划"/></entry><entry><title type="text">Total Unimodularity</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/total-unimodular/"/><id>https://allenz-me.github.io/posts/operations/total-unimodular/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-04-20T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Total Unimodularity 矩阵 $A$ 是全单位模 (totally unimodular) 矩阵，如果它的每一个方的子矩阵的行列式取值于 $\{0, \pm1\}$. 这意味着 $A$ 的……</summary><content type="html">&lt;h3 id="total-unimodularity">Total Unimodularity&lt;/h3>
&lt;p>矩阵 $A$ 是全单位模 (&lt;strong>totally unimodular&lt;/strong>) 矩阵，如果它的每一个方的子矩阵的行列式取值于 $\{0, \pm1\}$. 这意味着 $A$ 的所有元素都是 $0$ 或者 $\pm1$ .&lt;/p>
&lt;p>单位模矩阵在整数规划起着非常重要的作用。&lt;/p>
&lt;p>如果 $A$ 是 TUM (totally unimodular matrix)，对于 $b\in \mathbb{Z}^m$，多面体 $P=\{x\in \mathbb{R}^n: Ax\leq b\}$ 的顶点都是整点！&lt;/p>
&lt;p>多面体的每个顶点都是解 $n$ 个线性无关的方程组得来的，因此根据 Cramer 法则，容易得到上述结论。事实上这个结论反过来也是成立的，对于任意的 $b\in \mathbb{Z}^m$，多面体 $P=\{x\in \mathbb{R}^n: Ax\leq b\}$ 的顶点都是整点，那么 $A$ 是 TUM。&lt;/p>
&lt;p>以下结论是等价的：&lt;/p>
&lt;ul>
&lt;li>$A$ 是 TUM&lt;/li>
&lt;li>$A^T$ 是 TUM&lt;/li>
&lt;li>$-A$ 是 TUM&lt;/li>
&lt;li>$\begin{bmatrix} A &amp;amp; I \end{bmatrix}$ 是 TUM&lt;/li>
&lt;li>$\begin{bmatrix} I &amp;amp; A &amp;amp; -A \end{bmatrix}$ 是 TUM&lt;/li>
&lt;/ul>
&lt;p>于是，如果 $A$ 是 TUM，只要其它向量都是整的，则下面的多面体都是整的：&lt;/p>
&lt;ul>
&lt;li>$\{x : Ax \leq b\}$&lt;/li>
&lt;li>$\{x : Ax = b\}$&lt;/li>
&lt;li>$\{x : Ax \leq b, x\geq 0\}$&lt;/li>
&lt;li>$\{x : Ax = b, x\geq 0\}$&lt;/li>
&lt;li>$\{ x : c \leq Ax \leq d, l \leq x \leq u \}$&lt;/li>
&lt;/ul>
&lt;p>保持 TU 的运算：&lt;/p>
&lt;ul>
&lt;li>用 -1 乘它的一行或一列&lt;/li>
&lt;li>交换它的两行或两列&lt;/li>
&lt;li>将一行（列）从另外一行（列）中减去&lt;/li>
&lt;/ul>
&lt;p>保持 TU 的关键是不改变行列式的值。&lt;/p>
&lt;p>&lt;strong>(Hoffman, Kruskal)&lt;/strong>
$$
A \text{ is total unimodular} \Longleftrightarrow \{x: Ax \leq b, x\geq 0, A \text{ integral}\} \text{ is integral for every } b \in \mathbb{Z}^m
$$&lt;/p>
&lt;p>【Seymour's Decomposition Theorem】&lt;/p>
&lt;p>借助 total unimodularity，我们可以确定哪些整数规划问题可以直接等价于其线性松弛。&lt;/p>
&lt;h3 id="tum-and-graphs">TUM and Graphs&lt;/h3>
&lt;p>&lt;strong>(Ghouila-Houri,1962)&lt;/strong>&lt;/p>
&lt;p>$A\in \mathbb{R}^{m \times n}$ 是 TUM 的充分必要条件是：对每一个 $R\subseteq \{1, \dots, m\}$，都有一个划分 $R=R_1 \cup R_2, R_1 \cap R_2 = \emptyset$，使得对每一个 $j=1, \dots, n$，都有：
$$
\sum_{i\in R_1} a_{ij} - \sum_{i \in R_2} a_{ij} \in \{0, \pm 1\}
$$
这种性质也叫做 equitable row-bicoloring。&lt;/p>
&lt;p>&lt;strong>无向图 $G(V, E)$ 的 incidence edge matrix 是 TUM，当且仅当 $G$ 是二分图，&lt;/strong> 是上述定理的直接推论。&lt;/p>
&lt;p>&lt;strong>有向图的 incidence arc matrix 是 TUM。&lt;/strong>&lt;/p>
&lt;h4 id="max-matching-and-min-vertex-cover">Max Matching and Min Vertex Cover&lt;/h4>
&lt;p>图的最大基数匹配 (max matching) 问题是：
$$
\begin{aligned}
\max \; &amp;amp; \mathbf{1}^T x \\
\text{s.t. } &amp;amp; A_G x \leq \mathbf{1} \\
&amp;amp; x \in \{0, 1\}^E
\end{aligned}
$$
图的 vertex cover 指的的 $V$ 的一个子集，使得每一条边都至少包含这个子集的一个点。图的最小顶点覆盖 (min vertex cover) 问题是：
$$
\begin{aligned}
\min \; &amp;amp; \mathbf{1}^T y \\
\text{s.t. } &amp;amp; y_i + y_j \geq 1 \quad \forall (i, j) \in E \\
&amp;amp; y \in \{0, 1\}^V
\end{aligned}
$$&lt;/p>
&lt;p>&lt;strong>(König Theorem)&lt;/strong> 二分图的最大基数匹配等于其最小顶点覆盖。&lt;/p>
&lt;h3 id="total-dual-integrality">Total Dual Integrality&lt;/h3>
&lt;p>有理系统 $Ax \leq b$ 称作全对偶整的 (&lt;strong>TDI&lt;/strong>, total dual integral)，如果对任意的整向量 $c$，只要 $\max\{c^T x: A x \leq b\}$ 有解，其对偶问题 $\min \{b^T y : A^Ty = c, y \geq 0\}$ 都存在整数最优解。（不一定所有的最优解都要是整数的）&lt;/p>
&lt;p>TDI 是比 TU 更强的条件：$(A, b) \text{ TDI }, b \in \mathbb{Z}^m \Longrightarrow \{x: Ax \leq b\} \text{ is integral}$.&lt;/p>
&lt;p>反过来是可能不成立的，存在整的多面体不是TDI的**。但是，任何整的多面体都可以被一个 TDI 系统表示。**&lt;/p>
&lt;blockquote>
&lt;p>However, any integral polyhedron can always be represented by a TDI system whose coeﬃcients are all integer.&lt;/p>
&lt;p>TDI is a representation, not a polytope.&lt;/p>
&lt;/blockquote>
&lt;p>比如 $P = \{(x_1, x_2) : x_1 + 2x_2 \leq 6, 2x_2 + x_1 \leq 6, \; x_1, x_2 \geq 0\}$ ，问题 $\displaystyle\max_{(x_1, x_2) \in P}\, \{x_1 + x_2\}$ 的对偶问题无整数解。&lt;/p>
&lt;p>但是，取 $P$ 的另一种表示：$P = \{(x_1, x_2) : x_1 + 2x_2 \leq 6, 2x_2 + x_1 \leq 6, x_1+ x_2 \leq 4, \; x_1, x_2 \geq 0\}$，这时候问题 $\displaystyle\max_{(x_1, x_2) \in P}\, \{x_1 + x_2\}$ 的对偶问题存在整数解。&lt;/p>
&lt;p>【connection with Hilbert Basis】&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E6%95%B4%E6%95%B0%E5%92%8C%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/" term="整数和组合优化" label="整数和组合优化"/></entry><entry><title type="text">Distributionally Robust Optimization Under Moment Uncertainty with Application to Data-Driven Problems</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/18/"/><id>https://allenz-me.github.io/posts/papers/18/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-04-15T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Operations Research, 2010. DOI: https://doi.org/10.1287/opre.1090.0741. Subject classifications: programming: stochastic; statistics: estimation; finance: portfolio. Area of review: Optimization. 这篇文章讲的是 moment-based DRO . 文章一共就四章。 在我眼里……</summary><content type="html">&lt;p>发表在 Operations Research, 2010. DOI: &lt;a href="https://doi.org/10.1287/opre.1090.0741">https://doi.org/10.1287/opre.1090.0741&lt;/a>.&lt;/p>
&lt;p>Subject classifications: programming: stochastic; statistics: estimation; finance: portfolio.&lt;/p>
&lt;p>Area of review: Optimization.&lt;/p>
&lt;hr>
&lt;p>这篇文章讲的是 moment-based DRO . 文章一共就四章。&lt;/p>
&lt;p>&lt;strong>在我眼里，文章有俩可以说的贡献，一个是建模了moment-based DRSP，另一个是给出了协方差矩阵的置信区间。&lt;/strong>&lt;/p>
&lt;h3 id="robust-stochastic-programming-with-moment-uncertainty">Robust Stochastic Programming with Moment Uncertainty&lt;/h3>
&lt;p>这一章讲述如何构建带有矩不确定性的 DRSP 模型。&lt;/p>
&lt;p>目标函数
$$
\underset{\mathbf{x} \in \mathscr{X}}{\operatorname{minimize}}\left(\max _{F \in \mathscr{D}} \mathbb{E}_{F}[h(\mathbf{x}, \boldsymbol{\xi})]\right) \tag{DRSP}
$$
对于不确定参数 $\boldsymbol{\xi}$，我们能从经验分布得到它的均值 $\boldsymbol{\mu}_0$ 和协方差矩阵 $\boldsymbol{\Sigma}_0$；文章提出了一类不确定集：
$$
\begin{align}
&amp;amp;\left(\mathbb{E}[\boldsymbol{\xi}]-\boldsymbol{\mu}_{0}\right)^{\top} \boldsymbol{\Sigma}_{0}^{-1}\left(\mathbb{E}[\boldsymbol{\xi}]-\boldsymbol{\mu}_{0}\right) \leqslant \gamma_{1} \tag{1a} \\
&amp;amp;\mathbb{E}\left[\left(\boldsymbol{\xi}-\boldsymbol{\mu}_{0}\right)\left(\boldsymbol{\xi}-\boldsymbol{\mu}_{0}\right)^{\top}\right] \preceq \gamma_{2} \boldsymbol{\Sigma}_{0} \tag{1b} \\
\end{align}
$$
其中 $\gamma_1, \gamma_2$ 是参数。(1a) 用一个椭球来限制 $\boldsymbol{\xi}$ 的均值，(1b) 用一个矩阵不等式限制 $\boldsymbol{\xi}$ 的协方差矩阵。&lt;/p>
&lt;p>由此，文章定义了 distribution set:
$$
\mathscr{D}_{1}\left(\mathscr{S}, \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}, \gamma_{1}, \gamma_{2}\right) = \left\{\begin{array}{l|l}
F \in M &amp;amp; \begin{array}{l}
\mathbb{P}(\boldsymbol{\xi} \in \mathscr{S})=1 \\
\left(\mathbb{E}[\boldsymbol{\xi}]-\boldsymbol{\mu}_{0}\right)^{\top} \boldsymbol{\Sigma}_{0}^{-1}\left(\mathbb{E}[\boldsymbol{\xi}]-\boldsymbol{\mu}_{0}\right) \leqslant \gamma_{1} \\
\mathbb{E}\left[\left(\boldsymbol{\xi}-\boldsymbol{\mu}_{0}\right)\left(\boldsymbol{\xi}-\boldsymbol{\mu}_{0}\right)^{\top}\right] \preceq \gamma_{2} \boldsymbol{\Sigma}_{0}
\end{array}
\end{array}\right\}
$$&lt;/p>
&lt;h4 id="inner-moment-problem">Inner Moment Problem&lt;/h4>
&lt;p>记内层问题的最优值 $\Psi(\boldsymbol{x}; \gamma_1, \gamma_2) = \displaystyle\max_{F \in \mathscr{D}} \mathbb{E}_{F}[h(\mathbf{x}, \boldsymbol{\xi})]$，文章把它化成了一个非凸优化 .... 其中一个约束条件看上去不是凸的。&lt;/p>
&lt;p>然后开始 argue，就算非凸 也能在多项式时间内解决。用的是椭球算法，假设一些（不靠谱的？） oracle，推了几个结论出来。&lt;/p>
&lt;h4 id="distributionally-robust-stochastic-program">Distributionally Robust Stochastic Program&lt;/h4>
&lt;p>考虑整体的DRSP优化问题，举了几个例子，感觉比较有意义的是 DR-CVaR.&lt;/p>
&lt;h3 id="moment-uncertainty-in-data-driven-problems">Moment Uncertainty in Data-Driven Problems&lt;/h3>
&lt;p>这一章其实就是想表达：&lt;strong>在有限样本下，根据经验分布构造的 moment-based distributional set 有极高的概率包含了真实分布。&lt;/strong>&lt;/p>
&lt;p>文章假设随机向量 $\boldsymbol{\xi}$ 是有界的，&lt;em>我个人感觉这个假设是很强的，一些结论也许放宽到次高斯分布也成立&lt;/em>。&lt;/p>
&lt;p>Corollary 1 表明了样本均值 $\hat{\boldsymbol{\mu}} = (1/M) \sum_{i=1}^M \boldsymbol{\mu}_i$ 是可以很接近。&lt;/p>
&lt;p>Theorem 2 是文章自己推出来的关于协方差矩阵的结论。&lt;/p>
&lt;p>总之，文章说明了，可以找到 $\bar{\gamma}_1(\bar{\delta}),\, \bar{\gamma}_2(\bar{\delta})$ 使得 distribution set $\mathscr{D}_{1}\left(\mathscr{S}, \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}, \gamma_{1}, \gamma_{2}\right)$ 以 $1-\bar{\delta}$ 的概率包含 $\boldsymbol{\xi}$ 的真实分布。&lt;/p>
&lt;h3 id="application-to-portfolio-optimization">Application to Portfolio Optimization&lt;/h3>
&lt;p>拿投资组合做了个例子，假设的 piecewise linear and concave 效用函数。&lt;/p>
&lt;hr>
&lt;p>小结：大四上初看，看不咋懂，研一下重看，有了些别样的收获。个人认为本文的理论意义大于实践意义，假设过强，结论欠缺些优美。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/or/" term="OR" label="OR"/><category scheme="https://allenz-me.github.io/tags/robust-optimization/" term="Robust Optimization" label="Robust Optimization"/></entry><entry><title type="text">Robust Satisficing</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/17/"/><id>https://allenz-me.github.io/posts/papers/17/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-04-15T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Operations Research, 2022. DOI: https://doi.org/10.1287/opre.2021.2238. Area of Review: Optimization Keywords: robust optimization; robust satisficing; data-driven; discrete optimization; stochastic optimization; fragility measure 这篇文章提出了 robust satisficing 这一鲁棒优化的 fr……</summary><content type="html">&lt;p>发表在 Operations Research, 2022. DOI: &lt;a href="https://doi.org/10.1287/opre.2021.2238">https://doi.org/10.1287/opre.2021.2238&lt;/a>.&lt;/p>
&lt;p>Area of Review: Optimization&lt;/p>
&lt;p>Keywords: robust optimization; robust satisficing; data-driven; discrete optimization; stochastic optimization; fragility measure&lt;/p>
&lt;hr>
&lt;p>这篇文章提出了 robust satisficing 这一鲁棒优化的 framework。&lt;/p>
&lt;blockquote>
&lt;p>In this paper, we introduce a new target-oriented model for robust data-driven optimization termed robust satisﬁcing.&lt;/p>
&lt;p>The goal of robust satisﬁcing is to maximize the robustness to uncertainty of achieving a satisfactory target.&lt;/p>
&lt;/blockquote>
&lt;p>以下公式编号均为论文中的公式编号。&lt;/p>
&lt;p>对于一个带有不确定性的优化问题：
$$
\begin{array}{rlr}
Z^{\ast}=\min &amp;amp; \mathbb{E}_{\mathbb{P}^{\ast}}[f(x, \tilde{z})] \\
\text { s.t. } &amp;amp; x \in \mathcal{X},
\end{array}\tag{1}
$$
其中 $x$ 是决策变量，$\tilde{z} \sim \mathbb{P}^\ast$ . 事实上真实分布 $\mathbb{P}^\ast$ 我们是不知道的，我们只有一些 sample，有一个经验分布 $\hat{\mathbb{P}}$ 。&lt;/p>
&lt;p>但是真实分布应该是接近经验分布的，给出一个度量我们可以找到以经验分布为中心的一个球：
$$
\mathcal{B}(r):=\left\{\mathbb{P} \in \mathcal{P}_{0}(\mathcal{Z})\;\middle\vert\;\begin{array}{l}
\tilde{\boldsymbol{z}} \sim \mathbb{P} \\
\Delta(\mathbb{P}, \hat{\mathbb{P}}) \leq r
\end{array}\right\}
$$
所以 &lt;em>data-driven robust optimization&lt;/em> 可以写成：
$$
\begin{array}{rlr}
Z_r=\min &amp;amp; \displaystyle\sup_{{\mathbb{P}}\in \mathcal{B}(r)} \mathbb{E}[f(x, \tilde{z})] \\
\text { s.t. } &amp;amp; x \in \mathcal{X},
\end{array} \tag{4}
$$
定义 type-1 Wasserstein metric:
$$
\Delta_{W}(\mathbb{P}, \hat{\mathbb{P}}):=\inf _{\mathbb{Q} \in \mathcal{P}_{0}\left(\mathcal{Z}^{2}\right)}\left\{\mathbb{E}_{\mathbb{Q}}[\|\tilde{z}-\tilde{v}\|] \mid(\tilde{z}, \tilde{v}) \sim \mathbb{Q}, \tilde{z} \sim \mathbb{P}, \tilde{v} \sim \hat{\mathbb{P}}\right\}
$$
它是一个合适的度量。&lt;/p>
&lt;h3 id="robust-satisficing">Robust satisficing&lt;/h3>
&lt;p>文章提出了一个新的模型，叫做 robust satisficing:
$$
\begin{aligned}
\kappa_{\tau}=\min\; &amp;amp; k \\
\text {s.t. } &amp;amp; \mathbb{E}_{\mathbb{P}}[f(x, \tilde{z})]-\tau \leq k \Delta(\mathbb{P}, \hat{\mathbb{P}}) \quad \forall \mathbb{P} \in \mathcal{P}_{0}(\mathcal{Z}) \\
&amp;amp; x \in \mathcal{X}, k \geq 0
\end{aligned} \tag{6}
$$
$\tau$ 是一个事先给定的 targeted level。&lt;/p>
&lt;p>如果选择 $\tau = Z_r$，这意味着最优解满足：
$$
\begin{array}{ll}
\mathbb{E}_{\mathbb{P}}[f(x, \tilde{z})]-\tau \leq 0 &amp;amp; \forall \mathbb{P} \in \mathcal{B}(r) \\
\mathbb{E}_{\mathbb{P}}[f(x, \tilde{z})]-\tau \leq+\infty &amp;amp; \forall \mathbb{P} \in \mathcal{P}_{0}(\mathcal{Z}) \backslash \mathcal{B}(r)
\end{array}
$$
我个人感觉这样建模的思想与 Generalized Robust Counterparts 相似（《Robust Optimization》 Ben-Tal, Chapter 11）。能在一定程度上缓解鲁棒优化保守型的缺点。&lt;/p>
&lt;p>用经验分布解这个问题，得到 $Z_0$；用DRO方法解这个问题，得到 $Z_r$，一般来说 $Z_0 &amp;lt; Z_r$。&lt;/p>
&lt;p>Robust satisficing 要求：
$$
\mathbb{E}_{\mathbb{P}}[f(x, \tilde{z})]-\tau \leq k_\tau \Delta(\mathbb{P}, \hat{\mathbb{P}}) \quad \forall \mathbb{P} \in \mathcal{P}_{0}(\mathcal{Z})
$$
意思是说，并不要求所有的 $\mathcal{B}(r)$ 中的分布满足 $$&lt;/p>
&lt;h4 id="tractable-reformation">Tractable reformation&lt;/h4>
&lt;p>如果不确定参数的 support $\\mathcal{Z}$ 是一个闭凸集，那么 robust satisficing 等价于：
$$
\begin{aligned}
\kappa_{\tau}=\min &amp;amp; k \
\text { s.t. } &amp;amp; \frac{1}{S} \sum_{s \in[S]} y_{s} \leq \tau \
&amp;amp; y_{s} \geq \sup &lt;em>{z&lt;/em>{s} \in \mathcal{Z}}\left{f\left(x, z_{s}\right)-k\left|z_{s}-\hat{z}_{s}\right|\right} \quad \forall s \in[S] \
&amp;amp; x \in \mathcal{X}, k \geq 0,
\end{aligned} \tag{9}
$$
分析的方法类似于 Wasserstein DRO 。&lt;/p>
&lt;h3 id="stochastic-free-robust-optimization">Stochastic-free robust optimization&lt;/h3>
&lt;p>把 $r$ 取成0，问题变成普通的经验损失函数最小化，而把 $r$ 取的足够大（把所有 $\\mathcal{Z}$ 上的分布都考虑进来）就是经典的鲁棒优化。
直接根据式(9)，对于名义值为 $\\hat{z}$ 的不确定参数和不确定集 $\\mathcal{Z}$，stochastic-free robust satisﬁcing model 就是：
$$
\begin{array}{ll}
\min &amp;amp; k\
\text { s.t. } &amp;amp; f(x, z) \leq \tau+k|z-\hat{z}| \quad \forall z \in \mathcal{Z} \
&amp;amp; x \in \mathcal{X}, \quad k \geq 0,
\end{array}
$$&lt;/p>
&lt;h3 id="fragility-measure">Fragility measure&lt;/h3>
&lt;p>fragility measure 可以理解为一种反向的 satisficing measure，satisficing measure 衡量的是满意测度，而 fragility measure 衡量的是不满意测度。$\\tau$ 是我们期望的 target，如果 $f(x, \\tilde{z}) \\leq \\tau$，说明符合期待，如果 $f(x, \\tilde{z}) &amp;gt; \\tau$ ，说明不符合我们的期待。所以，最小化我们的 discontentment，就是如下的优化问题：
$$
\begin{array}{ll}
\min &amp;amp; \rho(f(x, \tilde{z})-\tau) \
\text { s.t. } &amp;amp; x \in \mathcal{X}
\end{array} \tag{12}
$$
为了契合这种思想，文章定义了一种 satisficing measure，叫做 fragility measure。
\ast\astDefinition (Fragility Measure).\ast\ast The functional $\\rho: \\mathcal{L}$ $\\longmapsto[0,\+\\infty]$ is a fragility measure associated with the probability distribution $\\hat{\\mathbb{P}} \\in \\mathcal{P}\_{0}$ if and only if it has the following representation
$$
\begin{aligned}
\rho(\tilde{v})= \min; &amp;amp; k \
\text { s.t. } &amp;amp; \mathbb{E}&lt;em>{\mathbb{P}}[\tilde{v}] \leq k \Delta(\mathbb{P}, \hat{\mathbb{P}}) \quad \forall \mathbb{P} \in \mathcal{P}&lt;/em>{0} \
&amp;amp; k \geq 0
\end{aligned}
$$
for some probability distance function $\\Delta$ .
文章的第三章，就是在强调 robust satisficing 的 implications。&lt;/p>
&lt;blockquote>
&lt;h3 id="satisﬁcing-measures">Satisﬁcing Measures&lt;/h3>
&lt;p>论文：Satisﬁcing Measures for Analysis of Risky Positions. DOI: &lt;a href="https://doi.org/10.1287/mnsc.1080.0929">https://doi.org/10.1287/mnsc.1080.0929&lt;/a>&lt;/p>
&lt;p>satisficing measure 是一个测度，它用来衡量一个人对自己 portfolio 的\ast\ast满意程度\ast\ast。&lt;/p>
&lt;p>设 $\\tau$ 是投资者的 aspiration level，它是一个随机变量；一个投资组合的 uncertain payoff $X$ 也是随机的。\asttarget premium\ast $V=X\-\\tau$ 也是一个随机变量，它的正负反映了投资组合的收益是否达到投资者的期望。&lt;/p>
&lt;p>\ast\astDEFINITION.\ast\ast A function $\\rho: \\mathcal{L} \\rightarrow[0, \\bar{\\rho}]$, where $\\bar{\\rho} \\in$ $\\{1, \\infty\\}$, is a \astsatisficing measure\ast defined on the target premium if it satisfies the following axioms for all $X, Y \\in \\mathscr{X}$ :&lt;/p>
&lt;ol>
&lt;li>Attainment content: If $X \\geq 0$, then $\\rho(X)=\\bar{\\rho}$&lt;/li>
&lt;li>Nonattainment apathy: If $X&amp;lt;0$, then $\\rho(X)=0$&lt;/li>
&lt;li>Monotonicity: If $X \\geq Y$, then $\\rho(X) \\geq \\rho(Y)$&lt;/li>
&lt;li>Gain continuity: $\\lim \_{a \\downarrow 0} \\rho(X\+a)=\\rho(X)$&lt;/li>
&lt;/ol>
&lt;p>最显然、最简单的一个 satisficing measure 就是 $\\rho(X) = \\mathbb{P}(X \\geq 0)$ 。&lt;/p>
&lt;/blockquote>
&lt;h3 id="simulation-study-i-portfolio-optimization">Simulation Study I: Portfolio Optimization&lt;/h3>
&lt;p>文章的第一个 simulation 的例子，是非常经典的投资组合优化。假设历史收益率数据 $(\\hat{z}\_1, \\dots, \\hat{z}\_n)$，则名义的投资组合优化优化是：
$$
\begin{aligned}
Z_{0}=\max ;&amp;amp; \mathbb{E}&lt;em>{\hat{\mathbb{P}}}\left[\boldsymbol{x}^{\top} \tilde{\boldsymbol{z}}\right] \
\text {s.t. } &amp;amp; \mathbb{C}&lt;/em>{\hat{\mathbf{P}}}^{\epsilon}\left[-\boldsymbol{x}^{\top} \tilde{\boldsymbol{z}}\right] \leq \beta \
&amp;amp; \mathbf{1}^{\top} \boldsymbol{x}=1 \
&amp;amp; \boldsymbol{x} \in \mathbb{R}&lt;em>{+}^{N}
\end{aligned}
$$
这里 $\\mathbb{C}\_{\\hat{\\mathbf{P}}}^{\\epsilon}$ 表示 CVaR，第一个约束就是 CVaR 约束。
DRO的优化模型是：
$$
\begin{aligned}
Z&lt;/em>{r}=\max &amp;amp; \inf &lt;em>{\mathbb{P} \in \mathcal{B}(r)} \mathbb{E}&lt;/em>{\mathbb{P}}\left[\boldsymbol{x}^{\top} \tilde{\boldsymbol{z}}\right] \
\text { s.t. } &amp;amp; \alpha+\frac{1}{\epsilon} \mathbb{E}&lt;em>{\mathbb{P}}\left[\left(-\boldsymbol{x}^{\top} \tilde{\boldsymbol{z}}-\alpha\right)^{+}\right] \leq \beta \quad \forall \mathbb{P} \in \mathcal{B}(r) \
&amp;amp; \boldsymbol{1}^{\top} \boldsymbol{x}=1 \
&amp;amp; \boldsymbol{x} \in \mathbb{R}&lt;/em>{+}^{N}, ,\alpha \in \mathbb{R},
\end{aligned}
$$
而 RS (robust satisficing) 的模型是：
$$
\begin{array}{rcl}
\kappa_{\tau}= &amp;amp; \min &amp;amp; k_{0}+w k_{1} \
&amp;amp; \text { s.t. } &amp;amp; \mathbb{E}&lt;em>{\mathbb{P}}\left[x^{\top} \tilde{\boldsymbol{z}}\right] \geq \tau-k&lt;/em>{0} \Delta_{W}(\mathbb{P}, \hat{\mathbb{P}}) \quad \forall \mathbb{P} \in \mathcal{P}&lt;em>{0}(\mathcal{Z}) \
&amp;amp; &amp;amp; \alpha+\displaystyle\frac{1}{\epsilon} \mathbb{E}&lt;/em>{\mathbb{P}}\left[\left(-x^{\top} \tilde{\boldsymbol{z}}-\alpha\right)^{+}\right] \leq \beta+k_{1} \Delta_{W}(\mathbb{P}, \hat{\mathbb{P}}) &amp;amp; \forall \mathbb{P} \in \mathcal{P}&lt;em>{0}(\mathcal{Z}) \
&amp;amp; &amp;amp; \mathbf{1}^{\top} x=1, , x \in \mathbb{R}&lt;/em>{+}^{N}
\end{array} \tag{33}
$$
文章的定理8给出了最终版：
$$
\begin{aligned}
\min ;&amp;amp; |x|&lt;em>{\infty} \
\text { s.t. } &amp;amp; \frac{1}{S} \sum&lt;/em>{s \in[S]} y_{1 s} \geq \tau &amp;amp; \
&amp;amp; y_{1 s} \leq \boldsymbol{x}^{\top} \hat{\boldsymbol{z}}&lt;em>{s} &amp;amp; \forall s \in[S] \
&amp;amp; \alpha+\frac{1}{\epsilon S} \sum&lt;/em>{s \in[S]} y_{2 s} \leq \beta &amp;amp; \
&amp;amp; y_{2 s} \geq-\boldsymbol{x}^{\top} \hat{\boldsymbol{z}}&lt;em>{s}-\alpha &amp;amp; \forall s \in[S] \
&amp;amp; y&lt;/em>{2 s} \geq 0 &amp;amp; \forall s \in[S] \
&amp;amp; \mathbf{1}^{\top} \boldsymbol{x}=1 &amp;amp; \
&amp;amp; \boldsymbol{x} \in \mathbb{R}_{+}^{N}, \alpha \in \mathbb{R} . &amp;amp;
\end{aligned}
$$&lt;/p>
&lt;p>【未完待续】&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/or/" term="OR" label="OR"/><category scheme="https://allenz-me.github.io/tags/robust-optimization/" term="Robust Optimization" label="Robust Optimization"/></entry><entry><title type="text">整数线性规划模型</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/integer-models/"/><id>https://allenz-me.github.io/posts/operations/integer-models/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-04-15T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Knapsack Problem 背包问题，指的是要往一个容量有限的背包里装尽可能价值更高的物品。 物品重量 $a_i……</summary><content type="html">&lt;h3 id="knapsack-problem">Knapsack Problem&lt;/h3>
&lt;p>背包问题，指的是要往一个容量有限的背包里装尽可能价值更高的物品。&lt;/p>
&lt;p>物品重量 $a_i$，背包容量 $b$，问题的可行域是：
$$
S:=\left\{x \in \mathbb{Z}^{n}: \sum_{i=1}^{n} a_{i} x_{i} \leq b, x \geq 0\right\}
$$
如果 $x$ 的取值是 0 或 1，那这就是 0-1 背包问题，其可行域：
$$
K:=\left\{x \in\{0,1\}^{n}: \sum_{i=1}^{n} a_{i} x_{i} \leq b\right\}
$$
对于 0-1 背包问题，其 &lt;strong>minimal cover&lt;/strong> 指的是一个指标集，背包不能装下所有的物品，但是任少一件都可以装下。根据 minimal cover 可以诱导出一个集合：
$$
K^{C}:=\left\{x \in\{0,1\}^{n}: \sum_{i \in C} x_{i} \leq|C|-1 \text { for every minimal cover } C \text { for } K\right\} .
$$
可以证明 $K=K^C$ ，也就是说0-1背包问题有两种表示方法。&lt;/p>
&lt;p>&lt;strong>虽然这两种表示方法是等价的，但是它们的线性松弛 (linear relaxation) 是不一样的。&lt;/strong> 这也说明我们根据其线性松弛得到的近似解的效果也是不一样的。&lt;/p>
&lt;h3 id="packing-partitioning-covering">Packing, Partitioning, Covering&lt;/h3>
&lt;p>令 $E=\{1, 2, \dots, n\}$ 是一个集合，$\mathcal{F}=\{F_1, F_2, \dots, F_m\}$ 是 $E$ 的一族子集。由此我们可以定义一个 incidence matrix $A=[a_{ij}]^{m \times n}$ :
$$
a_{ij} = \begin{cases}
1 &amp;amp; \text{if } j \in F_i \\
0 &amp;amp; \text{otherwise}
\end{cases}
$$
例：$E=\{1, 2, 3\}, F = \left\{\{1, 2\}, \{1, 3\}, \{2, 3\}\right\}, A = \begin{pmatrix} 1 &amp;amp; 1 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 1 \\ 0 &amp;amp; 1 &amp;amp; 1 \end{pmatrix}$ .&lt;/p>
&lt;p>我们说 $E$ 的一个 packing, partitioning, covering 指的分别是如下集合：&lt;/p>
&lt;p>$$
\begin{aligned}
S^P(A) &amp;amp;= \{x \in \{0, 1\}^n : Ax \leq \mathbf{1}\} \\
S^T(A) &amp;amp;= \{x \in \{0, 1\}^n : Ax = \mathbf{1}\} \\
S^C(A) &amp;amp;= \{x \in \{0, 1\}^n : Ax \geq \mathbf{1}\} \\
\end{aligned}
$$&lt;/p>
&lt;p>由此我们可以得到 set packing / partitioning / covering 问题&lt;/p>
&lt;p>$$
\begin{aligned}
\text{set packing:} &amp;amp; \qquad \max \left \{\sum_{i=1}^{n}{w_i} x_i: x \in S^P \right\} \\
\text{set partitioning:} &amp;amp; \qquad \min \left \{\sum_{i=1}^{n}{w_i} x_i: x \in S^T \right\} \\
\text{set covering:} &amp;amp; \qquad \min \left \{\sum_{i=1}^{n}{w_i} x_i: x \in S^C \right\} \\
\end{aligned}
$$&lt;/p>
&lt;p>许多实际问题都可以建模成 set packing / covering 问题。&lt;/p>
&lt;h4 id="independent-set---set-packing">Independent set -&amp;gt; set packing&lt;/h4>
&lt;p>一个无向图 $G(V, E)$ 的独立集 (independent set, stable set, coclique) 是 $V$ 的一个不相邻子集，即 $V$ 的独立集的顶点互不相邻。&lt;/p>
&lt;p>如果我们把 $E=\{(i, j) \cdots\}$ 看成是 $G$ 的一个子集族，那么每一个独立集都是一个 set packing。&lt;/p>
&lt;h3 id="traveling-salesman-problem">Traveling Salesman Problem&lt;/h3></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E6%95%B4%E6%95%B0%E5%92%8C%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/" term="整数和组合优化" label="整数和组合优化"/><category scheme="https://allenz-me.github.io/tags/polyhedron/" term="Polyhedron" label="Polyhedron"/></entry><entry><title type="text">网络流的原始对偶算法</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/primal-dual-1/"/><id>https://allenz-me.github.io/posts/operations/primal-dual-1/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-04-03T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Primal-dual Method to Network Flow 原始对偶算法是解决组合优化问题的通法。最短路和最大流是网络流中最经典的两个……</summary><content type="html">&lt;h1 id="primal-dual-method-to-network-flow">Primal-dual Method to Network Flow&lt;/h1>
&lt;p>原始对偶算法是解决组合优化问题的通法。最短路和最大流是网络流中最经典的两个问题，它们都可以在原始对偶算法的框架下进行解决。&lt;/p>
&lt;p>网络流问题大都可以写成线性规划，不同的网络流问题有着不同的LP形式，但解决的思路是通用的。&lt;/p>
&lt;p>对于有向图 $(V, E)$，它的 &lt;strong>node-arc incidence matrix&lt;/strong> 是一个 $|V| \times |E|$ 的矩阵：
$$
A_{v, \,e} = \begin{cases}
+1, &amp;amp;\text{if arc } e \text{ starts at node } v \\
-1, &amp;amp;\text{if arc } e \text{ ends at node } v \\
0, &amp;amp;\text{otherwise}
\end{cases}
$$
矩阵的每一行对应于一个 vertex，每一列对应于一个 arc (edge)。&lt;/p>
&lt;img src="../../figures/primal-dual-1/image-20220403151941394.png" alt="image-20220403151941394" style="zoom:67%;" />
&lt;p>上图的点—弧关联矩阵是：
$$
\begin{aligned}
&amp;amp; \quad\;\, e_1 \quad\; e_2 \quad\;\, e_3 \quad\;\, e_4 \quad\;\, e_5 \quad\; e_6 \quad\;\, e_7 \quad\; e_8 \quad\; e_9 \\
A=&amp;amp;\left(\begin{array}{rrrrrrrrr}
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 \\
-1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; -1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; -1 &amp;amp; 0 &amp;amp; -1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 &amp;amp; -1
\end{array}\right)\; \begin{array}{r}
v_1 \\
v_2 \\
v_3 \\
v_4 \\
v_5
\end{array}
\end{aligned}
$$
在点 $i \in V$ 处的流量守恒 (flow conservation) 是：
$$
a_i^T f = 0
$$
其中 $f \in \mathrm{R}^{|E|}$，表示每条边的流量。&lt;/p>
&lt;p>如果 $a_i^T f &amp;gt; 0$，表示 $i$ 处有流量流出；如果 $a_i^T f &amp;lt; 0$，表示 $i$ 处有流量流入。&lt;/p>
&lt;p>&lt;strong>注意上面的点—弧关联矩阵是一个缺秩的矩阵，它有一个多余的约束。&lt;/strong> 这点也好理解，当你知道 $|V|-1$ 个点的流量的时候，你自然就知道剩下一个点的流量了。&lt;/p>
&lt;blockquote>
&lt;p>对于无向图，有 node-edge incidence matrix，只是矩阵的元素全是1。&lt;/p>
&lt;/blockquote>
&lt;h3 id="shortest-path">Shortest Path&lt;/h3>
&lt;p>容易写出，$s\to t$ 的最短路问题的流量守恒是：
$$
Af =\left[\begin{array}{c}
1\\
-1 \\
0 \\
\vdots \\
0 \\
\end{array} \right]\;
\begin{array}{l}
s: \text{a unit flow leaving} \\
t: \text{a unit flow entering}\\
\text{flow conservation} \\
\vdots\\
\text{flow conservation} \\
\end{array}
$$
从 $s$ 到 $t$ 的一条路可以设想为：一个单位流，从 $s$ 流向 $t$，这样的流必然满足：&lt;/p>
&lt;ul>
&lt;li>起点 $s$ 流出一个单位的流&lt;/li>
&lt;li>终点 $t$ 流入一个单位的流&lt;/li>
&lt;li>其余点流量守恒&lt;/li>
&lt;/ul>
&lt;p>由于 $A$ 是缺秩的，去掉任意一行约束都可以，但是我们一般删去关于 $t$ 的那一行，记矩阵 $\bar{A}$。最后得到最短路线性规划模型为：
$$
\begin{aligned}
\min_f \; \; &amp;amp; c^T f \\
\text{s.t. } &amp;amp; \bar{A} f = \begin{bmatrix}
1 \\
0 \\
\vdots \\
0 \\
\end{bmatrix}, \quad f \geq 0
\end{aligned} \tag{SP}
$$
其中 $c$ 是每条边的权重（费用），决策目标是最小化 $s$ 到 $t$ 的单位流的费用。&lt;/p>
&lt;p>&lt;strong>最短路问题的原问题，决策变量是每一条边，约束条件是每个点的流量守恒。&lt;/strong>&lt;/p>
&lt;h3 id="max-flow">Max Flow&lt;/h3>
&lt;p>假设从 $s$ 到 $t$ 有一条容量为 $\mathrm{v}$ 的流，记 $d = \begin{bmatrix}-1 &amp;amp; +1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \end{bmatrix}^T$，最大流的流量守恒表示为：
$$
Af + d\mathrm{v} = 0
$$
不难发现 $Af + d \mathrm{v} = 0 \Leftrightarrow Af + d\mathrm{v} \leq 0$ .&lt;/p>
&lt;p>因此最大流的数学规划模型为：&lt;/p>
&lt;p>$$
\begin{aligned}
\max_{f, \,\mathrm{v}} \;\; &amp;amp; \mathrm{v} \\
\text{s.t. } \; &amp;amp; Af + d \mathrm{v} \leq 0 \\
&amp;amp; f \leq b \\
&amp;amp; f \geq 0 \\
\end{aligned} \tag{MF}
$$
其中 $b$ 是每条边的容量上限。&lt;/p>
&lt;h3 id="primal-dual--shortest-path">Primal Dual — Shortest Path&lt;/h3>
&lt;p>最短路的对偶问题是：
$$
\begin{aligned}
\max_y \; &amp;amp; y_s - y_t \\
\text{s.t. } &amp;amp; y_i - y_j \leq c_{ij} \, , \;\; \forall (i, j) \in E \\
&amp;amp; y \text{ free}
\end{aligned}
$$
注意到对偶问题的决策变量是每个顶点，约束条件是每条边。&lt;/p>
&lt;p>因为原问题多了一个约束条件，所以对偶问题多了一个决策变量，通常我们令 $y_t = 0$ .
$$
\begin{aligned}
\max \; &amp;amp; y_s \\
\text{s.t. } &amp;amp; y_i - y_j \leq c_{ij} \, , \;\; \forall\, (i, j) \in E \\
&amp;amp; y \text{ free}, \;\; y_t= 0
\end{aligned} \tag{SP-D}
$$
下面介绍用原始对偶算法解决最短路问题的思路：&lt;/p>
&lt;p>从一个可行解 $y=0$ 出发，记 $J$ 是有效约束指标集，我们可以直接写出 DRP：
$$
\begin{aligned}
\max \; &amp;amp; y_s \\
\text{s.t. } &amp;amp; y_i - y_j \leq 0 \, , \;\; \forall\, (i, j) \in J \\
&amp;amp; y_i \leq 1, \;\; y_t=0
\end{aligned}
\tag{SP-DRP}
$$
一般原始对偶算法是解RP，但事实上，对最短路这个例子来说，解DRP是足够容易的！因为 $y_s$ 要么为 1 要么为 0 。&lt;/p>
&lt;ul>
&lt;li>如果 $J$ 中存在 $s \to t$ 的路径，那么可以让 $y_s = 0$ ，说明已经达到最优&lt;/li>
&lt;li>如果 $J$ 中不存在 $s \to t$ 的路径，那么 $y_s$ 最大可以取到 1，继续迭代&lt;/li>
&lt;/ul>
&lt;p>SP-DRP 的最优解可以直接根据 $J$ 和图的结构给出：
$$
\bar{y}_i = \begin{cases}
1, &amp;amp; i\, \text{ reachable from } s \text{ using arcs in } J \\
0, &amp;amp; i\, \text{ from which } t \text{ is reachable using arcs in } J \\
1, &amp;amp; \text{all other nodes}
\end{cases}
$$
记：
$$
\theta=\min _{\substack{(i, j) \notin J \\ y_{i}-y_{j}&amp;gt;0}}\left\{c_{i j}-\left(y_{i}-y_{j}\right)\right\}
$$
则 $y_{\text{new}} = y + \theta \bar{y}$，使 $\theta$ 最小的 $(i, j)$ 将进入到 $J_{\text{new}}$ 中。&lt;strong>这一过程的思想就是，不断向 $J$ 中添加边，直到 $J$ 包含一条从 $s$ 到 $t$ 的路径。&lt;/strong>&lt;/p>
&lt;p>于是，我们把解最短路问题转化为求解多个可达性子问题。算法终止当且仅当存在一条使用 $J$ 中的边的 $s\to t$ 的路径。&lt;/p>
&lt;p>严格来说，PD算法的有限终止性在这里并不适用，因为PD算法有限终止的前提是用单纯型法解RP问题。但是，finiteness follows from two simple observations about DRP:&lt;/p>
&lt;ul>
&lt;li>Once edge $(i, j)$ becomes admissible (enters $J$) it never leaves $J$ at any later stage.&lt;/li>
&lt;li>At every iteration of DRP, at least one new $(i, j)$, the one that deﬁnes $\theta$ , becomes admissible.&lt;/li>
&lt;/ul>
&lt;p>以上两点结合一个例子就能很轻易的理解了。&lt;/p>
&lt;p>对于图：&lt;/p>
&lt;img src="../../figures/primal-dual-1/image-20220415145459276.png" alt="image-20220415145459276" style="zoom:80%;" />
&lt;p>它的迭代过程是：&lt;/p>
&lt;img src="../../figures/primal-dual-1/image-20220415145540334.png" alt="image-20220415145540334" style="zoom:90%;" />
&lt;p>其实这个算法的本质就是Dijkstra算法，从 $t$ 点开始做动态规划，它是一个多项式时间的算法！&lt;/p>
&lt;h3 id="max-flow--min-cuts">Max Flow — Min cuts&lt;/h3>
&lt;p>一个割 (cut) 指的是 $V$ 的一个子集 $W$，割的容量 (capacity)指的是 $c(W, \overline{W})=\displaystyle\sum_{(i, j):\, i\in W, j \in \overline{W}} b_{ij}$&lt;/p>
&lt;p>$s-t$ cut 指的是一个子集 $W \subset V$ 使得 $s\in W, t \in \overline{W}$ .&lt;/p>
&lt;img src="../../figures/primal-dual-1/image-20220415163414591.png" alt="image-20220415163414591" style="zoom:50%;" />
&lt;p>最大流的原问题是：
$$
\begin{aligned}
\max_{f, \,\mathrm{v}} \;\; &amp;amp; \mathrm{v} \\
\text{s.t. } \; &amp;amp; Af + d \mathrm{v} = 0 \\
&amp;amp; f \leq b \\
&amp;amp; f \geq 0 \\
\end{aligned}
$$
引入对偶变量 $p_i \;(i\in V)$ 和 $\gamma_{ij} \; (i, j) \in E$，上面问题的对偶问题就是：
$$
\begin{aligned}
\min \; &amp;amp; \sum_{i, j) \in E} \gamma_{ij} b_{ij} \\
\text{s.t. } &amp;amp; p_i - p_j + \gamma_{ij} \geq 0 \\
&amp;amp; -p_s + p_t \geq 1 \\
&amp;amp; \gamma_{ij} \geq 0, \;\; p \text{ free }
\end{aligned}
$$
给定一个 $s-t$ cut $(W, \overline{W})$，定义：
$$
\gamma_{ij} = \begin{cases}
1 &amp;amp; \text{if } (i, j) \in E \text{ and } i \in W, j \in \overline{W} \\
0 &amp;amp; \text{otherwise} \\
\end{cases}, \qquad p_i = \begin{cases}
0 &amp;amp; i \in W \\
1 &amp;amp; i \in \overline{W} \\
\end{cases}
$$
可以确定一个对偶问题的可行解。这其实说明了最小割问题是最大流问题的上界。&lt;/p>
&lt;p>注意到：&lt;/p>
&lt;ul>
&lt;li>如果 $(i, j) \in E$ 并且 $i \in W, j \in \overline{W}$ ，则约束条件 $p_i - p_j - \gamma_{ij} = 0 \geq 0$ 是紧的&lt;/li>
&lt;li>如果 $(i, j) \in E$ 并且 $i \in\overline{W}, j \in W$ ，则约束条件 $p_i - p_j - \gamma_{ij} = 1 \geq 0$ 是严格成立的&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Max-flow min-cut theorem&lt;/strong>&lt;/p>
&lt;p>Max-flow = min-cut 如果互补松弛条件成立：
$$
\begin{aligned}
f_{ij} = 0 \quad &amp;amp; \forall (i, j) \in E \;\; \text{ s.t. } i \in \overline{W}, j \in W \\
f_{ij} = b \quad &amp;amp; \forall (i, j) \in E \;\; \text{ s.t. } i \in W, j \in \overline{W}
\end{aligned}
$$&lt;/p>
&lt;p>下图展示了如何从最大流的结果得到最小割：&lt;/p>
&lt;img src="../../figures/primal-dual-1/image-20220415164840986.png" alt="image-20220415164840986" style="zoom:50%;" />
&lt;p>最优性可以体现在找到一个割使得割的容量与流量相等。&lt;/p>
&lt;h3 id="primal-dual--max-flow">Primal Dual — Max Flow&lt;/h3>
&lt;p>对于最大流问题，我们可以直接把问题看成是某个问题的对偶问题：
$$
\begin{aligned}
\max_{f, \,\mathrm{v}} \;\; &amp;amp; \mathrm{v} \\
\text{s.t. } \; &amp;amp; Af + d \mathrm{v} \leq 0 \\
&amp;amp; f \leq b \\
&amp;amp; f \geq 0 \\
\end{aligned} \tag{MF-D}
$$
因为 $Af + d\mathrm{v} \leq 0$ 等号总是成立的，所以DRP是：
$$
\begin{array}{crl}
\max\; &amp;amp; \mathrm{v} \\
\text{s.t.} &amp;amp; A f+d \mathrm{v} &amp;amp; \leq 0 \quad \text { for all rows } \\
&amp;amp;f &amp;amp; \leq 0 \quad \text { for rows where } f=b \text { in } D \\
&amp;amp;-f &amp;amp; \leq 0 \quad \text { for rows where } f=0 \text { in } D \\
&amp;amp;f &amp;amp; \leq 1 \\
&amp;amp;\mathrm{v} &amp;amp; \leq 1
\end{array} \tag{MF-DRP}
$$
如果 DRP 的最优解是 0，那么这表明问题达到最优。&lt;/p>
&lt;p>令 $\mathrm{v}=1$，对 DRP 而言，这意味着存在一条从 $s\to t$ 的路 $P$，其饱和弧 (saturated arc, $f_e=b_e$) 必然是反向弧 (in backward direction)，其空弧 (unused arc, $f_e=0$) 必然是前向弧 (in forward direction)，其它弧可以是任意方向的。&lt;/p>
&lt;p>&lt;strong>这样的路我们叫做增广路 (augmenting path)。求解 DRP 就等价于找增广路。沿着增广路就可以增加网络的流量。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Augmenting path: an undirect path restricted to using an edge in forward direction if that edge is unsaturated and using an edge in backward direction if that edge had positive flow.&lt;/p>
&lt;/blockquote>
&lt;p>此时：
$$
\theta = \min_{(i, j) \in P}\begin{cases}
b_{ij} - f_{ij} &amp;amp; \forall (i, j) \text{ forward } \\
f_{ij} &amp;amp; \forall (i, j) \text{ backward }
\end{cases}
$$&lt;/p>
&lt;p>Ford-Fulkerson 算法的基本思路与通过多次求解DRP来找到最大流是一样的。&lt;/p>
&lt;p>&lt;strong>Ford-Fulkerson method&lt;/strong>&lt;/p>
&lt;p>Start with 0 ﬂow.&lt;/p>
&lt;p>While there exists an augmenting path:&lt;/p>
&lt;ul>
&lt;li>find an augmenting path&lt;/li>
&lt;li>compute bottleneck capacity&lt;/li>
&lt;li>increase flow on that path by bottleneck capacity&lt;/li>
&lt;/ul>
&lt;p>Ford-Fulkerson 方法值的并不是一个确定的算法，&lt;strong>增广路可能不是唯一的，怎么确定好的增广路是非常重要的。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Ford-Fulkerson Labeling Algorithm&lt;/strong>&lt;/p>
&lt;p>这是一个具体算法，由 Ford 和 Fulkerson 在1955年提出。&lt;/p>
&lt;p>例子在：https://www.doc88.com/p-16816167056590.html&lt;/p>
&lt;p>如果流量是有理数（整数），那么算法保证在有限步终止，时间复杂度为 $O(E^2U)$，其中 $U$ 表示边的最大容量。如果流量是无理数，那么算法可能不能在有限步终止。&lt;/p>
&lt;p>其它算法：&lt;/p>
&lt;p>&lt;img src="../../figures/primal-dual-1/image-20220415172612932.png" alt="image-20220415172612932">&lt;/p>
&lt;h2 id="minimum-cost-flow-problem">Minimum-cost flow problem&lt;/h2>
&lt;p>最小费用流问题是最短路问题和最大流问题的结合，&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E6%95%B4%E6%95%B0%E5%92%8C%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/" term="整数和组合优化" label="整数和组合优化"/><category scheme="https://allenz-me.github.io/tags/%E7%BD%91%E7%BB%9C%E6%B5%81/" term="网络流" label="网络流"/><category scheme="https://allenz-me.github.io/tags/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/" term="线性规划" label="线性规划"/></entry><entry><title type="text">A quantitative method for assessing resilience of interdependent infrastructures</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/16/"/><id>https://allenz-me.github.io/posts/papers/16/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-03-19T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Reliability Engineering and System Safety, 2017. DOI: https://doi.org/10.1016/j.ress.2016.08.013. Keywords: Interdependent critical infrastructure; Resilience; Reliability; Agent-based modeling; Interdependency 这篇文章提出了一个定量地评估系统韧性的方法。……</summary><content type="html">&lt;p>发表在 Reliability Engineering and System Safety, 2017. DOI: &lt;a href="https://doi.org/10.1016/j.ress.2016.08.013">https://doi.org/10.1016/j.ress.2016.08.013&lt;/a>.&lt;/p>
&lt;p>Keywords: Interdependent critical infrastructure; Resilience; Reliability; Agent-based modeling; Interdependency&lt;/p>
&lt;hr>
&lt;p>&lt;strong>这篇文章提出了一个定量地评估系统韧性的方法。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>In this paper, a quantitative method for the assessment of the system resilience is proposed. The method consists of two components: an integrated metric for system resilience quantiﬁcation and a hybrid modeling approach for representing the failure behavior of infrastructure systems.&lt;/p>
&lt;/blockquote>
&lt;p>韧性一词最早由生态学家 Holling 提出，他定义韧性 resilience: a measure of the persistence of systems and of their ability to absorb change and disturbance and still maintain the same relationships between populations or state variables. 随后韧性这个概念被用在各个领域中。在文章里，韧性指的是: the ability of a system to resist the effects of a disruptive force and to reduce performance deviations.&lt;/p>
&lt;p>系统韧性 (resilience capabilities) 可以分解为三种能力：&lt;/p>
&lt;ul>
&lt;li>Absorptive capability&lt;/li>
&lt;li>Adaptive capability&lt;/li>
&lt;li>Restorative capability&lt;/li>
&lt;/ul>
&lt;p>下图描述了这三种能力：&lt;/p>
&lt;img src="../../figures/16/image-20220409200059406.png" alt="image-20220409200059406" style="zoom:67%;" />
&lt;p>纵坐标 MOP 指的是 &lt;em>measurement of performance&lt;/em> ，一般是0-1之间的数值。&lt;/p>
&lt;p>接着文章给出了一些 resilience metrics。&lt;/p>
&lt;img src="../../figures/16/image-20220410180709960.png" alt="image-20220410180709960" style="zoom:67%;" />
&lt;ul>
&lt;li>
&lt;p>robustness $R = \min \{MOP(t) \text{ for } \mathrm{td} \leq t \leq \mathrm{tns}\}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>rapidity $R A P I_{D P}=\displaystyle\frac{M O P\left(t_{d}\right)-M O P\left(t_{r}\right)}{t_{r}-t_{d}}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>performance loss $\mathrm{PL}_{\mathrm{DP}}=\displaystyle\int_{t_{d}}^{t_{\mathrm{r}}}\left(\operatorname{MOP}\left(t_{\mathrm{o}}\right)-\operatorname{MOP}(t)\right) \mathrm{d} t$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>time averaged performance loss $T A P L_{D P}=\displaystyle\frac{\displaystyle\int_{t_{d}}^{t_{r}}\left(\operatorname{MOP}\left(t_{0}\right)-M O P(t)\right) \mathrm{d} t}{t_{r}-t_{d}}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>recovery ability $R A=\left|\displaystyle\frac{M O P\left(t_{n s}\right)-M O P\left(t_{r}\right)}{M O P\left(t_{o}\right)-M O P\left(t_{r}\right)}\right|$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Integrated resilience metric&lt;/strong>&lt;/p>
&lt;p>文章提炼出了一个整体的指标，用于综合评估系统韧性：
$$
\begin{aligned}
G R=&amp;amp; f\left(R, R A P I_{D P}, R A P I_{R P}, T A P L, R A\right)=R \times\left(\frac{R A P I_{R P}}{R A P I_{D P}}\right) \times(T A P L)^{-1} \times R A
\end{aligned}
$$
$GR$ 是无量纲的，越大代表系统韧性越强&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/></entry><entry><title type="text">Interior-point methods</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cvxopt/interior-point/"/><id>https://allenz-me.github.io/posts/cvxopt/interior-point/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-03-19T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">这是凸优化这本书的最后一章。 在之前已经介绍过了，如果凸优化问题只有等式约束，那么可以……</summary><content type="html">&lt;p>这是凸优化这本书的最后一章。&lt;/p>
&lt;p>在之前已经介绍过了，如果凸优化问题&lt;strong>只有等式约束&lt;/strong>，那么可以通过牛顿法（二阶近似）化为求解等式约束的二次规划问题，进而通过KKT条件或者消去等式约束的办法求解。&lt;/p>
&lt;p>这最后一章，解决的正是最后一个问题，如何&lt;strong>把混合约束（不等式+等式）的问题转化为只含有等式约束的凸优化问题&lt;/strong>。&lt;/p>
&lt;h2 id="inequality-constrained-minimization-problems">Inequality constrained minimization problems&lt;/h2>
&lt;p>本章讨论内点算法，适用于非常一般性的凸优化问题：
$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f_{0}(x) \\
\text {subject to } &amp;amp; f_{i}(x) \leq 0, \quad i=1, \ldots, m \\
&amp;amp; A x=b
\end{array}
$$
始终假设凸优化问题存在&lt;strong>严格可行&lt;/strong>的可行解。&lt;/p>
&lt;h2 id="logarithmic-barrier-function-and-central-path">Logarithmic barrier function and central path&lt;/h2>
&lt;p>我们的目标是希望消除问题中的不等式约束，障碍函数（barrier function）是一种约束函数值的函数，比如：
$$
I_{-}(u)=\left\{\begin{array}{ll}0 &amp;amp; u \leq 0 \\ \infty &amp;amp; u&amp;gt;0\end{array}\right.
$$&lt;/p>
&lt;p>将不等式约束的障碍函数加入到目标函数，可以得到一个等式约束的优化问题：
$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f_{0}(x)+\sum_{i=1}^{m} I_{-}\left(f_{i}(x)\right) \\
\text {subject to } &amp;amp; A x=b
\end{array}
$$
当可行解触及不等式约束的边界的时候，目标函数会剧烈的增大，这就要求，解要严格满足不等式约束条件。这就是内点法的基本思想！&lt;/p>
&lt;p>但是上面的这种障碍函数，加到目标函数里面，会严重破坏目标函数的光滑性，这就提示我们&lt;strong>选择一个光滑的、凸的障碍函数&lt;/strong>。&lt;/p>
&lt;h3 id="logarithmic-barrier">Logarithmic barrier&lt;/h3>
&lt;p>$$
\widehat{I}_{-}(u)=-(1 / t) \log (-u), \quad \operatorname{dom} \widehat{I}_{-}=-\mathbf{R}_{++}
$$&lt;/p>
&lt;p>$\hat{I}_{-}(u)$ 是一个凸函数，参数 $t$ 确定了它与 $I_{-}(u)$ 的近似程度。其函数图像为：&lt;/p>
&lt;img src="../../figures/Interior-point/1240.png" style="zoom:80%;" />
&lt;p>利用上面这个障碍函数，将原优化问题变成：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f_{0}(x)+\sum_{i=1}^{m}-(1 / t) \log \left(-f_{i}(x)\right) \\
\text{subject to } &amp;amp; A x=b
\end{array}
$$&lt;/p>
&lt;p>引入函数：$\phi(x)=-\sum_{i=1}^{m} \log \left(-f_{i}(x)\right)$，可以把上述问题写成：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; t f_{0}(x)+\phi(x) \\
\text {subject to } &amp;amp; A x=b
\end{array}
$$&lt;/p>
&lt;p>$t$ 越大，求解这个问题得到的最优解越接近原问题的最优解。但是，$t$ 越大，目标函数的 Hessian 矩阵在约束边界附近变动幅度越大，这里面就有一个trade-off。&lt;/p>
&lt;blockquote>
&lt;p>$\phi(x)$ 的梯度和 Hessian 矩阵分别是：
$$
\begin{aligned}
\nabla \phi(x) &amp;amp;=\sum_{i=1}^{m} \frac{1}{-f_{i}(x)} \nabla f_{i}(x) \\
\nabla^{2} \phi(x) &amp;amp;=\sum_{i=1}^{m} \frac{1}{f_{i}(x)^{2}} \nabla f_{i}(x) \nabla f_{i}(x)^{T}+\sum_{i=1}^{m} \frac{1}{-f_{i}(x)} \nabla^{2} f_{i}(x)
\end{aligned}
$$&lt;/p>
&lt;/blockquote>
&lt;h4 id="central-path">Central path&lt;/h4>
&lt;p>对 $t&amp;gt;0$，记函数 $x^\ast(t)$ 为给定 $t$ 值得到的近似问题的最优值，所谓 central path 就是这个函数的图像，图像上的每个点叫做 central point。随着 $t$ 的增大，$x\ast(t)$ 愈发接近最优解。&lt;/p>
&lt;h2 id="barrier-method">Barrier method&lt;/h2>
&lt;p>考虑到对一个很大的 $t$ 直接求解会存在这样那样的问题，我们可以先对一个较小的$t$求出对应的 $x^\ast$，再增大 $t$，同时以上次求出的最优解 $x^\ast$ 为初始点，再次进行迭代，最后可以得到一系列趋于最优解的点。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/" term="凸优化" label="凸优化"/></entry><entry><title type="text">线性规划的原始对偶算法</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/primal-dual-lp/"/><id>https://allenz-me.github.io/posts/operations/primal-dual-lp/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-03-19T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">线性规划的原始-对偶方法 (primal-dual method) 是设计组合优化问题算法的标准工具，它由 Dantzig、F……</summary><content type="html">&lt;p>线性规划的原始-对偶方法 (primal-dual method) 是设计组合优化问题算法的标准工具，它由 Dantzig、Ford 和 Fulkerson 提出，是一种多项式时间的算法。&lt;/p>
&lt;p>PD方法的思路是从一个对偶可行解出发，逐步找到满足互补松弛条件的一组解。
$$
(\mathbf{P})\left\{\begin{array} { l l }
{ \underset { \mathbf { x } \in \mathbb { R } ^ { n } } {\operatorname {minimize}} } &amp;amp; { \mathbf { c } ^ { \top } \mathbf { x } } \\
{ \text {subject to} } &amp;amp; { A \mathbf { x } = \mathbf { b } } \geq \mathbf{0} \\
{ } &amp;amp; { \mathbf { x } \geq \mathbf { 0 } }
\end{array} \qquad ( \mathbf { D } ) \left\{\begin{array}{ll}
\underset{\mathbf{u} \in \mathbb{R}^{m}}{\operatorname{maximize}} &amp;amp; \mathbf{u}^{\top} \mathbf{b} \\
\text {subject to} &amp;amp; \mathbf{u}^{\top} A \leq \mathbf{c}^{\top} \\
&amp;amp; \mathbf{u} \text { unrestricted }
\end{array}\right.\right.
$$
给定如上的原问题和对偶问题，$\mathbf{b} \geq \mathbf{0}$ 是一个 WLOG 但是有意义的条件。&lt;/p>
&lt;p>线性规划的 complementary slackness 给出了可行解 $(\bar{\mathbf{x}}, \bar{\mathbf{u}})$ 是一组最优解的充要条件：&lt;/p>
&lt;p>$$
\begin{align}
\bar{\mathbf{u}}^\top (A\bar{\mathbf{x}} - \mathbf{b}) = 0 \tag{1} \\
(\mathbf{A}^\top \bar{\mathbf{u}} - \mathbf{c})^\top \bar{\mathbf{x}} = 0 \tag{2}
\end{align}
$$&lt;/p>
&lt;p>(1)式由解的可行性自动满足，(2)式能够揭示 primal-dual 方法的一个思想：&lt;strong>对于一个对偶可行解，找到一个满足互补松弛条件、最接近可行的原始解&lt;/strong>；如果能找到一个满足互补松弛条件的、可行的原始解，说明这两组解都是最优的。&lt;/p>
&lt;p>以下阐述 primal-dual 方法的思路和步骤。&lt;/p>
&lt;p>首先，从一个对偶可行解 $\mathbf{u}$ 出发，我们希望找到一个方向 $\mathbf{v}$，使得从 $\mathbf{u}$ 出发沿着 $\mathbf{v}$ 方向前进一定距离，目标函数值 $\mathbf{u}^T \mathbf{b}$ 能得到提升。&lt;/p>
&lt;p>记指标集 $J = \{j : \mathbf{u}^\top A_j = c_j\}$ 表示紧的约束，用 $J^C$ 表示不紧的约束。&lt;/p>
&lt;p>对于 $J$ 对应的约束，方向 $\mathbf{v}$ 必须满足 $\mathbf{v}^\top A_J \leq \mathbf{0}^\top$，这样才能保证存在 $t &amp;gt; 0$ 使得 $\mathbf{u} + t \mathbf{v}$ 依然是对偶可行的；对于其它约束，则并没有这样的限制。当然，必须谨记我们要找的是一个方向，此时 $\mathbf{v}$ 或者 $2 \mathbf{v}$ 都可以是我们的要的解，所以我们需要 normalize 一下。非线性规划里面，我们可以选择 $\| \mathbf{v} \|_2 \leq 1$ 作为 normalization constraint，但是在线性规划里面，选择 $\| \mathbf{v} \|_\infty \leq 1 \Rightarrow v_i \leq 1$ 是一个更好的选择！&lt;/p>
&lt;p>由此我们得到寻找方向 $\mathbf{v}$ 的线性规划问题：
$$
\begin{array}{ll}
{\operatorname{maximize}} &amp;amp; \mathbf{v}^{\top} \mathbf{b} \\
\text {subject to } &amp;amp; \mathbf{v}^{\top} A_{J} \leq \mathbf{0}^{\top} \\
&amp;amp; v_{1}, \ldots, v_{m} \leq 1
\end{array} \tag{DRP}
$$
从这里可以看出 $\mathbf{b} \geq \mathbf{0}$ 的意义，normalization constraint $v_i \leq 1$ 是为了防止 $\mathbf{v}$ 无限增大，而那些小于0的 $v_i$ 趋于负无穷并不会增加目标函数值，所以没有必要添加相应的约束了。&lt;/p>
&lt;p>这个问题我们把它叫做 dual restricted program (DRP)&lt;/p>
&lt;ul>
&lt;li>如果 DRP 的最优值是0，这意味着对偶问题(D)达到最优，因为沿着任何可行方向都不能增加目标函数值了。&lt;/li>
&lt;li>如果 DRP 的最优值大于0，这说明存在一个足够小的 $t&amp;gt;0$，使得 $\mathbf{u}$ 沿着 DRP 的解 $\mathbf{v}$ 前进一段距离能保持对偶可行性同时提升函数值！现在我们要找到最大的 $t$，$\mathbf{u} + t \mathbf{v}$ 必须要保证可行性，对于 指标集 $J$ 对应的约束，可行性是一定满足的。
&lt;ul>
&lt;li>如果 $\mathbf{v}^\top A_j \leq 0 , \; \forall j \notin J$ ，那么 $\mathbf{u}$ 沿着 $\mathbf{v}$ 前进多少都不会违背可行性，这说明对偶问题(D)是无上界的，从而原问题没有可行解。&lt;/li>
&lt;li>如果 $\exists j \notin J, \;\mathbf{v}^\top A_j &amp;gt; 0$，那么可以选择的最大的 $t$ 是 $\min\limits_{j \notin J,\, \mathbf{v}^\top A_j &amp;gt; 0} \displaystyle\frac{c_j - \mathbf{u}^\top A_j}{\mathbf{v}^\top A_j}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这样，我们可以更新对偶可行解 $\mathbf{u}_{\text{new}} = \mathbf{u} + t \mathbf{v}$ ，这就是一步迭代。&lt;/p>
&lt;p>继续这么做，就能不断逼近最优解。&lt;strong>但是&lt;/strong>，这么做是很费力的，因为要不断地解一个新的线性规划。&lt;/p>
&lt;p>注意到(DRP)的对偶问题是：
$$
\begin{array}{ll}
\underset{\mathbf{x} \in \mathbb{R}^{n}, \mathbf{y} \in \mathbb{R}^{m}}{\operatorname{minimize}} &amp;amp; y_{1}+\cdots+y_{m} \\
\text {subject to } &amp;amp; A_{J} \mathbf{x}_J+\mathbf{I}\mathbf{y}=\mathbf{b} \\
&amp;amp; \mathbf{x}, \mathbf{y} \geq \mathbf{0}
\end{array} \tag{RP}
$$
我们把它称作是 restricted primal (RP)，它暗含了 $\mathbf{x}_{J^C} = \mathbf{0}$. (RP) 输出一个满足互补松弛条件、并把不可行性降到最低的 $\mathbf{x}$ . （注意到 $\mathbf{b} \geq 0$ 使得 (RP) 直接有一个初始解，这是它的第二个意义。）&lt;/p>
&lt;p>(RP) 可以用单纯型法高效地求解，同时因为单纯型法的优点，我们同时还能得到 (DRP) 的解。&lt;/p>
&lt;p>乍一看，求解 (RP) 跟求解 DRP 的计算量差不多啊，但是，&lt;strong>原始对偶算法的精髓就在于，在每一次迭代，都可以由前一次迭代得到的最优解开始求解 (RP)&lt;/strong>，&lt;/p>
&lt;p>令 $\mathcal{B}$ 是 (RP) 一步迭代中的最优基，$\mathcal{B}\subseteq J \cup [m]$，则 $\mathcal{B}\cap J \subseteq J_{\text{new}}$. 这意味着如果 $j \in \mathcal{B} \cap J$，那么 $j \subseteq J_{\text{new}}$. （证明是容易的，这里省略了）&lt;/p>
&lt;p>在前一次迭代中非零的 $x, y$ ，可以直接放在下一次迭代的单纯型表中！如果我们用的是 revised simplex，那么直接拿之前的 $B^{-1}$ 作为下次迭代的 $B^{-1}$，&lt;strong>这就节省了大量的计算&lt;/strong>！&lt;/p>
&lt;blockquote>
&lt;p>值得一提的是根据单纯型法的理论，原问题最优解 $B^{-1} b$，对偶问题最优解 $c_B^T B^{-1}$，最优值 $c_B^T B^{-1}b$ .&lt;/p>
&lt;/blockquote>
&lt;p>让我们来总结一下 PD 算法的步骤：&lt;/p>
&lt;ol>
&lt;li>找到一个对偶可行解 $\mathbf{u}$，确定指标集 $J = \{j : \mathbf{u}^\top A_j = c_j\}$&lt;/li>
&lt;li>用单纯型法求解 RP，得到 RP 的最优基变量、最优解、最优值和 DRP 的最优解 $\mathbf{v}$&lt;/li>
&lt;li>如果 step2 的最优值等于0，说明达到最优，输出 RP 的最优解和对偶可行解 $\mathbf{u}$；如果 step2 的最优值大于0：&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>如果 $\mathbf{v}^\top A_j \leq 0 , \; \forall j \notin J$ ，那么原问题没有可行解&lt;/li>
&lt;li>如果 $\exists j \notin J, \;\mathbf{v}^\top A_j &amp;gt; 0$，计算 $t=\min\limits_{j \notin J,\, \mathbf{v}^\top A_j &amp;gt; 0} \displaystyle\frac{c_j - \mathbf{u}^\top A_j}{\mathbf{v}^\top A_j}$，重新确定指标集 $J$，回到 step2，复用之前的最优基变量和最优解进行单纯型迭代。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>最后，简单说明一下算法的有限终止性。假定问题是有解并且是非退化的 (nondegenerate)&lt;/p>
&lt;p>假设第三步中 $j_0=\underset{{j \notin J,\, v_j^\top A &amp;gt; 0}}{\operatorname{argmin}} \displaystyle\frac{c_j - u_j^\top A}{v_j^\top A}$，注意到 $j_0 \notin J$ 但是 $j_0 \in J_{\text{new}}$，在将要开始的 RP 的单纯型迭代中，$x_{j_0}$ 的 reduced cost 是 $\bar{c}_{j_0} = 0 - \mathbf{v}^\top A_{j_0} &amp;lt; 0$，这说明下一步迭代 RP 能有严格的提升(也就是说函数值会严格下降)，因为多面体的 extreme point 的个数是有限的，&lt;strong>我们至多解有限次RP（子问题）&lt;/strong>，因此算法必然在有限步终止。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E6%95%B4%E6%95%B0%E5%92%8C%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/" term="整数和组合优化" label="整数和组合优化"/><category scheme="https://allenz-me.github.io/tags/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/" term="线性规划" label="线性规划"/></entry><entry><title type="text">From Predictive to Prescriptive Analytics</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/15/"/><id>https://allenz-me.github.io/posts/papers/15/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-03-11T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Management Science, 2020. DOI: https://doi.org/10.1287/mnsc.2018.3253. Area of review: optimization. Keywords: data-driven decision making • machine learning • stochastic optimization We combine ideas from machine learning (ML) and operations research and management science (OR/MS) in developing a framework, along with……</summary><content type="html">&lt;p>发表在 Management Science, 2020. DOI: &lt;a href="https://doi.org/10.1287/mnsc.2018.3253">https://doi.org/10.1287/mnsc.2018.3253&lt;/a>.&lt;/p>
&lt;p>Area of review: optimization.&lt;/p>
&lt;p>Keywords: data-driven decision making • machine learning • stochastic optimization&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>We combine ideas from machine learning (ML) and operations research and management science (OR/MS) in developing a framework, along with speciﬁc methods, for using data to prescribe optimal decisions in OR/MS problems.&lt;/p>
&lt;p>We demonstrate how our proposed methods are generally applicable to a wide range of decision problems and prove that they are computationally tractable and asymptotically optimal under mild conditions, even when data are not independent and identically distributed and for censored observations.&lt;/p>
&lt;/blockquote>
&lt;p>【未完待续】&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/ms/" term="MS" label="MS"/><category scheme="https://allenz-me.github.io/tags/prescriptive-analytics/" term="Prescriptive Analytics" label="Prescriptive Analytics"/></entry><entry><title type="text">Data-Driven Dynamic Pricing and Ordering with Perishable Inventory in a Changing Environment</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/14/"/><id>https://allenz-me.github.io/posts/papers/14/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-03-10T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Management Science, 2022. DOI: https://doi.org/10.1287/mnsc.2021.4011. Area of review: Management Science Special Section on Data-Driven Prescriptive Analytics. Keywords: dynamic pricing; inventory control; perishable inventory; nonstationary environment; data-driven analysis; estimation; exploration-exploitation 这篇文章的内容非常之……</summary><content type="html">&lt;p>发表在 Management Science, 2022. DOI: &lt;a href="https://doi.org/10.1287/mnsc.2021.4011">https://doi.org/10.1287/mnsc.2021.4011&lt;/a>.&lt;/p>
&lt;p>Area of review: Management Science Special Section on Data-Driven Prescriptive Analytics.&lt;/p>
&lt;p>Keywords: dynamic pricing; inventory control; perishable inventory; nonstationary environment; data-driven analysis; estimation; exploration-exploitation&lt;/p>
&lt;hr>
&lt;p>这篇文章的内容非常之多，因为文章考虑的东西很多，joint pricing and inventory、perishable、changing environment、data-driven .&lt;/p>
&lt;p>文章有三个主要贡献：&lt;/p>
&lt;ol>
&lt;li>Formulating a Model Motivated by Observations on Real-Life Data&lt;/li>
&lt;li>Theoretical Analysis: Deriving Rate-Optimal Regret Bounds&lt;/li>
&lt;li>Data-Driven Case Study: Managerial Insights for Practice&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Furthermore, our analysis sheds light on the value of accounting for inventory perishability and changing environments in pricing and inventory decisions.&lt;/p>
&lt;/blockquote>
&lt;p>首先文章结合实际数据（生鲜销售），指出 demand-pricing 的关系会随着时间变化（可能的原因有 pandemic, weather, technology 等&lt;/p>
&lt;img src="../../figures/14/image-20220311153047482.png" alt="" style="zoom: 41%;" />
&lt;p>接着又指出易逝品的损坏率是随机的。&lt;/p>
&lt;img src="../../figures/14/image-20220311153215724.png" alt="" style="zoom:40%;" />
&lt;p>最后还指出了 demand noise 可能是非参数的。&lt;/p>
&lt;p>文章设计了两种 &lt;strong>data-driven pricing and ordering (DDPO)&lt;/strong> 策略，一种针对 nonparametric demand noise，另一种针对 exponential-family demand noise，两者的 regret 分别为：$O\left(T^{{2 / 3}}(\log T)^{1 / 2}\right)$ and $O\left(T^{1 / 2} \log T\right)$，这说明，如果未知的东西能参数化，能使问题更简单。&lt;/p>
&lt;p>基本设定，在 $t = 1, \dots, T$ 时期内：&lt;/p>
&lt;ol>
&lt;li>期初观测到库存 $x_t$&lt;/li>
&lt;li>选择价格 $p_t \in [p_\min, p_\max]=\mathscr{P}$，以及 order-up-to level $y_t \in [y_\min, y_\max]=\mathscr{Y}$&lt;/li>
&lt;li>lead-time 为0. (overnight delivery) $q_t$ 比例的商品损坏/腐烂。&lt;/li>
&lt;li>需求 $D_t$ observable&lt;/li>
&lt;li>期末库存水平 $x_{t+1}=\left[\left(1-q_{t}\right) y_{t}-D_{t}\right]^{+}$&lt;/li>
&lt;/ol>
&lt;p>需求是价格的函数：
$$
\begin{aligned}
D_{t}=&amp;amp;g\left(\alpha_{t}+\beta_{t} p_{t}\right)+\varepsilon_{t} \text { for } t=1,2, \ldots \\
=&amp;amp; g\left(\boldsymbol{X}_{t}^{\top} \boldsymbol{\theta}_{t}\right)+\varepsilon_{t} \text { for } t=1,2, \ldots
\end{aligned}
$$
记 $\boldsymbol{X}_{t}=\left(1, p_{t}\right)^{\top}$ and $\boldsymbol{\theta}_{t}=\left(\alpha_{t}, \beta_{t}\right)^{\top}$&lt;/p>
&lt;p>参数 $\boldsymbol{\theta}(t)$ 会在几个时间点内发生变化，但是决策者并不知道准确的时刻。&lt;/p>
&lt;p>腐烂率 $q_t$ 服从参数 $\boldsymbol{\xi} = (\lambda, \nu)$ 的 beta 分布。&lt;/p>
&lt;h4 id="ddpo-policy">DDPO Policy&lt;/h4>
&lt;p>以 full-information anticipatory policy 为 benchmark，一个 admissible policy 指的是 $\pi_t : \mathbf{I}_t \to \mathscr{P} \times \mathscr{Y}$&lt;/p>
&lt;p>评价标准是 &lt;em>regret / profit loss&lt;/em>
$$
\Delta_{\boldsymbol{\theta}, \boldsymbol{\xi}}^{\pi}(T)=\mathbb{E}_{\boldsymbol{\theta}, \boldsymbol{\xi}}^{\pi}\left[\sum_{t=1}^{T}\left(Q\left(p_{t}^{\ast}, y_{t}^{\ast} ; \boldsymbol{\theta}_{t}, \boldsymbol{\xi}\right)-Q\left(p_{t}^{\pi}, y_{t}^{\pi} ; \boldsymbol{\theta}_{t}, \boldsymbol{\xi}\right)\right)\right]
$$&lt;/p>
&lt;p>考虑以下两种 setting&lt;/p>
&lt;ul>
&lt;li>Setting N: The demand noise distribution $F_{\varepsilon}$ does not necessarily bear a parametric form.&lt;/li>
&lt;li>Setting E: The demand noise distribution $F_{\varepsilon}$ is known to belong to the exponential family of distributions with a continuous density.&lt;/li>
&lt;/ul>
&lt;p>对于两种 setting，文章分别给出了 DDPO-N 和 DDPO-E 两种 policy.&lt;/p>
&lt;p>首先把时间划分成若干段：$\tau=0,1, \ldots,\lfloor T / n\rfloor$&lt;/p>
&lt;p>【未完待续】&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/ms/" term="MS" label="MS"/><category scheme="https://allenz-me.github.io/tags/inventory/" term="Inventory" label="Inventory"/></entry><entry><title type="text">多面体与线性不等式组</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/polyhedron-theory/"/><id>https://allenz-me.github.io/posts/operations/polyhedron-theory/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-03-07T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">多面体与线性不等式 $Ax \leq b$ 有着紧密的联系，围绕这它们，主要有两个研究问题： 系统 $Ax \leq b$ 是……</summary><content type="html">&lt;p>多面体与线性不等式 $Ax \leq b$ 有着紧密的联系，围绕这它们，主要有两个研究问题：&lt;/p>
&lt;ol>
&lt;li>系统 $Ax \leq b$ 是否有解&lt;/li>
&lt;li>系统 $Ax \leq b$ 的几何性质&lt;/li>
&lt;/ol>
&lt;h3 id="fourier-elimination">Fourier Elimination&lt;/h3>
&lt;p>Fourier 最先设计了一种类似高斯消元的方法来研究 $Ax\leq b$ 的有解性。&lt;/p>
&lt;p>它的思想可以理解为：给出 $(\bar{x}_1, \bar{x}_2, \dots, \bar{x}_{n-1})$，我们是否能找到一个 $\bar{x}_n$ ，使得 $(\bar{x}_1, \dots, \bar{x}_n)$ 是 $Ax\leq b$ 的解 。&lt;/p>
&lt;p>令 $I=\{1, \ldots, m\}$，引入记号：&lt;/p>
&lt;p>$$
I^{+}:=\left\{i \in I: a_{i n}&amp;gt;0\right\}, \quad I^{-}:=\left\{i \in I: a_{i n}&amp;lt;0\right\}, \quad I^{0}:=\left\{i \in I: a_{i n}=0\right\}
$$&lt;/p>
&lt;p>则：&lt;/p>
&lt;p>$$
Ax \leq b \quad \Longleftrightarrow \quad\begin{array}{llll}
\sum_{j=1}^{n-1} a_{i j}^{\prime} x_{j} &amp;amp; +x_{n} &amp;amp; \leq b_{i}^{\prime}, &amp;amp; i \in I^{+} \\
\sum_{j=1}^{n-1} a_{i j}^{\prime} x_{j} &amp;amp; -x_{n} &amp;amp; \leq b_{i}^{\prime}, &amp;amp; i \in I^{-} \\
\sum_{j=1}^{n-1} a_{i j} x_{j} &amp;amp; &amp;amp; \leq b_{i}, &amp;amp; i \in I^{0}
\end{array} \tag{1}
$$&lt;/p>
&lt;p>其中 $a_{i j}^{\prime}=a_{i j} /\left|a_{i n}\right|, b_{i}^{\prime}=b_{i} /\left|a_{i n}\right|, i \in I^{+} \cup I^{-}$ .&lt;/p>
&lt;p>$I^{+}, I^{-}$ 里的不等式分别相加，移去 $|I^{+}| + |I^{-}|$ 个并增加 $|I^+| \times |I^-|$ 个不等式，可以得到：&lt;/p>
&lt;p>$$
\begin{array}{l}
\sum_{j=1}^{n-1}\left(a_{i j}^{\prime}+a_{k j}^{\prime}\right) x_{j} &amp;amp; \leq b_{i}^{\prime}+b_{k}^{\prime}, &amp;amp; i \in I^{+}, k \in I^{-} \\
\sum_{j=1}^{n-1} a_{i j} x_{j} &amp;amp; \leq b_{i}, &amp;amp; i \in I^{0}
\end{array} \tag{2}
$$&lt;/p>
&lt;p>现在我们可以回答上面那个问题了，$(\bar{x}_1, \bar{x}_2, \dots, \bar{x}_{n-1})$ 满足(2) 当且仅当 存在 $\bar{x}_n$，使得 $(\bar{x}_1, \dots, \bar{x}_n)$ 是 $Ax\leq b$ 的解。&lt;/p>
&lt;p>令&lt;/p>
&lt;p>$$
l= \begin{cases} \displaystyle\max _{k \in I^{-}}\left\{\sum_{j=1}^{n-1} a_{k j}^{\prime} \bar{x}_{j}-b_{k}^{\prime}\right\}, &amp;amp; I^- \neq \emptyset \\
- \infty , &amp;amp; I^- = \emptyset
\end{cases}\;\, \qquad
u= \begin{cases} \displaystyle\min _{i \in I^{+}}\left\{b_{i}^{\prime}-\sum_{j=1}^{n-1} a_{i j}^{\prime} \bar{x}_{j}\right\}, &amp;amp; I^+ \neq \emptyset \\
+ \infty, &amp;amp; I^+ = \emptyset
\end{cases}
$$&lt;/p>
&lt;p>$l \leq \bar{x}_n \leq u$ 就是满足条件的数！&lt;/p>
&lt;p>这意味着 $n$ 个线性不等式组 $A^n x \leq b^n$ 的有解性可以等价于一个 $n-1$ 个线性不等式组 $A^{n-1}x \leq b^{n-1}$ 的有解性。所以 Fourier elimination 的思路就是不断递推到 $A^1 x \leq b^1$ ，但是每一次迭代都有可能加入非常多的新的不等式，算法执行起来可能需要指数时间。&lt;/p>
&lt;p>实际上，这种算法也可以看成是投影，多面体 $P$ 非空，当且仅当它在一个低一维的子空间上的投影非空。&lt;/p>
&lt;img src="../../figures/polyhedron-theory/image-20220402141107763.png" alt="image-20220402141107763" style="zoom:55%;" />
&lt;h3 id="polyhedron">Polyhedron&lt;/h3>
&lt;p>线性规划的可行域 $P$ 是一个多面体，混合整数线性规划有时候可以转换成多面体上的线性优化。因此，多面体组合学是线性规划，包括整数线性规划在内的组合优化的一个重要理论。&lt;/p>
&lt;p>&lt;strong>Polyhedron&lt;/strong>
$$
P:=\left\{x \in \mathbb{R}^{n}: A x \leq b\right\}
$$&lt;/p>
&lt;p>polyhedron 的复数形式是 polyhedra，bounded polyhedron 称为 polytope。&lt;/p>
&lt;p>&lt;strong>Rational polyhedron&lt;/strong>&lt;/p>
&lt;p>如果 $A \in \mathbb{Q}^{m\times n}, b \in \mathbb{Q}^m$ 都是有理数。&lt;/p>
&lt;p>&lt;strong>Polyhedral cone&lt;/strong>&lt;/p>
&lt;p>$$
C:=\left\{x \in \mathbb{R}^{n}: A x \leq 0\right\}
$$&lt;/p>
&lt;p>既是多面体又是锥。&lt;/p>
&lt;h3 id="minkowskiweyl-theorem">Minkowski–Weyl Theorem&lt;/h3>
&lt;p>Minkowski–Weyl 定理给出了多面体的一个刻画。&lt;/p>
&lt;p>一个集合 $C$ 是有限生成的 (finitely generated)，如果它是有限个点的凸锥包。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>A subset of $\mathbb{R}^n$ is a ﬁnitely generated cone if and only if it is a polyhedral cone&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>这句话说明 polyhedral cone 有两种表示方法：&lt;/p>
&lt;p>$$
C = \left\{x \mid A x \leq 0\right\} = \{By \mid y \geq 0\}
$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>A subset $P$ of $\mathbb{R}^{n}$ is a polyhedron if and only if $P=Q+C$ for some polytope $Q \subset \mathbb{R}^{n}$ and finitely generated cone $C \subseteq \mathbb{R}^{n}$.&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>这句话说明了一个 polyhedron 的表达方式：&lt;/p>
&lt;p>$$
P=\operatorname{conv}\left(v^{1}, \ldots, v^{p}\right)+\operatorname{cone}\left(r^{1}, \ldots, r^{q}\right)
$$&lt;/p>
&lt;blockquote>
&lt;p>参考：https://scaron.info/robotics/polyhedra-and-polytopes.html&lt;/p>
&lt;/blockquote>
&lt;p>图示如下：&lt;/p>
&lt;img src="../../figures/polyhedron-theory/image-20220320151845791.png" alt="" style="zoom:67%;" />
&lt;p>一个多面体 = 极点的凸包 + 极线的锥包&lt;/p>
&lt;h3 id="lineality-space-and-recession-cone">Lineality Space and Recession Cone&lt;/h3>
&lt;p>定义 polyhedron $P$ 的 recession cone 是：&lt;/p>
&lt;p>$$
\operatorname{rec}(P):=\left\{r \in \mathbb{R}^{n}: x+\lambda r \in P \text { for all } x \in P \text { and } \lambda \in \mathbb{R}_{+}\right\}
$$&lt;/p>
&lt;p>定义 polyhedron $P$ 的 lineality space 是：&lt;/p>
&lt;p>$$
\operatorname{lin}(P):=\left\{r \in \mathbb{R}^{n}: x+\lambda r \in P \text { for all } x \in P \text { and } \lambda \in \mathbb{R}\right\} \text {. }
$$&lt;/p>
&lt;p>注意到 $\operatorname{lin}(P)=\operatorname{rec}(P) \cap-\operatorname{rec}(P)$，当 $\operatorname{lin}(P)=\{0\}$ 时，称多面体 $P$ 是 pointed.&lt;/p>
&lt;p>直观上看，a nonempty polyhedron is pointed when it does not contain any line. $d \in \operatorname{lin}(P)$ 当且仅当对任意的 $x \in P$，直线 $\{x + \alpha d \mid \alpha \in \mathrm{R}\}$ 被包含在 $P$ 内。&lt;/p>
&lt;p>如果 $P:=\left\{x \in \mathbb{R}^{n}: A x \leq b\right\}=\operatorname{conv}\left(v^{1}, \ldots, v^{p}\right)+ \operatorname{cone}\left(r^{1}, \ldots, r^{q}\right)$ 非空，那么&lt;/p>
&lt;p>$$
\begin{aligned}
\operatorname{rec}(P)&amp;amp;=\left\{r \in \mathbb{R}^{n}: A r \leq 0\right\}=\operatorname{cone}\left(r^{1}, \ldots, r^{q}\right) \\
\operatorname{lin}(P)&amp;amp;=\left\{r \in \mathbb{R}^{n}: A r=0\right\} = \operatorname{ker}(A) \\
\end{aligned}
$$&lt;/p>
&lt;h3 id="implicit-equalities">Implicit Equalities&lt;/h3>
&lt;p>多面体 $P=\left\{x \in \mathbb{R}^{n}: A x \leq b\right\}$ 是由一组不等式系统 $a_i^T x \leq b_i, \, i \in M$ 组成的，称不等式 $a_i^T x \leq b_i$ 是一个 implicit equality，如果 $P$ 被包含在超平面 $\{x \mid a_i^T x = b_i\}$ 中。&lt;/p>
&lt;p>记 $M^{=}=\left\{i \in M \mid a^T_{i} x=b_{i},\, \forall x \in P\right\}$，$M^{&amp;lt;}=\left\{i \in M \mid a^T_{i} x&amp;lt;b_{i},\, \exists x \in P\right\}$，并引入记号 $(A^=, b^=), \, (A^&amp;lt;, b^&amp;lt;)$ 使得：&lt;/p>
&lt;p>$$
P=\left\{x \in \mathbb{R}^{n}: A^{=} x \leq b^{=}, A^{&amp;lt;} x \leq b^{&amp;lt;}\right\}=\left\{x \in \mathbb{R}^{n}: A^{=} x=b^{=}, A^{&amp;lt;} x \leq b^{&amp;lt;}\right\}
$$&lt;/p>
&lt;ul>
&lt;li>
&lt;p>称 $x$ 是 $P$ 的 &lt;em>interior point&lt;/em>，如果 $a_i^T x &amp;lt; b_i \, \text{ for all }\, i \in M$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>称 $x$ 是 $P$ 的 &lt;em>inner point&lt;/em>，如果 $a_i^T x &amp;lt; b_i \, \text{ for all }\, i \in M^{&amp;lt;}$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>显然，一个多面体可能没有 interior point。但是，可以证明非空的多面体一定有 inner point。&lt;/p>
&lt;p>根据 $M^{&amp;lt;}$ 的定义，$\forall i \in M^{&amp;lt;}$，都存在 $x_i \in P$ 使得 $a_i^T x_i &amp;lt; b_i$，由于多面体是凸集，不难发现 $\bar{x}:=\displaystyle\frac{1}{\left|M^{&amp;lt;}\right|} \sum_{i \in M^{&amp;lt;}} x_{i}$ 是一个满足条件的点。&lt;/p>
&lt;p>由此不难证明：
$$
\operatorname{aff}(P)=\left\{x \in \mathbb{R}^{n}: A^{=} x=b^{=}\right\}=\left\{x \in \mathbb{R}^{n}: A^{=} x \leq b^{=}\right\}
$$
于是，我们有 $\operatorname{dim}(P)+\operatorname{rank}\left(A^{=}\right)=n$ .&lt;/p>
&lt;p>称一个多面体 $P \subseteq \mathbb{R}^n$ full-dimensional，如果它有一个内点。这意味着 $\operatorname{dim} P=n$ ，也意味这 $P$ 没有 implicit equality。&lt;/p>
&lt;p>例：
$$
P:= \left\{x \in \mathbb{R}^{n^2}: \quad \begin{aligned}
\sum_{j=1}^{n} x_{i j} &amp;amp;=1, \quad i=1, \ldots n \\
\sum_{i=1}^{n} x_{i j} &amp;amp;=1, \quad j=1, \ldots n \\
x_{i j} &amp;amp; \geq 0, \quad i, j=1, \ldots n
\end{aligned} \,\right\}
$$
的维数是 $n^2-2n+1$ .&lt;/p>
&lt;h3 id="faces">Faces&lt;/h3>
&lt;p>一个不等式 $c^T x \leq \delta$ 称作对 $P$ 有效 (valid)，如果 $c^T x \leq \delta, \, \forall x \in P$ .&lt;/p>
&lt;p>有效性可以通过一组线性系统的有解性来判断：
$$
c^T x \leq \delta, \, \forall x \in P = \{x \mid Ax \leq b\} \Leftrightarrow \exists u \geq 0, \,\text{s.t.} \; u^TA=c^T, u^Tb \leq \delta
$$
上面的命题可以通过线性规划的对偶原理轻松证明。&lt;/p>
&lt;p>多面体 $P$ 的一个 &lt;strong>face&lt;/strong>，定义为：
$$
F:=P \cap\left\{x \in \mathbb{R}^{n}: c^Tx=\delta\right\}
$$
其中 $c^Tx \leq \delta$ 是一个 valid inequality for $P$ .&lt;/p>
&lt;p>如果这样定义的 $F$ 非空，就称超平面 $\{x \mid c^T x = \delta\}$ 是 $P$ 的支撑超平面。&lt;/p>
&lt;p>$\emptyset$ 和 $P$ 本身都是 $P$ 的 face，称 $P$ 的其它 face 都是 &lt;em>proper&lt;/em> 的。&lt;/p>
&lt;p>&lt;strong>直观上理解，face 就是多面体与它的一个支撑超平面的交。&lt;/strong>&lt;/p>
&lt;h5 id="characterization-of-the-faces">Characterization of the Faces&lt;/h5>
&lt;p>非空集合 $F$ 是 $P$ 的 face 当且仅当，对某个 $I \subseteq M$， $F$ 可以写成：&lt;/p>
&lt;p>$$
F =\left\{x \in \mathbb{R}^{n}: a_{i}^T x=b_{i}, \,i \in I, a_{i}^T x \leq b_{i}, \,i \in M \backslash I\right\}
$$&lt;/p>
&lt;p>多面体 $P$ 的 face 有以下性质：&lt;/p>
&lt;ul>
&lt;li>face 的数量是有限的&lt;/li>
&lt;li>$F$ 是 $P$ 的 face，那么 $\operatorname{lin}(F) = \operatorname{lin}(P)$&lt;/li>
&lt;li>$P$ 的两个 face $F \neq F^\prime$ 当且仅当 $\operatorname{aff}(F) \neq \operatorname{aff}(F^\prime)$&lt;/li>
&lt;/ul>
&lt;h3 id="facets">Facets&lt;/h3>
&lt;p>如果 $P$ 删去一个不等式仍然不变就说这个不等式是 &lt;em>&lt;strong>redundant&lt;/strong>&lt;/em> 的，一个多面体的各个约束可能都是多余的，比如一个不等式重复使用多次。&lt;/p>
&lt;p>如果系统 $Ax \leq b$ 定义的多面体不包含任何多余的约束，那么就称这个系统是多面体的 &lt;em>minimal representation&lt;/em> .&lt;/p>
&lt;p>$P$ 的一个 face $F$ 称为 &lt;strong>facet&lt;/strong>，如果 $F$ 非空且 $\operatorname{dim} F = \operatorname{dim}P - 1$ .&lt;/p>
&lt;p>比如说正方体的一条棱是 face，一个面就是 facet 。&lt;/p>
&lt;h3 id="minimal-faces">Minimal Faces&lt;/h3>
&lt;p>如果 $F$ 是 $P$ 的一个非空的 face，称 $F$ 是 $P$ 的一个 minimal face，如果 $F$ 不包含任何 $P$ 的 proper face，也就是说，没有维数更低的 face 了。&lt;/p>
&lt;p>一个非空的 face $F$ 是 $P$ 的 minimal face，当且仅当 $F=\{x\in \mathbb{R}^n : A^\prime x = b^\prime\}$ 且 $\text{rank}(A^\prime) = \text{rank}(A),\, (A^\prime, b^\prime)$ 是 $(A, b)$ 的子系统，也就是说，$F$ 是极大线性无关组的等式解，是一个仿射子空间。注意到 $\operatorname{lin}(P) =\ker A$，&lt;strong>这说明 $F$ 是 $\operatorname{lin}(P)$ 平移得到的&lt;/strong>。 由此立刻得到推论：如果 $F$ 是 $P$ 的非空的 face，那么 $\operatorname{dim}(\operatorname{lin}(P)) \leq \operatorname{dim}(F) \leq \operatorname{dim}(P)$ .&lt;/p>
&lt;p>一个极小的 face 不一定是顶点，比如“空间中一本摊开的无穷大的书”，它没有顶点，只有一条（无限长的）边。&lt;/p>
&lt;p>&lt;strong>Vectices&lt;/strong>&lt;/p>
&lt;p>多面体的0维的 face 称为 &lt;em>vertex&lt;/em> 。&lt;/p>
&lt;p>$P$ 存在顶点当且仅当 $\operatorname{lin}(P) = \{0\}$ ，即 $P$ has a vertex if and only if $P$ is pointed.&lt;/p>
&lt;p>$\bar{v}$ 是 $P$ 的顶点，当且仅当 $\bar{v}$ 是 $P$ 的极点 (extreme point)；当且仅当 $\bar{v}$ 满足 $Ax \leq b$ 中 $n$ 个线性无关的等式。&lt;/p>
&lt;p>通常来说，凸集 $C$ 的极点 $x$ 定义为，如果 $x = (x_1+x_2)/2$，那么 $x_1 = x_2 = x$，顶点和极点的概念对于多面体来说是相同的。&lt;/p>
&lt;p>&lt;strong>Edges&lt;/strong>&lt;/p>
&lt;p>多面体的1维的 face 称为 &lt;em>edge&lt;/em> 。&lt;/p>
&lt;p>一个 edge 最多包含两个顶点，如果 $P$ 是 pointed 的，那么每个 edge 都至少会包含一个 vertex 。&lt;/p>
&lt;p>多面体 $P$ 的 skeleton 指的是图 $G(P)$，其点为多面体 $P$ 的顶点，边为多面体的边。&lt;/p>
&lt;p>&lt;strong>Extreme Rays&lt;/strong>&lt;/p>
&lt;p>一个 pointed polyhedral cone $C$ 的 &lt;em>extreme ray&lt;/em> 指的是 $C$ 的 edge。&lt;/p>
&lt;p>一个 pointed polyhedron $P$ 的 &lt;em>extreme ray&lt;/em> 指的是它的 recession cone 的 extreme ray。&lt;/p>
&lt;p>Extreme ray 的概念类似于 extreme point，它不能写成两条不同线的 conic combination。&lt;/p>
&lt;!-- ### Decomposition Theorem for Polyhedra -->
&lt;h3 id="linear-optimization">Linear Optimization&lt;/h3>
&lt;p>一个线性优化指的是 $\max \; \{c^T x : x \in P\}$ .&lt;/p>
&lt;h3 id="integral-polyhedra">Integral Polyhedra&lt;/h3>
&lt;p>A convex set $C \subseteq \mathbb{R}^n$ is &lt;em>integral&lt;/em> if $C = \operatorname{conv} (C \cap \mathbb{Z}^n)$ 。&lt;/p>
&lt;p>一般我们把多面体 $P$ 的所有整点的凸包记作 $P_I$ .&lt;/p>
&lt;img src="../../figures/polyhedron-theory/image-20220430152134249.png" alt="image-20220430152134249" style="zoom:50%;" />
&lt;p>如果 $P=P_I$，那么多面体 $P$ 是整的。&lt;/p>
&lt;p>一个多面体是整的，如果：&lt;/p>
&lt;ul>
&lt;li>$P$ 的最小面包含一个整数点&lt;/li>
&lt;li>对任意整向量 $c$，$\max \{ c^T x : x \in P \}$ 的最优解是整的&lt;/li>
&lt;li>对任意整向量 $c$，$\max \{ c^T x : x \in P \}$ 的最优值是整数&lt;/li>
&lt;/ul>
&lt;p>以上三条性质是刻画 TDI(total dual integrality) 的重要工具。&lt;/p>
&lt;p>&lt;strong>(Integer Farkas Lemma or Kronecker Approximation Theorem)&lt;/strong>&lt;/p>
&lt;p>令 $A \in \mathbb{Z}^{m\times n}, b\in \mathbb{Z}^m$ 都是整的，则 $Ax=b$ 没有整数解 $\Leftrightarrow$ 存在 $u \in \mathbb{R}^m$ 使得 $uA \in \mathbb{Z}^n, u^T b \notin \mathbb{Z}$ 。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E6%95%B4%E6%95%B0%E5%92%8C%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/" term="整数和组合优化" label="整数和组合优化"/><category scheme="https://allenz-me.github.io/tags/polyhedron/" term="Polyhedron" label="Polyhedron"/></entry><entry><title type="text">A Markov Chain Approximation to Choice Modeling</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/13/"/><id>https://allenz-me.github.io/posts/papers/13/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-03-05T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Operations Research, 2016. DOI: https://doi.org/10.1287/opre.2016.1505. Keywords: choice modeling; assortment optimization; model selection. Subject classifications: probability: stochastic model applications; queues: Markovian. Area of review: Operations and Supply Chain. 这篇文章提出了一个用 Markov chain……</summary><content type="html">&lt;p>发表在 Operations Research, 2016. DOI: &lt;a href="https://doi.org/10.1287/opre.2016.1505">https://doi.org/10.1287/opre.2016.1505&lt;/a>.&lt;/p>
&lt;p>Keywords: choice modeling; assortment optimization; model selection.&lt;/p>
&lt;p>Subject classifications: probability: stochastic model applications; queues: Markovian.&lt;/p>
&lt;p>Area of review: Operations and Supply Chain.&lt;/p>
&lt;hr>
&lt;p>这篇文章提出了一个用 Markov chain 对顾客的选择行为进行建模的方式，并给出了一个多项式时间内解决 assortment optimization 的方法。&lt;/p>
&lt;p>商品集 $\mathcal{N} = \{1, 2, \dots, N\}$, $S_+ = S \cup \{0\}$，$\pi(i, S)$ 表示在 $S_+$ 中选择商品 $i$ 的概率。&lt;/p>
&lt;p>考虑用 Markov chain $\mathcal{M}$ 来模拟顾客选择的行为。$\mathcal{M}$ 有 $N + 1$ 个状态，初始的概率分布为 $\vec{\lambda} = (\pi(0, S), \dots, \pi(N, S))$。&lt;/p>
&lt;p>组合 $S$ 的选择过程如下：&lt;/p>
&lt;p>首先，一个顾客以 $\vec{\lambda}$ 为概率分布选择一个商品 $i$，如果 $i \in S$，那么顾客选择它。否则以概率 $\rho_{ij}$ 转移到另一个状态 $j \neq i$，转移概率如下：&lt;/p>
&lt;p>$$
\rho_{i j}= \begin{cases}1, &amp;amp; \text { if } i=0, j=0 \\ \displaystyle\frac{\pi(j, \mathcal{N} \backslash\{i\})-\pi(j, \mathcal{N})}{\pi(i, \mathcal{N})}, &amp;amp; \text { if } i \in \mathcal{N}, j \in \mathcal{N}_{+}, i \neq j \\ 0, &amp;amp; \text { otherwise }\end{cases}
$$&lt;/p>
&lt;p>文章接下来列举了一些选择模型的 Markov 近似。&lt;/p>
&lt;p>&lt;em>Multinomial logit (MNL) model&lt;/em>&lt;/p>
&lt;p>对于参数为 $u_0, \dots, u_N\; (\sum u_i = 1)$ 的 MNL 模型，初始状态分布和转移概率是：&lt;/p>
&lt;p>$$
\begin{aligned}
\lambda_{i} &amp;amp;=\pi(i, \mathcal{N})=u_{i} \\
\rho_{i j} &amp;amp;=\frac{\pi(j, \mathcal{N} \backslash\{i\})-\pi(j, \mathcal{N})}{\pi(i, \mathcal{N})}=\frac{u_{j}}{1-u_{i}}
\end{aligned}
$$&lt;/p>
&lt;p>&lt;em>Generalized attraction model (GAM)&lt;/em>&lt;/p>
&lt;p>$$
\pi(j, S)=\frac{u_{j}}{u_{0}+\sum_{i \notin S} v_{i}+\sum_{i \in S} u_{i}}
$$&lt;/p>
&lt;p>文章后面证明了这种近似对于 MNL 和 GAM 是精确的。&lt;/p>
&lt;p>接着，文章说明了如何从 Markov chain 中解构出一个选择模型。&lt;/p>
&lt;p>文章的第三章证明了 GAM 可以被 Markov chain 精确表示，并揭示了这类转移概率矩阵是秩1的。&lt;/p>
&lt;blockquote>
&lt;p>The proof of Theorem 3.1 shows that GAM can be represented exactly by a Markov chain where the transition submatrix over states $\mathcal{N}$ is a rank one matrix.&lt;/p>
&lt;/blockquote>
&lt;p>在第四章，文章给出了 Markov chain 近似任意一个选择模型的 bound。&lt;/p>
&lt;p>$$
\left(1-\alpha(\bar{S})^{2}\right) \cdot \pi(j, S) \leqslant \hat{\pi}(j, S) \leqslant\left(1+\frac{\tau(\bar{S}) \cdot \alpha(\bar{S})}{1-\alpha(\bar{S})}\right) \cdot \pi(j, S)
$$&lt;/p>
&lt;h4 id="assortment-optimization-for-markov-chain-model">Assortment Optimization for Markov Chain Model&lt;/h4>
&lt;p>在第五章，文章考虑如何对 Markov chain discrete choice model 做 assortment optimization，即，研究以下的优化问题：&lt;/p>
&lt;p>$$
\max _{S \subseteq \mathcal{N}} \sum_{j \in S}\left(r_{j} \cdot \hat{\pi}(j, S)\right)
$$&lt;/p>
&lt;p>当然，从 Markov chain 反推 $\hat{\pi}(j, S)$ 是困难的。但是，并不需要这样做。&lt;/p>
&lt;p>Let $g_i(S)$ denote the expected revenue from a customer that arrives in state $i$. According to the transitions in the Markov chain&lt;/p>
&lt;p>$$
g_{i}(S)=\sum_{j \in \mathcal{N}} P_{i j} g_{j}(S)
$$&lt;/p>
&lt;p>Reformulate as follows&lt;/p>
&lt;p>$$
\max _{S \subseteq \mathcal{N}} \sum_{j \in \mathcal{N}} \lambda_{j} \cdot g_{j}(S)
$$&lt;/p>
&lt;p>This optimization problem is equivalent to selecting an optimal set of stopping (or absorbing) states in the Markov chain, and is related to the classical optimal stopping time problem.&lt;/p>
&lt;blockquote>
&lt;p>Since the Markov chain model provides a good approximation of most discrete choice models and the corresponding assortment optimization problem can be solved efﬁciently, it can be useful to use the Markov chain model instead of the known true choice model.&lt;/p>
&lt;/blockquote>
&lt;p>【未完待续】&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/or/" term="OR" label="OR"/><category scheme="https://allenz-me.github.io/tags/choice-model/" term="Choice Model" label="Choice Model"/></entry><entry><title type="text">Dynamic Assortment Optimization with a Multinomial Logit Choice Model and Capacity Constraint</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/archives/12/"/><id>https://allenz-me.github.io/posts/archives/12/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-03-04T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Operations Research, 2010. DOI: https://doi.org/10.1287/opre.1100.0866. Subject classfications: assortment optimization; multinomial logit choice model; capacity constraint. Area of review: Revenue Management. 这篇文章讲述在有 capacity constraint 的情况下如何求解 MNL……</summary><content type="html">&lt;p>发表在 Operations Research, 2010. DOI: &lt;a href="https://doi.org/10.1287/opre.1100.0866">https://doi.org/10.1287/opre.1100.0866&lt;/a>.&lt;/p>
&lt;p>Subject classfications: assortment optimization; multinomial logit choice model; capacity constraint.&lt;/p>
&lt;p>Area of review: Revenue Management.&lt;/p>
&lt;hr>
&lt;p>这篇文章讲述在有 capacity constraint 的情况下如何求解 MNL 的 optimal assortment，讨论了静态和动态的问题。&lt;/p>
&lt;blockquote>
&lt;p>In the static problem, we assume that the parameters of the logit model are known in advance; we then develop a simple algorithm for computing a profit-maximizing assortment based on the geometry of lines in the plane and derive structural properties of the optimal assortment.&lt;/p>
&lt;p>For the dynamic problem, the parameters of the logit model are unknown and must be estimated from data. By exploiting the structural properties found for the static problem, we develop an adaptive policy that learns the unknown parameters from past data and at the same time optimizes the profit.&lt;/p>
&lt;/blockquote>
&lt;p>在经典的 MNL 模型下，给定组合 $S \subseteq \{1, 2, \dots, N\}$，顾客选择商品 $i \in S$ 的概率为：&lt;/p>
&lt;p>$$
\theta_{i}(S)= \begin{cases}v_{i} /\left(1+\sum_{k \in S} v_{k}\right), &amp;amp; \text { if } i \in S \cup\{0\} \\ 0, &amp;amp; \text { otherwise }\end{cases}
$$&lt;/p>
&lt;p>期望利润为：&lt;/p>
&lt;p>$$
f(S)=\sum_{i \in S} w_{i} \theta_{i}(S)=\frac{\sum_{i \in S} w_{i} v_{i}}{1+\sum_{i \in S} v_{i}}
$$&lt;/p>
&lt;p>在可选商品的数量有限的条件下，利润最大化的优化问题可表达为：&lt;/p>
&lt;p>$$
Z^{\ast}=\max \{f(S): S \subseteq\{1, \ldots, N\} \text { and }|S| \leqslant C\} \tag{1}
$$&lt;/p>
&lt;h3 id="static-optimization">Static Optimization&lt;/h3>
&lt;p>对于静态的问题，$v_i$ 事先已知，所以这是一个组合优化问题。&lt;/p>
&lt;p>文章首先给出一个反例证明 revenue-ordered assortment 不能给出最优解，然后提出了一个非递归的算法 &lt;em>STATICMNL&lt;/em>。&lt;/p>
&lt;p>式(1)等价于：&lt;/p>
&lt;p>$$
\begin{aligned}
\max_{S \subseteq N, \lambda} &amp;amp; \; \lambda \\
\text{s.t. } &amp;amp; |S| \leqslant C \\
&amp;amp; \frac{\sum_{i \in S} w_{i} v_{i}}{1+\sum_{i \in S} v_{i}} = \lambda
\end{aligned}
$$&lt;/p>
&lt;p>也即：&lt;/p>
&lt;p>$$
\begin{aligned}
\max_{S \subseteq N, \lambda} &amp;amp; \; \sum_{i \in S} (w_i - \lambda) v_i \\
\text{s.t. } &amp;amp; |S|\leqslant C \\
\end{aligned}
$$&lt;/p>
&lt;p>这意味着，我们要找使得 $\lambda = \displaystyle\max_{|S| \leqslant C} \sum_{i \in S} (w_i - \lambda) v_i$ 成立的 $\lambda$。&lt;/p>
&lt;p>记&lt;/p>
&lt;p>$$
A(\lambda)=\underset{X:|X| \leqslant C}{\arg \max } \sum_{i \in X} v_{i}\left(w_{i}-\lambda\right)
$$&lt;/p>
&lt;p>那么最优值就是 $\max \{f(A(\lambda)) \mid \lambda \in \mathrm{R}\}$，但是并不需要遍历所有的实数。&lt;/p>
&lt;p>记&lt;/p>
&lt;p>$$
\begin{aligned}
&amp;amp;h_{0}(\lambda)=0 \quad \text { and } \\
&amp;amp;h_{i}(\lambda)=v_{i}\left(w_{i}-\lambda\right), \quad \text { for } i=1, \ldots, N
\end{aligned}
$$&lt;/p>
&lt;p>这 $N + 1$ 条直线有 $O(N^2)$ 个交点，遍历 $A(\lambda)$ 只要在这些交点中枚举即可。&lt;/p>
&lt;blockquote>
&lt;p>THEOREM 2.4 (SENSITIVITY OF THE OPTIMAL ASSORTMENT). The output of the &lt;em>STATICMNL&lt;/em> algorithm depends only on the ordering $\boldsymbol{}{\sigma}^{0}$ of the customer preference weights $v_{i}$ and the ordering $\boldsymbol{\tau}$ of the intersection points $\mathscr{I}(i, j)$.&lt;/p>
&lt;/blockquote>
&lt;h3 id="dynamic-optimization">Dynamic Optimization&lt;/h3>
&lt;p>文章用 regret 衡量算法的表现。记随机变量 $X_t(S)$ 和 $Y_t(S)$ 分别表示顾客的选择和利润。其中：&lt;/p>
&lt;p>$$
P\left\{X_{t}(S)=i\right\}=\theta_{i}(S)=\frac{v_{i}}{1+\sum_{k \in S} v_{k}}, \quad Y_{t}(S)=w_{X_{t}(S)}
$$&lt;/p>
&lt;p>并且&lt;/p>
&lt;p>$$
\mathrm{E}[Y_t(S)] = f(S)
$$&lt;/p>
&lt;p>算法：根据历史信息选取一个 assortment&lt;/p>
&lt;p>$$
\psi_{t}: \mathscr{H}_{t-1} \rightarrow\{S \subseteq\{1, \ldots, N\}:|S| \leqslant C\}
$$&lt;/p>
&lt;p>$T$ 时刻的累积 regret 是：&lt;/p>
&lt;p>$$
\operatorname{Regret}(T, \psi)=\sum_{t=1}^{T} \mathrm{E}\left[Z^{\ast}-Y_{t}\left(S_{t}\right)\right]=\sum_{t=1}^{T} \mathrm{E}\left[Z^{\ast}-f\left(S_{t}\right)\right]
$$&lt;/p>
&lt;p>对于动态的问题，$v_i$ 只能从 past sales and assortment decisions 推断出来。但是，根据上一部分提出的算法，真正重要的是 customer preference 和 intersection points。&lt;/p>
&lt;p>文章提出了一个算法，叫做 &lt;em>Adaptive Assortment&lt;/em> (AA)。算法的每个循环都包含一个 exploration phase，以及随后的 exploitation phase。&lt;/p>
&lt;p>令 $\mathscr{E} = \{S \mid \{i, j\} \subseteq S, |S| = C\}$，在第 $m$ 个 exporation 阶段，可以根据顾客的选择信息得到 $\hat{\Theta}_i (m, S)$，顾客选择商品 $i$ 的近似概率，并由此估计出 customer preference 和 intersection points，文章进一步还证明了这样得到的 sequence of assortments ${\hat{\mathscr{A}}(m)}$ 与真实的 $\mathscr{A}(m)$ 有着足够高的概率是一样的。&lt;/p>
&lt;p>AA 算法的 regret 是：&lt;/p>
&lt;p>$$
\operatorname{Regret}(T, A A) \leqslant a_{2} N^{2} \log ^{2} T
$$&lt;/p>
&lt;h3 id="numerical-experiments">Numerical Experiments&lt;/h3>
&lt;p>这一部分，文章以一个 DVD 零售商在 2005 年一段时间的销售数据检验了前面两个算法的有效性。分为 &lt;em>static optimization&lt;/em> 和 &lt;em>dynamic optimization&lt;/em> 两个部分。&lt;/p>
&lt;h3 id="extensions-and-future-work">Extensions and Future Work&lt;/h3>
&lt;p>文章指出，上面的模型没有考虑到顾客的异质性 (heterogeneity)。同时 MNL 有 IIA 的缺点。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/or/" term="OR" label="OR"/><category scheme="https://allenz-me.github.io/tags/choice-model/" term="Choice Model" label="Choice Model"/></entry><entry><title type="text">次梯度与次梯度方法</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/subgrad/"/><id>https://allenz-me.github.io/posts/operations/subgrad/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-03-04T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">次梯度 subgradient 当优化问题的目标函数不可微的时候，我们就会去考虑梯度的替代品：次梯度。 一些目……</summary><content type="html">&lt;h3 id="次梯度-subgradient">次梯度 subgradient&lt;/h3>
&lt;p>当优化问题的目标函数不可微的时候，我们就会去考虑梯度的替代品：&lt;strong>次梯度&lt;/strong>。&lt;/p>
&lt;p>一些目标函数不可微的凸优化问题，就可以使用次梯度方法来求解。&lt;/p>
&lt;p>凸函数有一个很好的性质，那就是任意一点的切平面都位于函数图形的下方。对于凸函数 $f$，如果&lt;/p>
&lt;p>$$
f(y) \geq f(x)+g^{T}(y-x) \quad \text{for all } y \tag{1}
$$&lt;/p>
&lt;p>就说 $g$ 是 $x$ 这点的次梯度。&lt;/p>
&lt;p>函数的梯度是一个向量，注意次梯度方向可能不唯一，所有次梯度构成的集合叫做&lt;strong>次微分(subdifferential)&lt;/strong>，$x$ 的次微分记作 $\partial f(x)$。&lt;/p>
&lt;p>在上面的定义下，凸函数的次微分总是非空，凹函数的次微分是空集。类似地，把式(1)的不等号方向改变可以得到对于凹函数的次微分的定义。&lt;/p>
&lt;p>如果要把次梯度的概念推广到一般函数的话，那么会发现，一般函数可能再某个点不存在次梯度（次梯度为空集），而凸函数不会存在这样的问题。在凸函数的定义域的&lt;strong>相对内点集&lt;/strong>上，次梯度集总是非空（凸函数在定义域的边界的连续性比较复杂）。&lt;/p>
&lt;p>以一维为例，$f$ 在 $x_0$ 不一定可导，但是有左导数 $a$，右导数 $b$，那么 $[a,b]$ 就是 $f$ 在 $x_0$ 这点的次微分。比如 $f=|x|$ 在 $x=0$ 处的次微分就是 $[-1, 1]$。&lt;/p>
&lt;p>&lt;strong>通过次梯度可以得到一个函数（不管是不是凸的）全局最优解的性质。&lt;/strong> 如果 $0\in \partial f(x^\ast)$，那么 $x^\ast$ 就是全局最小点。&lt;/p>
&lt;h3 id="次梯度方法">次梯度方法&lt;/h3>
&lt;p>次梯度方法就是沿着次梯度的方向进行迭代的方法。如：&lt;/p>
&lt;p>$$
\max_x f(x) = \max_x \: \min_{i=1\dots n} \; a_i^T x + b_i
$$&lt;/p>
&lt;p>目标函数是分段线性的凹函数，在若干点是不可微的，但是可以计算到 $f(x)$ 的次梯度，令 $I(x^\ast) = \{i \mid f(x^\ast) = a_i^T x^\ast + b_i \}$，则&lt;/p>
&lt;ol>
&lt;li>$a_i \in \partial f(x^\ast) \;\; \text{for all }\; i \in I$&lt;/li>
&lt;li>$\partial f(x^\ast) = \left\{s \in \mathrm{R}^{n}\mid s=\Sigma_{i \in I\left(x^{\ast}\right)} \lambda_{i} a^{i}, \Sigma_{i \in I\left(x^{\ast}\right)} \lambda_{i}=1, \lambda_{i} \geq 0 \text { for } i \in I\left(x^{\ast}\right)\right\}$&lt;/li>
&lt;/ol>
&lt;p>由此可以用次梯度方法来求解。&lt;/p>
&lt;p>参考：
&lt;a href="https://zhuanlan.zhihu.com/p/103359560">【凸优化笔记5】-次梯度方法（Subgradient method）
&lt;/a>&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/tags/subgradient/" term="Subgradient" label="Subgradient"/></entry><entry><title type="text">H-index 的计算</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/h-index/"/><id>https://allenz-me.github.io/posts/coding/h-index/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-03-02T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">一名科研人员的 h 指数是指 ta 的 (n 篇论文中) 总共有 h 篇论文分别被引用了至少 h 次。且其余的……</summary><content type="html">&lt;p>一名科研人员的 h 指数是指 ta 的 (n 篇论文中) 总共有 h 篇论文分别被引用了至少 h 次。且其余的 n - h 篇论文每篇被引用次数不超过 h 次。&lt;/p>
&lt;p>如果 n 篇论文已经排序好，那么使用二分法能达到 $O(\log n)$ 的时间复杂度。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">def&lt;/span> &lt;span class="nf">hIndex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">citations&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">citations&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">lo&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hi&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">lo&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">hi&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">mid&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">lo&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">hi&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">citations&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">mid&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">mid&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">lo&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mid&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">hi&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mid&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">lo&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>使用线性扫描的时间复杂度是 $O(n)$&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">def&lt;/span> &lt;span class="nf">hIndex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">citations&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">citations&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">reverse&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">citations&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">citations&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">res&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>如果未排序，排序后再二分的复杂度是 $O(n \log n)$。&lt;/p>
&lt;p>也可直接使用桶排序，发表 n 篇文章，h 指数最高能达到 n，因此维护一个 n + 1 大小的桶再线性扫描即可。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">def&lt;/span> &lt;span class="nf">hIndex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">citations&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">citations&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">bucket&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">citations&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">bucket&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="n">cnt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">reversed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">cnt&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">bucket&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">cnt&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">i&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/></entry><entry><title type="text">泛函分析小结（一）</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/functional-1/"/><id>https://allenz-me.github.io/posts/analysis/functional-1/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-26T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">There are only two kinds of math books: those you cannot read beyond the first sentence, and those you cannot read beyond the first page. —— C. N. Yang 度量空间 (Metric Space) 一个集合 $X……</summary><content type="html">&lt;!-- 这一节主要应该涵盖度量空间的基本内容 -->
&lt;blockquote>
&lt;p>There are only two kinds of math books: those you cannot read beyond the first sentence, and those you cannot read beyond the first page.&lt;/p>
&lt;p>—— C. N. Yang&lt;/p>
&lt;/blockquote>
&lt;br>
&lt;h3 id="度量空间-metric-space">度量空间 (Metric Space)&lt;/h3>
&lt;p>一个集合 $X$，在其上定义一个函数 $d$，满足：&lt;/p>
&lt;ul>
&lt;li>$d(p, q)&amp;gt;0$ if $p \neq q ; d(p, p)=0$;&lt;/li>
&lt;li>$d(p, q)=d(q, p)$;&lt;/li>
&lt;li>$d(p, q) \leq d(p, r)+d(r, q)$, for any $r \in X$.
这时候 $d$ 是一个距离函数，$(X, d)$ 构成一个度量空间。&lt;/li>
&lt;/ul>
&lt;p>由度量可以引出一大串的拓扑性质。&lt;/p>
&lt;p>邻域 (Neighborhood)&lt;/p>
&lt;p>A &lt;strong>neighborhood&lt;/strong> of a point $p \in X$ is a set $N_{r}(p)$ consisting of all points $q$ such that $d(p, q)&amp;lt;r .$ The number $r$ is called the radius of $N_{r}(p)$.&lt;/p>
&lt;p>内点 (Interior Point)&lt;/p>
&lt;p>A point $p$ is an &lt;strong>interior point&lt;/strong> of $E$ if there is a neighborhood $N$ of $p$ such that $N \subset E$.&lt;/p>
&lt;p>开集 (Open Set)&lt;/p>
&lt;p>$E$ is &lt;strong>open&lt;/strong> if every point of $E$ is an interior point of $E$.&lt;/p>
&lt;p>聚点 (Limit Point)&lt;/p>
&lt;p>A point $p$ is a &lt;strong>limit point&lt;/strong> of the set $E$ if every neighborhood of $p$ contains a point $q \neq p$ such that $q \in E$.&lt;/p>
&lt;p>这意味着极限点总能找到一串点列去逼近它。&lt;/p>
&lt;p>闭集 (Closed Set)&lt;/p>
&lt;p>$E$ is &lt;strong>closed&lt;/strong> if every limit point of $E$ is a point of $E$.&lt;/p>
&lt;p>一个集合是闭集当且仅当它的补集是开的。&lt;/p>
&lt;p>闭包 (Closure)&lt;/p>
&lt;p>If $X$ is a metric space, if $E \subset X$, and if $E^{\prime}$ denotes the set of all limit points of $E$ in $X$, then the closure of $E$ is the set $\bar{E}=E \cup E^{\prime}$.&lt;/p>
&lt;p>一个集合的闭包是包含这个集合最小的闭集。&lt;/p>
&lt;!-- 紧集的闭子集是紧的。 -->
&lt;h3 id="度量空间的例子">度量空间的例子&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>离散度量
$$
\rho(x, y)= \begin{cases}0, &amp;amp; x=y \\ 1, &amp;amp; x \neq y\end{cases}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在 $\mathrm{R}$ 上定义度量
$$
\rho_{1}(x, y)=\frac{|x-y|}{1+|x-y|}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>定义 $C^\infty[a, b]$ 为区间 $[a, b]$ 上无穷次可微的函数，定义度量
$$
\rho(f, g)=\sum_{\nu=0}^{\infty} \frac{1}{2^{\nu}} \max _{t \in[a, b]} \frac{\left|f^{(\nu)}(t)-g^{(\nu)}(t)\right|}{1+\left|f^{(\nu)}(t)-g^{(\nu)}(t)\right|}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="赋范线性空间-normed-vector-space">赋范线性空间 (normed vector space)&lt;/h3>
&lt;p>如果 $X$ 是一个度量空间，$\mathrm{K}$ 是其对应的数域，则定义在 $X$ 的函数 $p(\cdot)$ 成为一个 &lt;strong>范数(norm)&lt;/strong> 的条件是：&lt;/p>
&lt;ol>
&lt;li>$p(x) \geqslant 0 , \;\forall x \in X$ (positive semidefiniteness)&lt;/li>
&lt;li>$p(\alpha x) = |\alpha| p(x), \;\forall x \in X, \alpha \in \mathrm{K}$ (positive homogeneity)&lt;/li>
&lt;li>$p(x) + p(y) \leqslant p(x + y), \; \forall x, y \in X$ (the triangle inequality / subadditivity)&lt;/li>
&lt;li>$p(x) = 0 \Rightarrow x = 0$ (positive definiteness)&lt;/li>
&lt;/ol>
&lt;p>满足前三个条件的称为&lt;strong>半范数(seminorm)&lt;/strong>，半范数可以为非零的向量赋予零长度。&lt;/p>
&lt;blockquote>
&lt;p>满足条件2和3的称为 sublinear function / quasi-seminorm / Banach functional&lt;/p>
&lt;p>范数和半范数都是 sublinear function&lt;/p>
&lt;p>sublinear function 是 convex function&lt;/p>
&lt;/blockquote>
&lt;p>赋范线性空间记为 $(X, \| \cdot \|)$。&lt;/p>
&lt;p>当度量满足条件：&lt;/p>
&lt;p>$$
\rho(x, y)=\rho(x-y, 0), \rho(\alpha x, 0)=|\alpha| \rho(x, 0)
$$&lt;/p>
&lt;p>可定义范数 $\| x\| = \rho(x, 0)$，度量空间可以成为一个赋范线性空间。&lt;/p>
&lt;h3 id="赋范线性空间的例子">赋范线性空间的例子&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>在所有 $[a, b]$ 上的连续函数 $C[a, b]$ 可定义范数：
$$
\|f\|=\max _{x \in[a, b]}|f(x)|
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>有界数列 $l^\infty$ 可定义范数：
$$
\|x\|_{\infty}:=\sup _{i \geqslant 1}\left|x_{i}\right|
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="lp-空间">$L^p$ 空间&lt;/h3>
&lt;p>$L^p$空间是连接实变函数与泛函分析的钥匙。&lt;/p>
&lt;p>任意给定一个 measure space $(X, \mathcal{B}, \mu)$，那些 $p$ 次幂 Lebesgue 可积的函数组成的全体就构成了 $L^p$ 空间。&lt;/p>
&lt;p>因为&lt;/p>
&lt;p>$$
|f+g|^{p} \leqslant(|f|+|g|)^{p} \leqslant 2^{p}\left(|f|^{p}+|g|^{p}\right)
$$&lt;/p>
&lt;p>$L^p$ 空间可以成为一个线性空间。如果函数是实值函数那么对应的数域就是 $\mathrm{R}$；如果是复值函数，那么数域就是$\mathrm{C}$。&lt;/p>
&lt;p>进一步我们可以定义这些函数的范数：&lt;/p>
&lt;p>$$
\lVert f\rVert_p=\int_X |f|^p \mathrm{d}\mu \quad (p\geq 1)
$$&lt;/p>
&lt;blockquote>
&lt;p>在证明范数的过程中，要用到 Hölder 不等式 $\|f g\|_{1} \leqslant\|f\|_{p}\|g\|_{q}$ 和 Minkowski 不等式 $\|f+g\|_{p} \leqslant\|f\|_{p}+\|g\|_{p}$&lt;/p>
&lt;/blockquote>
&lt;p>但这个范数只是半范数，几乎处处为0值的函数构成了 $L^p$ 的一个子空间，由此我们可以得到一个商空间，半范数也就成了范数。 $f$ 与 $g$ 几乎处处相等是这里的等价关系。&lt;/p>
&lt;p>对 $p = \infty$，此时定义函数的本性上确界 $\mathrm{ess\,sup} f{}$ 为除去一个零测集外的最大的上确界&lt;/p>
&lt;p>$$
\|f\|_{L^{\infty}}=\inf \{M:|f(x)| \leqslant M, x \in X\, \text { a.e. }\}
$$&lt;/p>
&lt;p>这个空间记作 $L^\infty$ 。&lt;/p>
&lt;p>特别地，取 $X=N$ 表示全体自然数，$\mu$ 是 counting measure，这样就得到了 $p$ 次幂有界数列空间 $l^p$。&lt;/p>
&lt;p>有了范数之后就能够讨论收敛性了&lt;/p>
&lt;p>$L^p$ 下的依范数收敛：$\int_X|f_n(x)-f(x)|^p\mathrm{d}\mu\to 0, \,\; \text{as} \;\;n \to \infty$，在概率论里面叫做 converges in the r-th mean.&lt;/p>
&lt;blockquote>
&lt;p>特别地，令 $1 \leq r \leq s$，$X_{n} \stackrel{L^{s}}{\longrightarrow} X \Rightarrow X_{n} \stackrel{L^{r}}{\longrightarrow} X$.&lt;/p>
&lt;/blockquote>
&lt;p>$L^\infty$ 下的依范数收敛：$\sup\limits _{X-E,\, \mu(E)=0}\left|f_{n}(x)-f(x)\right| \rightarrow 0$，其实就是除去一个零测集外的一致收敛。&lt;/p>
&lt;p>这两种意义下的收敛都要强于依测度收敛（依概率收敛 converge in probability）。&lt;/p>
&lt;p>对于 $1 \leq p \leq \infty$ 的情况，$L^p$ 空间是完备的。&lt;/p>
&lt;p>对于 $0 &amp;lt; p &amp;lt; 1$ 的情况，Minkowski 不等式不再成立，因此不能藉此定义范数。&lt;/p>
&lt;blockquote>
&lt;p>$L^p$ 空间是可分的&lt;/p>
&lt;/blockquote>
&lt;h3 id="一般拓扑">一般拓扑&lt;/h3>
&lt;p>$X$ 上的一个拓扑指的是满足如下条件的集类 $\mathcal{T} \subseteq 2^X$:&lt;/p>
&lt;ul>
&lt;li>$\emptyset, X \in \mathcal{T}$&lt;/li>
&lt;li>$\mathcal{T}$ 中任意个集合的并在 $\mathcal{T}$ 中&lt;/li>
&lt;li>$\mathcal{T}$ 中有限个集合的交在 $\mathcal{T}$ 中&lt;/li>
&lt;/ul>
&lt;p>$(X, \mathcal{T})$ 成为拓扑空间，$\mathcal{T}$ 中的集合称为开集，$X$ 中的元素称为点，任何开集的补集称为闭集。&lt;/p>
&lt;p>如果对任何 $x, y \in X$，都能找到开集 $U, V$ 使得：&lt;/p>
&lt;p>$$
x \in U, y \in V, \, U \cap V = \emptyset
$$&lt;/p>
&lt;p>则称 $(X, \mathcal{T})$ 为 Hausdorff 空间。&lt;/p>
&lt;p>最简单的拓扑是 $\{\emptyset, X\}$，称为 discrete topology / trivial topology。当 $X$ 包含至少两点时，它不是 Hausdorff 的。&lt;/p>
&lt;p>显然在一个集合上不止一种定义拓扑的方法，拓扑也有粗细之分，如果两个拓扑 $\mathcal{T}, \mathcal{T}^\prime$ 满足 $\mathcal{T} \subseteq \mathcal{T}^\prime$，就说 $\mathcal{T}$ 比 $\mathcal{T}^\prime$ 粗(coarser)，$\mathcal{T}^\prime$ 比 $\mathcal{T}$ 细(finer)。&lt;/p>
&lt;p>在拓扑的这套框架下，可以定义内点集和闭包。&lt;/p>
&lt;p>$$
A^\circ = \bigcup_{U \text{ is open}, \, U \subset A} U, \quad \bar{A} = \bigcap_{U \text{ is closed}, \, U \supset A} U
$$&lt;/p>
&lt;p>根据定义可以得到关系：&lt;/p>
&lt;p>$$
A^\circ \subseteq A \subseteq \bar{A}
$$&lt;/p>
&lt;p>定义 $A$ 的极限点 $x$，如果任意一个 $x$ 的邻域交 $A$ 都包含至少两个点。$A$ 的极限点组成了 $A$ 的导集 $A^\prime$，且存在关系 $\bar{A} = A \cup A^\prime$。&lt;/p>
&lt;p>在拓扑空间中，点列的收敛是用开集定义的。$\{x_n, n \geq 1\} \to x$ 的定义是，对于任意 $x$ 的邻域 $U$，都存在正整数 $N$，使得 $x_n \in U,\, \forall n \geq N$。在一般的拓扑空间中，点列的极限可能是不唯一的，但是 Hausdorff 空间中点列的极限是唯一的！&lt;/p>
&lt;blockquote>
&lt;p>$T_1$ 公理 ($T_1$ axiom)&lt;/p>
&lt;p>有限点集是闭集&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Continuity&lt;/strong>&lt;/p>
&lt;p>拓扑空间上的连续性由开集的原像是开集来表述，当然也可以用闭集的原像是闭集。 &lt;strong>一个函数的连续性，不仅取决于这个函数本身，还取决于这个函数定义域和值域空间的拓扑！&lt;/strong> 如果 $f: X \to Y$ 是连续函数，$Y$ 中的拓扑是 $\mathcal{B}$，那么只需要 $f^{-1}(B), \, \forall B \in \mathcal{B}$ 是开集即可保证 $f$ 的连续性。另外，连续性的一个等价条件是 $f(\bar{A}) \subset \overline{f(A)}$。&lt;/p>
&lt;p>连续还有局部的定义：对 $f(x)$ 的每个邻域 $V$，都存在包含 $x$ 的开集 $U$ 使得 $f(U) \subset V$。&lt;/p>
&lt;p>&lt;strong>Homeomorphsim&lt;/strong>&lt;/p>
&lt;p>同胚指的是连续并且逆映射也连续的双射。同胚保持拓扑性质。&lt;/p>
&lt;p>&lt;strong>Compactness&lt;/strong>&lt;/p>
&lt;p>任意开覆盖都存在有限子覆盖的集合是紧集。(Every open covering contains a finite subcollection.)&lt;/p>
&lt;p>紧集有非常多好的性质&lt;/p>
&lt;ul>
&lt;li>Hausdorff 空间中的紧集是闭的。&lt;/li>
&lt;li>连续映射将紧集映成紧集。&lt;/li>
&lt;li>任意个紧空间的直积是紧的。&lt;/li>
&lt;/ul>
&lt;p>每一个点列都有极限点的集合是列紧的。在可度量化的拓扑空间里，紧性和列紧是等价的。&lt;/p>
&lt;p>&lt;strong>Local Compactness&lt;/strong>&lt;/p>
&lt;p>$X$ 是局部紧的，如果对于任意的 $x \in X$，都存在一个包含了 $x$ 的一个邻域的紧集。比如 $(\mathrm{R}, |\cdot|)$ 就是局部紧的。&lt;/p>
&lt;p>one-point compactification.&lt;/p>
&lt;h3 id="由度量诱导的拓扑">由度量诱导的拓扑&lt;/h3>
&lt;p>度量可以自然的诱导出拓扑。&lt;/p>
&lt;h3 id="不动点定理">不动点定理&lt;/h3></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/></entry><entry><title type="text">Facility Location: A Robust Optimization Approach</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/archives/11/"/><id>https://allenz-me.github.io/posts/archives/11/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-25T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Production and Operations Management, 2010. DOI: https://doi.org/10.3401/poms.1080.01194. Key words: facility location; robust optimization; uncertainty; robust counterpart 这是一篇中规中矩的讲述利用经典鲁棒优化方法来解……</summary><content type="html">&lt;p>发表在 Production and Operations Management, 2010. DOI: &lt;a href="https://doi.org/10.3401/poms.1080.01194">https://doi.org/10.3401/poms.1080.01194&lt;/a>.&lt;/p>
&lt;p>Key words: facility location; robust optimization; uncertainty; robust counterpart&lt;/p>
&lt;hr>
&lt;p>这是一篇中规中矩的讲述利用经典鲁棒优化方法来解决选址问题的文章。&lt;/p>
&lt;p>关于经典的选址问题及其求解，参考：&lt;a href="https://allenz-me.github.io/facility_location_gcl.html">选址问题及其Gurobi求解&lt;/a>&lt;/p>
&lt;blockquote>
&lt;p>This paper contributes to the literature by formulating robust models for a facility location problem, and by providing insights into the solution structure.&lt;/p>
&lt;/blockquote>
&lt;p>名义模型是：&lt;/p>
&lt;p>令 $G(N, A)$ 是一个连通图，$d_{ij}$ 表示两点间的运输成本（距离），$Z_{i0}$ 是 $i$ 处设施的最大生产能力，$C_{i0}$ 是建设单位产能的成本，$c_{it}$ 是 $t$ 时刻 $i$ 处生产的单位成本，总的周期数是 $T$ ，$K_i$ 是建设的固定成本，&lt;/p>
&lt;p>决策变量 $I_i$ 表示是否在 $i$ 处选址，$Z_{it}$ 是设施 $i$ 在时刻 $t$ 的产量，$X_{ijt}$ 表示 $t$ 时刻 $j$ 处的需求被 $i$ 处满足的比例。&lt;/p>
&lt;p>每一单位的销售额产生 $\eta$ 的利润，先假设需求的确定的，目标是最大化利润。&lt;/p>
&lt;p>$$
\begin{gathered}
\left(P^{\prime}\right) \max _{\mathbf{X}, \mathbf{Z}, \mathbf{I}, \mathbf{Z}_{0}, \tau} \tau \\
\sum_{i=1}^{N} \sum_{j=1}^{N} \sum_{t=1}^{T}\left(\eta-d_{i j}\right) D_{j t} X_{i j t}-\sum_{i=1}^{N} \sum_{t=1}^{T} c_{i t} Z_{i t} -\sum_{i=1}^{N}\left(C_{i 0} Z_{i 0}+K_{i} I_{i}\right) \geq \tau \\
\sum_{j=1}^{N} D_{j t} X_{i j t} \leq Z_{i t} \quad \text { for all } i, t \\
\sum_{i=1}^{N} X_{i j t} \leq 1 \quad \text { for all } j, t \\
Z_{i 0} \leq M I_{i} \quad \text { for all } i \\
Z_{i t} \leq Z_{i 0} \quad \text { for all } i, t \\
X_{i j t} \geq 0 ; I_{i} \in\{0,1\} \quad \text { for all } i, j, t
\end{gathered}
$$&lt;/p>
&lt;p>这个名义模型是一个 MILP.&lt;/p>
&lt;h4 id="box-uncertainty">Box Uncertainty&lt;/h4>
&lt;p>假定需求有如下的不确定性： $D_{jt} \in U_{i t}^{B}=\left[\bar{D}_{j t}\left(1-\varepsilon_{t}\right), \bar{D}_{j t}\left(1+\varepsilon_{t}\right)\right]$&lt;/p>
&lt;p>问题仍旧转化为一个MILP&lt;/p>
&lt;h4 id="ellipsoidal-uncertainty">Ellipsoidal Uncertainty&lt;/h4>
&lt;p>假定需求有如下的不确定性：$U_{1}^{E}=\left\{\tilde{D} \in \Re^{N T} \mid \sum_{j=1}^{N} \sum_{t=1}^{T}\left[\frac{\tilde{D}_{j t}-\bar{D}_{j t}}{\varepsilon_{t} \bar{D}_{j t}}\right]^{2} \leq \Omega_{1}^{2}\right\}$&lt;/p>
&lt;p>问题变成一个MISOCP&lt;/p>
&lt;h3 id="numerical-study">Numerical Study&lt;/h3>
&lt;p>文章用数值模拟来比较三种方法。&lt;/p>
&lt;p>&lt;strong>鲁棒方法倾向于设置少的 facility 并且建设较大的生产能力，存在较多的运输&lt;/strong>，会出现一个点同时满足好几个点的需求这种情况；同时鲁棒方法能保证需求的满足。&lt;/p>
&lt;img src="../../figures/11/image-20220225205344678.png" alt="image-20220225205344678" style="zoom:50%;" />
&lt;p>文章接下来比较了三种方法的表现（利润），总的来说椭球不确定集的鲁棒优化方法表现较好。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/pom/" term="POM" label="POM"/><category scheme="https://allenz-me.github.io/tags/facility-location/" term="Facility Location" label="Facility Location"/><category scheme="https://allenz-me.github.io/tags/robust-optimization/" term="Robust Optimization" label="Robust Optimization"/></entry><entry><title type="text">函数的连续性</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/continuity/"/><id>https://allenz-me.github.io/posts/analysis/continuity/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-25T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">绝对连续 Absolute Continuity A function $f:[a, b] \rightarrow \mathbb{R}$ is said to be absolutely continuous on $[a, b]$ if, given $\varepsilon&amp;gt;0$, there exists some $\delta&amp;gt;0$ such that $$ \sum_{i=1}^{n}\left|f\left(y_{i}\right)-f\left(x_{i}\right)\right|&amp;lt;\varepsilon $$ whenever $\left\{\left[x_{i}, y_{i}\right]: i=1, \ldots, n\right\}$ is a finite……</summary><content type="html">&lt;h2 id="绝对连续-absolute-continuity">绝对连续 Absolute Continuity&lt;/h2>
&lt;p>A function $f:[a, b] \rightarrow \mathbb{R}$ is said to be absolutely continuous on $[a, b]$ if, given $\varepsilon&amp;gt;0$, there exists some $\delta&amp;gt;0$ such that
$$
\sum_{i=1}^{n}\left|f\left(y_{i}\right)-f\left(x_{i}\right)\right|&amp;lt;\varepsilon
$$
whenever $\left\{\left[x_{i}, y_{i}\right]: i=1, \ldots, n\right\}$ is a finite collection of mutually disjoint subintervals of $[a, b]$ with $\sum_{i=1}^{n}\left|y_{i}-x_{i}\right|&amp;lt;\delta$.&lt;/p>
&lt;p>绝对连续函数一定一致连续。Cantor 函数是一致连续的，但不绝对连续。&lt;/p>
&lt;p>绝对连续最重要的是与积分的联系，对于可积的函数 $f$
$$
F(x):=\int_{a}^{x} f(t) d t, \quad a \leq x \leq b
$$
在 $[a, b]$ 上是绝对连续的。&lt;/p>
&lt;p>绝对连续函数一定是有界变差的，所以绝对连续函数几乎处处可微。&lt;/p>
&lt;h2 id="一致连续-uniform-continuity">一致连续 Uniform Continuity&lt;/h2>
&lt;p>Given metric spaces $\left(X, d_{1}\right)$ and $\left(Y, d_{2}\right)$, a function $f: X \rightarrow Y$ is called uniformly continuous if for every real number $\varepsilon&amp;gt;0$ there exists real $\delta&amp;gt;0$ such that for every $x, y \in X$ with $d_{1}(x, y)&amp;lt;\delta$, we have that $d_{2}(f(x), f(y))&amp;lt;\varepsilon$.&lt;/p>
&lt;p>对应到一元函数，区间 $X$ 上的函数 $f$ 一致连续，如果 $\forall \epsilon &amp;gt; 0, \exists \delta &amp;gt; 0, \text{s.t. } \; |x - y| &amp;lt; \delta \Rightarrow |f(x) - f(y) | &amp;lt; \epsilon$&lt;/p>
&lt;p>Heine–Cantor theorem 说明了紧集上的连续函数一定是一致连续的。&lt;/p>
&lt;p>不一致连续的函数的例子，如 $f(x) = 1/x, \; x\in (0, 1)$、$f(x) = e^x , x \in \mathrm{R}$ .&lt;/p>
&lt;h2 id="半连续-semi-continuity">半连续 Semi-Continuity&lt;/h2>
&lt;p>半连续是对连续性的一种弱化，跟连续性类似，它有分析学上的定义，也有拓扑学意义上的定义。&lt;/p>
&lt;p>&lt;strong>分析学意义&lt;/strong>&lt;/p>
&lt;p>称 $f$ 在 $\bar{x}$ 下半连续, 如果 $\displaystyle\liminf _{x \rightarrow \bar{x}} f(x)\geq f(\bar{x})$&lt;/p>
&lt;p>称 $f$ 在 $\bar{x}$ 上半连续, 如果 $\displaystyle\limsup _{x \rightarrow \bar{x}} f(x) \leq f(\bar{x})$&lt;/p>
&lt;p>上（下）半连续函数是在各个点都上（下）半连续的函数。&lt;/p>
&lt;p>&lt;strong>拓扑学意义&lt;/strong>&lt;/p>
&lt;p>Let $f$ be a real (or extended-real) function on a topological space. If
$$
\{x: f(x)&amp;gt;\alpha\}
$$
is open for every real $\alpha, f$ is said to be &lt;em>lower semi-continuous&lt;/em>. If
$$
\{x: f(x)&amp;lt;\alpha\}
$$
is open for every real $\alpha, f$ is said to be &lt;em>upper semi-continuous&lt;/em>.&lt;/p>
&lt;p>最简单的例子是，开集的 indicator function $\mathbf{1}_A(x) = \begin{cases} 1 &amp;amp; x\in A\\ 0 &amp;amp; x \notin A \end{cases}$ 是下半连续的，闭集的 indicator function 是上半连续的。&lt;/p>
&lt;p>$\mathrm{R}$ 上的半连续函数：&lt;/p>
&lt;img src="../../figures/continuity/Lower-left-and-upper-right-semi-continuous-functions.png" alt="Lower (left) and upper (right) semi-continuous functions" style="zoom:67%;" />
&lt;p>&lt;strong>等价定义&lt;/strong>&lt;/p>
&lt;p>$f: X \to \mathrm{\bar{R}}$ 是上半连续的等价于：&lt;/p>
&lt;ul>
&lt;li>All superlevel sets $\{x \in X: f(x) \geq y\}$ with $y \in \mathrm{R}$ are closed in $X$.&lt;/li>
&lt;li>The hypograph $\{(x, t) \in X \times \mathrm{R}: t \leq f(x)\}$ is closed in $X \times \mathrm{R}$.&lt;/li>
&lt;/ul>
&lt;p>$f: X \to \mathrm{\bar{R}}$ 是下半连续的等价于：&lt;/p>
&lt;ul>
&lt;li>All sublevel sets $\{x \in X: f(x) \leq y\}$ with $y \in \mathrm{R}$ are closed in $X$.&lt;/li>
&lt;li>The epigraph $\{(x, t) \in X \times \mathrm{R}: t \geq f(x)\}$ is closed in $X \times \mathrm{R}$.&lt;/li>
&lt;/ul>
&lt;p>在凸优化中，有时把闭函数定义为 epigraph 为闭集的函数，它与下半连续函数是等价的。&lt;/p>
&lt;p>&lt;strong>性质&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>$f$ 连续当且仅当它是上半连续和下半连续的。&lt;/li>
&lt;li>下半连续函数的和是下半连续的；上半连续函数的和是下半连续的。&lt;/li>
&lt;li>$f$ 下半连续当且仅当 $-f$ 是上半连续的。&lt;/li>
&lt;li>紧集上的下半连续函数存在最小值；紧集上的上半连续函数存在最大值。两个联系起来就是紧集上的连续函数存在最值。(Weierstrass extreme value theorem)&lt;/li>
&lt;/ul></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/></entry><entry><title type="text">Customer Choice Models vs. Machine Learning: Finding Optimal Product Displays on Alibaba</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/10/"/><id>https://allenz-me.github.io/posts/papers/10/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-15T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Operations Research, 2021(Articles in Advance). DOI: https://doi.org/10.1287/opre.2021.2158. Subject Classiﬁcations: Buyer behavior, Choice models, Computational complexity Area of Review: OR Practice Keywords: choice models; product assortment;……</summary><content type="html">&lt;p>发表在 Operations Research, 2021(Articles in Advance). DOI: &lt;a href="https://doi.org/10.1287/opre.2021.2158">https://doi.org/10.1287/opre.2021.2158&lt;/a>.&lt;/p>
&lt;p>Subject Classiﬁcations: Buyer behavior, Choice models, Computational complexity&lt;/p>
&lt;p>Area of Review: OR Practice&lt;/p>
&lt;p>Keywords: choice models; product assortment; machine learning; field experiment; retail operations&lt;/p>
&lt;hr>
&lt;p>这篇文章记录了阿里巴巴算法团队在 2018 年针对商品推荐的两种方法 machine learning VS customer choice models 的过程和结果。&lt;/p>
&lt;h3 id="product-display-problem">Product-Display Problem&lt;/h3>
&lt;p>天猫的一次活动中，店铺会给予新人顾客一个红包，然后指定六件商品，红包只能用在这六件商品中。天猫当然希望用户有更高的购买率和购买金额，所以算法团队的任务就是找出六件商品推荐给用户。&lt;/p>
&lt;img src="../../figures/10/image-20220221160640582.png" alt="image-20220221160640582" style="zoom:50%;" />
&lt;p>推荐算法的核心可以看成是一个优化问题（assortment optimization）：
$$
\max _{S \subseteq \mathcal{N}:|S|=6} \sum_{j \in S} r_{j} \cdot P_{j t}
$$
$P_{jt}$ 是顾客 $t$ 购买商品 $j$ 的概率，$r_j$ 是商品的利润。$\mathcal{N}$ 表示全体商品，优化的目标是找到一个子集 $S$，使得期望利润最大。&lt;/p>
&lt;p>这个问题最重要的一步就是计算 $P_{jt}$ （estimation）&lt;/p>
&lt;p>阿里巴巴团队在过去一直使用的是一套复杂的机器学习算法来估计 $P_{jt}$，&lt;strong>在 2018 年 3 月的为期两周的实验中，阿里团队比较了机器学习方法和 multinomial logit (MNL) model 的效果，并发现 MNL 方法能给平台带来显著更高的收益。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Our ﬁeld experiments revealed that the MNL-based approach generated 5.17 renminbi (RMB) per customer visit, compared with the 4.04 RMB per customer visit generated by the machine-learning-based approach when both approaches were given access to the same set of the 25 most important features.&lt;/p>
&lt;/blockquote>
&lt;h3 id="the-estimation-problem">The Estimation Problem&lt;/h3>
&lt;p>在机器学习方法中，预测 $P_{jt}$ 主要用的是产品特征（价格、评价、浏览次数）、顾客特征（历史消费行为、人口统计信息）。团队提取出了其中最重要的 25 个特征。&lt;/p>
&lt;p>训练数据由 $\mathcal{PH}_{ML} = \{ (\mathbf{X_{jt}}, c_{jt}, z_{jt}): 1 \leq t \leq T,\; j \in S_t \}$ 组成，这些都是历史的推荐结果。由于模型保密原因，文章没有过多透露机器学习方法是怎样实践的。它一共分为两步，第一步是训练模型 $f(\mathbf{X_{jt}})$ 预测 $\mathbb{P}(c_{jt} = 1)$，第二部是训练模型 $g(\mathbf{X_{jt}})$ 预测 $\mathbb{P}\left(z_{i t}=1 \mid c_{i t}=1\right)$，最后顾客的购买概率就是 $\mathbb{P}_{jt} (\mathbf{X_{jt}}) = f(\mathbf{X_{jt}}) \cdot g(\mathbf{X_{jt}})$。&lt;/p>
&lt;p>MNL Choice Model 更加简单，它就是一个参数模型：
$$
P_{j t}\left(S_{t}, X_{t}\right)=\mathbb{P}\left[U_{j t}=\max _{i \in S_{t}} U_{i t}\right]=\frac{e^{\beta^{T} X_{j t}}}{1+\sum_{i \in S_{t}} e^{\beta^{T} X_{j t}}}
$$
$\mathcal{PH}_{MNL} = \{(S_t, \mathbf{X_{jt}}, z_t): t = 1, \dots, \tau\}$ 。文章用极大似然法来估计 $\mathbf{\beta}$：
$$
\max _{\beta}\;
\mathcal{L} \mathcal{L}\left(\boldsymbol{\beta} ; \mathcal{P} \mathcal{H}_{M N L}\right)=\sum_{t=1}^{\tau}\left(\boldsymbol{\beta}^{T} X_{z_{t} t}-\log \left(1+\sum_{j \in S_{t}} e^{\boldsymbol{\beta}^{T} \mathbf{X}_{\mathbf{j t}}}\right)\right)
$$
目标函数是凹的，因此是一个凸优化问题。$\tau$ 在两千万到三千万之间，阿里团队选择使用 TensorFlow 用随机梯度下降法来解。&lt;/p>
&lt;h3 id="the-assortment-problem">The Assortment Problem&lt;/h3>
&lt;p>机器学习方法的 assortment 非常简单，挑选使得 $r_j \cdot P(\mathbf{X_{jt}})$ 最大的六种商品即可。&lt;/p>
&lt;p>MNL 的 assortment 是：
$$
Z_{O P T}=\max _{\mathbf{y} \in \mathcal{F}} R(\mathbf{y}) =\sum_{j \in N} P_{j}(\mathbf{y}) \cdot r_{j}=\frac{\sum_{j \in \mathcal{N}} r_{j} v_{j} \mathbf{y}_{j}}{1+\sum_{i \in \mathcal{N}} v_{i} \mathbf{y}_{i}}
$$
其中 $\mathcal{F}=\left\{\mathbf{y} \in\{0,1\}^{n}: \sum_{j=1}^{n} \mathbf{y}_{j}=6\right\}$ .&lt;/p>
&lt;p>文章在前文的基础上给出了一个 $O(n^2)$ 的算法。&lt;/p>
&lt;h4 id="extension">Extension&lt;/h4>
&lt;p>&lt;em>Joint pricing and assortment&lt;/em>&lt;/p>
&lt;p>如果价格有 $m$ 种选择，那么可行集是
$$
\begin{gathered}
\mathcal{F}_{1}=\left\{\mathbf{y} \in\{0,1\}^{m \times n}: \sum_{j=1}^{m} \mathbf{y}_{i j} \leq 1 \;\;\forall i \in N\right\} \\
\cap \left\{\mathbf{y} \in\{0,1\}^{m \times n}: \sum_{i=1}^{n} \sum_{j=1}^{m} \mathbf{y}_{i j}=6\right\}
\end{gathered}
$$
问题变成了：
$$
\max _{\mathbf{y} \in \mathcal{F}_{1}} R(\mathbf{y})=\max _{\mathbf{y} \in \mathcal{F}_{1}} \frac{\sum_{i=1}^{n} \sum_{j=1}^{m} r_{i} v_{i j} \mathbf{y}_{i j}}{1+\sum_{i=1}^{n} \sum_{j=1}^{m} v_{i j} \mathbf{y}_{i j}}
$$
文章还给出了一种转变为线性规划求解上述问题的方法。&lt;/p>
&lt;p>&lt;em>Icon display size&lt;/em>&lt;/p>
&lt;p>如果商品图标的大小是可变的。假定一定大小的 icon 对顾客的购买行为有影响。&lt;/p>
&lt;h3 id="experiment-design-and-data--main-results">Experiment Design and Data &amp;amp; Main Results&lt;/h3>
&lt;p>这一部分文章叙述了实验的设计过程并从统计学的角度说明了实验的有效性。&lt;/p>
&lt;p>在第一周的实验里，团队对比了一下三种方法：&lt;/p>
&lt;ol>
&lt;li>The MNL-based approach (MNL approach) 使用 top25 特征的MNL&lt;/li>
&lt;li>The same-feature-ML-based approach (SF-ML approach) 使用 top25 特征的机器学习&lt;/li>
&lt;li>The all-feature-ML-based approach (AF-ML approach) 使用全部特征的机器学习&lt;/li>
&lt;/ol>
&lt;p>实验的数据量在百万量级。三组的人数分别是：1,879,903、1,879,598、1,876,940。&lt;/p>
&lt;p>实验结果的统计数据如下图示：&lt;/p>
&lt;img src="../../figures/10/image-20220306102610935.png" alt="" style="zoom:50%;" />
&lt;p>Panel A 显示了三组顾客的统计信息，t-test 显示两组顾客并没有显著的差异。&lt;/p>
&lt;p>Panel B 显示了三种方法的效果&lt;/p>
&lt;h4 id="financial-performance">Financial Performance&lt;/h4>
&lt;p>总的来说，MNL比SF-ML表现好，跟AF-ML方法不分伯仲。&lt;/p>
&lt;img src="../../figures/10/image-20220306103851084.png" alt="" style="zoom:50%;" />
&lt;h4 id="purchase-probability-accuracy">Purchase-Probability Accuracy&lt;/h4>
&lt;img src="../../figures/10/image-20220306104532405.png" alt="image-20220306104532405" style="zoom:50%;" />
&lt;p>就用户购买的准确率来说，MNL方法显著差于机器学习方法。&lt;/p>
&lt;blockquote>
&lt;p>We ascribe the superior ﬁnancial performance of the MNL approach to its ability to produce six-product displays that ultimately lead to higher revenue products being purchased.&lt;/p>
&lt;/blockquote>
&lt;h4 id="average-purchase-price">Average Purchase Price&lt;/h4>
&lt;img src="../../figures/10/image-20220306104839997.png" alt="" style="zoom:50%;" />
&lt;p>上表展示了 MNL 方法能产生更高的利润和购买率。&lt;/p>
&lt;blockquote>
&lt;p>Hence, although the SF-ML approach produces more accurate estimates of the purchase probabilities, it is not able to offer assortments that are as desirable or as proﬁtable as those offered by the MNL approach.&lt;/p>
&lt;/blockquote>
&lt;h4 id="weakness-of-the-mnl-based-approach">Weakness of the MNL-Based Approach&lt;/h4>
&lt;ol>
&lt;li>MNL 只假定了顾客购买一件商品，但实际上顾客可能购买多件商品。&lt;/li>
&lt;li>不能通过顾客的点击来推断顾客的偏好。&lt;/li>
&lt;/ol>
&lt;p>附：&lt;/p>
&lt;p>GMV（全称Gross Merchandise Volume），即商品交易总额，是成交总额（一定时间段内）的意思。&lt;/p>
&lt;p>电商平台给出的计算指标是：GMV=销售额+取消订单金额+拒收订单金额+退货订单金额。&lt;/p>
&lt;p>GMV不是实际的购买交易数据，但是可以用GMV来研究顾客的购买意向，顾客买了之后发生退单的比率，GMV与实际成交额的比率等等。&lt;/p>
&lt;hr>
&lt;p>Appendix C. 说明了实验没有带来长期的负面效应&lt;/p>
&lt;p>Appendix D. 用全部的特征比较了 MNL 和 ML，这是一个在2018年9月的实验。&lt;/p>
&lt;img src="../../figures/10/image-20220306111911242.png" alt="image-20220306111911242" style="zoom:50%;" />
&lt;p>实验显示全量特征的 MNL 方法比全量特征的 ML 方法能带来更高的顾客购买额。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/or/" term="OR" label="OR"/><category scheme="https://allenz-me.github.io/tags/choice-model/" term="Choice Model" label="Choice Model"/></entry><entry><title type="text">Multi-armed Bandits (1)</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cs234/none/"/><id>https://allenz-me.github.io/posts/cs234/none/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-15T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">多臂老虎机问题（multi-armed bandits）是一类重要的在线学习问题。 问题……</summary><content type="html">&lt;p>多臂老虎机问题（multi-armed bandits）是一类重要的在线学习问题。&lt;/p>
&lt;h2 id="问题简述">问题简述&lt;/h2>
&lt;p>带有不确定性的序贯决策问题：有 $N$ 种行为，每次选择一种，收到回报（reward）；目标是最大化 $T$ 轮的总回报。&lt;/p>
&lt;p>根据历史回报选择下一步行为的方法称为算法。&lt;/p>
&lt;p>&lt;strong>应用场景：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>在线广告投放：每次推荐给用户一个广告，用户点击则收到回报。&lt;/li>
&lt;li>商品推荐：每次推荐给用户一件商品，用户点击或购买则收到回报。&lt;/li>
&lt;li>资产投资：对多类资产进行配置，赚取收益。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>问题分类：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>根据 Feedback：
&lt;ul>
&lt;li>bandit feedback: 做出选择之后只能得到该行为回报的信息。&lt;/li>
&lt;li>partial feedback. 做出选择之后能得到该行为回报的信息，和其它行为回报的部分信息。&lt;/li>
&lt;li>full feedback. 做出选择之后能得到全部行为的信息。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>根据 Rewards:
&lt;ul>
&lt;li>IID rewards: 不同行为的回报相互独立且服从一个固定的概率分布。&lt;/li>
&lt;li>Adversarial rewards: 对手可以随意更改回报。&lt;/li>
&lt;li>Constrained adversary: 对手每轮只能有限制地更改回报。&lt;/li>
&lt;li>Stochastic rewards: 回报服从一个随机过程。比如，两个行为的回报服从一个二维的布朗运动。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The simplest setting: bandit feedback + IID rewards.&lt;/p>
&lt;p>算法的Regret:&lt;/p>
&lt;p>$$
R(T)=\mu^{\ast} \cdot T-\sum_{t=1}^{T} \mu\left(a_{t}\right)
$$&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/mab/" term="MAB" label="MAB"/><category scheme="https://allenz-me.github.io/tags/mab/" term="MAB" label="MAB"/></entry><entry><title type="text">离散选择模型 Discrete Choice Models</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/discrete-choice-models/"/><id>https://allenz-me.github.io/posts/operations/discrete-choice-models/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-15T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">参考文献：Discrete Choice Models and Applications in Operations Management. In INFORMS TutORials in Operations Research. https://doi.org/10.1287/educ.2021.0229 Discrete Choice Models 离散选择模型关注决策者如……</summary><content type="html">&lt;p>参考文献：Discrete Choice Models and Applications in Operations Management. In INFORMS TutORials in Operations Research. &lt;a href="https://doi.org/10.1287/educ.2021.0229">https://doi.org/10.1287/educ.2021.0229&lt;/a>&lt;/p>
&lt;h2 id="discrete-choice-models">Discrete Choice Models&lt;/h2>
&lt;p>离散选择模型关注决策者如何在多个备选方案中做出选择，已成为研究消费者面对多种产品的购买行为的重要工具，并已广泛应用于经济、营销、交通和运营等多个领域。&lt;/p>
&lt;h3 id="luce-model">Luce Model&lt;/h3>
&lt;p>记 $q_i(S)$ 为选择 item $i \in S$ 的概率&lt;/p>
&lt;p>&lt;strong>Regularity Condition&lt;/strong>&lt;/p>
&lt;p>the choice probability for any product in the offer set decreases as the offer set enlarges.&lt;/p>
&lt;p>&lt;strong>choice axiom&lt;/strong>
$$
q_i(S) = q_{S^\prime}(S) \cdot q_i (S^\prime) \quad \forall i \in S^\prime \subseteq S
$$
选择公理把选择一件商品分成了两步。&lt;/p>
&lt;p>Luce 证明了满足 choice axiom 的 $q$ 一定有如下形式：&lt;/p>
&lt;p>$$
q_{i}(S)= \frac{a_{i}}{\sum_{j \in S} a_{j}}
$$&lt;/p>
&lt;p>上式也满足正则性条件。&lt;/p>
&lt;h3 id="rum-framework">RUM framework&lt;/h3>
&lt;p>Random Utility Maximization Framework 假设顾客会选择效用最高的商品，效用是随机的：&lt;/p>
&lt;p>$$
U_i = u_i + \xi_i
$$&lt;/p>
&lt;p>其中 $u_i$ 是常数，$\xi_i$ 是 i.i.d. 参数为 $\mu$ 的 Gumbel 随机变量。&lt;/p>
&lt;blockquote>
&lt;p>$F_{\xi_i}(x)=\mathrm{P}\left(\xi_{i} \leq x\right)=\exp (-\exp (-(x / \mu+\gamma)))$，其中 $\gamma$ 是欧拉常数。&lt;/p>
&lt;/blockquote>
&lt;p>这时候就有：&lt;/p>
&lt;p>$$
q_{i}(S)=\operatorname{Pr}\left(U_{i} \geq U_{j}, \forall j \in S, j \neq i\right), \text { for any } i \in S
$$&lt;/p>
&lt;p>在这样一套理论框架下，Holman and Marley 给出了 multinominal logit 模型&lt;/p>
&lt;p>$$
q_{i}(S)=\frac{\exp \left(u_{i} / \mu\right)}{\sum_{j \in S} \exp \left(u_{j} / \mu\right)}, \text { for any } i \in S
$$&lt;/p>
&lt;p>容易看出 MNL 模型和 Luce 模型的等价性。&lt;/p>
&lt;p>MNL/Luce 模型有一个局限性，那就是不能描述具有相关性商品的选择概率。问题就出现在它满足 IIA 这个性质。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>IIA: independence of irrelevant alternatives.&lt;/strong>&lt;/p>
&lt;p>The basic idea of IIA is that the ratio of any two products' shares should be independent of all other products.&lt;/p>
&lt;p>&lt;strong>“red bus/blue bus” paradox&lt;/strong>&lt;/p>
&lt;p>{red bus, car} vs {red bus, blue bus, car}&lt;/p>
&lt;p>假设市场上有 cars 和 red buses 两种商品，各占50%市场份额，现在增加 blue buses 这种商品，它的效用跟 red buses 是一样的，MNL 模型会把 cars 的份额调低成 33%，但这很明显是不符合实际的。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Substitution Effect&lt;/strong>&lt;/p>
&lt;p>The choice probability decreases as any other alternative becomes more appealing&lt;/p>
&lt;p>MNL 也满足 substitution effect.&lt;/p>
&lt;h3 id="nested-logit-model">Nested Logit Model&lt;/h3>
&lt;p>NL 将商品的相关性加入到 MNL 中。&lt;/p>
&lt;h3 id="mnl-with-network-effects">MNL with Network Effects&lt;/h3>
&lt;p>一个人的购买行为会受到身边的人的影响，这其实构成了一个网络。&lt;/p>
&lt;h2 id="pricing-optimization">Pricing Optimization&lt;/h2>
&lt;p>价格影响顾客选择商品的概率，假定效用关于价格是线性的：
$$
u_i = \alpha_i - \beta_i p_i
$$
并令 no-purchase option $a_0=1$，这时候的选择模型：
$$
q_{i}\left(S^{+}, \mathbf{p}\right)=\frac{\exp \left(\alpha_{i}-\beta_{i} p_{i}\right)}{1+\sum_{j \in S} \exp \left(\alpha_{j}-\beta_{j} p_{j}\right)}, \quad \forall i \in S, \quad (S^+ = S \cup \{0\})\quad
$$
以最大化期望收益为目标，多产品定价优化问题可表示为：&lt;/p>
&lt;p>$$
\max _{\mathbf{p}} R(S, \mathbf{p}):=\sum_{i \in S}\left(p_{i}-c_{i}\right) \cdot q_{i}\left(S^{+}, \mathbf{p}\right)
$$&lt;/p>
&lt;h2 id="assortment-optimization">Assortment Optimization&lt;/h2>
&lt;p>另一种零售策略，如果商品的价格是给定的，商家只能调整展示给顾客的商品种类。&lt;/p>
&lt;p>这时候的优化目标变为：
$$
\max _{S \subseteq N} R(S):=\sum_{i \in S}\left(p_{i}-c_{i}\right) \cdot q_{i}\left(S^{+}\right)
$$&lt;/p>
&lt;p>&lt;strong>Revenue-Ordered Assortment&lt;/strong>&lt;/p>
&lt;p>假定商品索引按利润排序，即 $p_{1}-c_{1} \geq p_{2}-c_{2} \geq \cdots \geq p_{n}-c_{n}$，则 $S_i = \{1, 2, \dots, i\}, i = 1, \dots n$ 被称为 &lt;em>revenue-ordered assortment&lt;/em>&lt;/p>
&lt;p>对 MNL 模型来说，最优的 assortment 在 revenue-ordered assortments 中取到。时间复杂度是 $O(n)$。&lt;/p>
&lt;h3 id="constrained-assortment-optimization">Constrained Assortment Optimization&lt;/h3>
&lt;p>当可以展示的商品数量是有限的的情况下，问题变成：
$$
\begin{array}{ll}
\max\limits_{S \subseteq N} &amp;amp; R(S):=\displaystyle\sum_{i \in S}\left(p_{i}-c_{i}\right) \cdot \frac{a_{i}}{1+\sum_{j \in S} a_{j}} \\
\text { s.t. } &amp;amp; |S| \leq C
\end{array}
$$&lt;/p>
&lt;p>更进一步地，如果商品种类和价格都是可以变化的，这时商家面临 Joint Assortment and Price Optimization。&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>总的来说，对于 discrete choice model 的研究主要分为以下两块：&lt;/p>
&lt;ol>
&lt;li>如何从数据中拟合出一个好的选择模型&lt;/li>
&lt;li>给定一类选择模型，如何设计高效的算法解决 pricing / assortment 等问题&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;strong>The Red-Bus/Blue-Bus Problem&lt;/strong>&lt;/p>
&lt;p>While choice simulators have proven eminently useful for simulating buyer behavior, one of the most common simulation models (the Logit or Share of Preference model) has displayed a problematic result often described as the &lt;em>Red-Bus/Blue-Bus problem&lt;/em>. The underlying property leading to this problem is termed IIA, which is shorthand for &amp;quot;Independence from Irrelevant Alternatives.&amp;quot; The basic idea of IIA is that the ratio of any two products' shares should be independent of all other products. This sounds like a good thing, and at first, IIA was regarded as a beneficial property.&lt;/p>
&lt;p>However, another way to say the same thing is that an improved product gains share from all other products in proportion to their original shares; and when a product loses share, it loses to others in proportion to their shares. Stated that way, it is easy to see that IIA implies an unrealistically simple model. In the real world, products compete unequally with one another and when an existing product is improved, it usually gains most from a subset of products with which it competes most directly.&lt;/p>
&lt;p>Imagine a transportation market with two products, cars and red buses, each having a market share of 50%. Suppose we add a second bus, colored blue. An IIA simulator would predict that the blue bus would take share equally from the car and red bus, so that the total bus share would become 67%. But it's clearly more reasonable to expect that the blue bus would take share mostly from the red bus, and that total bus share would remain close to 50%.&lt;/p>
&lt;p>It is important to note that some degree of IIA is appropriate and useful within market simulations. In many markets, there is some degree of randomness to buyer behavior. It is not that people are irrational, but that buyers must balance the costs of making a utility maximizing decision against the costs of taking the time to make perfect decisions. It is quite reasonable for rational buyers to make what on the surface may seem as haphazard decisions — especially for low-involvement purchases. A similar or even duplicate offering could thus be expected to capture more share in the real world than a rational simulation model might suggest.&lt;/p>
&lt;p>In general, market simulation models based on disaggregate models of preference (utilities estimated at the individual level) are less subject to IIA difficulties than aggregate models of preference (aggregate logit, as offered by our CBC System). However, IIA issues are worse as more product alternatives are added to the market simulation.&lt;/p>
&lt;p>In addition to modeling respondent preferences at the individual level, there are market simulation methods (such as Randomized First Choice, First Choice, and Share of Preference with Top N setting) that help deal with IIA. These are described in the next sections.&lt;/p>
&lt;p>&lt;a href="https://sawtoothsoftware.com/help/lighthouse-studio/manual/hid_thered-bus.html">https://sawtoothsoftware.com/help/lighthouse-studio/manual/hid_thered-bus.html&lt;/a>&lt;/p>
&lt;/blockquote></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/tags/choice-model/" term="Choice Model" label="Choice Model"/></entry><entry><title type="text">Lecture 5: Value Function Approximation</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cs234/lecture5/"/><id>https://allenz-me.github.io/posts/cs234/lecture5/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-13T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Many real world problems have enormous state and/or action spaces, so tabular representation is insufficient. Value Function Approximation Represent a (state/state-action) value function with a parameterized function instead of a table Many possible function approximators including Linear combinations of features Neural networks……</summary><content type="html">&lt;p>Many real world problems have enormous state and/or action spaces, so tabular representation is insufficient.&lt;/p>
&lt;h2 id="value-function-approximation">Value Function Approximation&lt;/h2>
&lt;p>Represent a (state/state-action) value function with a parameterized function instead of a table&lt;/p>
&lt;img src="../../figures/lecture5/vf.png" alt="" style="zoom:50%;" />
&lt;p>Many possible function approximators including&lt;/p>
&lt;ul>
&lt;li>Linear combinations of features&lt;/li>
&lt;li>Neural networks&lt;/li>
&lt;li>Decision trees&lt;/li>
&lt;li>Nearest neighbors&lt;/li>
&lt;li>Fourier/wavelet bases&lt;/li>
&lt;/ul>
&lt;p>Stochastic-gradient Descent
$$
\min_{\mathbf{w}_t} [v_\pi(S_t) - \hat{v}(S_t, \mathbf{w}_t)]^2
$$
By applying gradient descent&lt;/p>
&lt;p>$$
\begin{aligned}
\mathbf{w}_{t+1} &amp;amp; \doteq \mathbf{w}_{t}-\frac{1}{2} \alpha \nabla \left[v_{\pi}\left(S_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right]^{2} \\
&amp;amp;=\mathbf{w}_{t}+\alpha\left[v_{\pi}\left(S_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right] \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
\end{aligned}
$$&lt;/p>
&lt;p>$$
\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha\left[U_{t}-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right] \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
$$&lt;/p>
&lt;p>If $U_t$ is an &lt;strong>unbiased&lt;/strong> estimate, $\mathbf{w}_t$ is guaranteed to converge to a local optimum.&lt;/p>
&lt;h3 id="monte-carlo-vfa">Monte Carlo VFA&lt;/h3>
&lt;p>Use $G_t$ as an unbiased estimat of $v_\pi(S_t)$&lt;/p>
&lt;p>$$
\mathbf{w} \leftarrow \mathbf{w}+\alpha\left[G_{t}-\hat{v}\left(S_{t}, \mathbf{w}\right)\right] \nabla \hat{v}\left(S_{t}, \mathbf{w}\right)
$$&lt;/p>
&lt;h3 id="semi-gradient-td0">Semi-gradient TD(0)&lt;/h3>
&lt;p>$$
\mathbf{w} \leftarrow \mathbf{w}+\alpha\left[R+\gamma \hat{v}\left(S^{\prime}, \mathbf{w}\right)-\hat{v}(S, \mathbf{w})\right] \nabla \hat{v}(S, \mathbf{w})
$$&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>The idea of state aggregation&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;h2 id="linear-approximators">Linear Approximators&lt;/h2>
&lt;p>A linear function is one of the most important special cases.
$$
\hat{v}(s, \mathbf{w}) \doteq \mathbf{w}^{\top} \mathbf{x}(s) \doteq \sum_{i=1}^{d} w_{i} x_{i}(s)
$$&lt;/p>
&lt;p>The vector $\mathbf{x}(s)$ is called a &lt;em>feature vector&lt;/em> representing state $s$.&lt;/p>
&lt;p>Specifically, if $\hat{v}(S_t, \mathbf{w}) = \mathbf{x}(s)^T \mathbf{w}$, then&lt;/p>
&lt;p>$$
\mathbf{w} \leftarrow \mathbf{w}+
\alpha\left[U_{t}-\mathbf{x}(s)^T \mathbf{w} \right] \mathbf{x}(s)
$$&lt;/p>
&lt;p>&lt;strong>MC update&lt;/strong>
$$
\mathbf{w} \leftarrow \mathbf{w}+
\alpha\left[G_{t}-\mathbf{x}(s)^T \mathbf{w} \right] \mathbf{x}(s)
$$
&lt;strong>TD update&lt;/strong>
$$
\begin{aligned}
\mathbf{w}_{t+1} &amp;amp; \doteq \mathbf{w}_{t}+\alpha\left(R_{t+1}+\gamma \mathbf{w}_{t}^{\top} \mathbf{x}_{t+1}-\mathbf{w}_{t}^{\top} \mathbf{x}_{t}\right) \mathbf{x}_{t} \\
&amp;amp;=\mathbf{w}_{t}+\alpha\left(R_{t+1} \mathbf{x}_{t}-\mathbf{x}_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right)^{\top} \mathbf{w}_{t}\right)
\end{aligned}
$$
The expected next weight vector can be written
$$
\mathbb{E}\left[\mathbf{w}_{t+1} \mid \mathbf{w}_{t}\right]=\mathbf{w}_{t}+\alpha\left(\mathbf{b}-\mathbf{A} \mathbf{w}_{t}\right)
$$
where
$$
\mathbf{b} \doteq \mathbb{E}\left[R_{t+1} \mathbf{x}_{t}\right] \in \mathbb{R}^{d} \quad \text { and } \quad \mathbf{A} \doteq \mathbb{E}\left[\mathbf{x}_{t}\left(\mathbf{x}_{t}-\gamma \mathbf{x}_{t+1}\right)^{\top}\right] \in \mathbb{R}^{d \times d}
$$
Hence we obtain &lt;em>TD ﬁxed point&lt;/em>
$$
\mathbf{w}_{\mathrm{TD}} \doteq \mathbf{A}^{-1} \mathbf{b}
$$
Semi-gradient TD(0) converges to this point.&lt;/p>
&lt;h2 id="control-methods-with-approximation">Control Methods with Approximation&lt;/h2></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/categories/cs234/" term="cs234" label="cs234"/></entry><entry><title type="text">Lecture 4.5: n-step Bootstrapping</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cs234/lecture4-cont/"/><id>https://allenz-me.github.io/posts/cs234/lecture4-cont/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-12T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">$n$-step TD Prediction The idea of $n$-step TD Monte Carlo target $$ G_{t} \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots+\gamma^{T-t-1} R_{T} $$ 1-step TD target $$ G_{t: t+1} \doteq R_{t+1}+\gamma V_{t}\left(S_{t+1}\right) $$ 2-step TD target $$ G_{t: t+2} \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} V_{t+1}\left(S_{t+2}\right) $$ n-step TD……</summary><content type="html">&lt;h3 id="n-step-td-prediction">$n$-step TD Prediction&lt;/h3>
&lt;p>The idea of $n$-step TD&lt;/p>
&lt;img src="../../figures/lecture4.5/image-20220212161120843.png" alt="image-20220212161120843" style="zoom:67%;" />
&lt;p>Monte Carlo target
$$
G_{t} \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots+\gamma^{T-t-1} R_{T}
$$&lt;/p>
&lt;p>1-step TD target
$$
G_{t: t+1} \doteq R_{t+1}+\gamma V_{t}\left(S_{t+1}\right)
$$
2-step TD target
$$
G_{t: t+2} \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} V_{t+1}\left(S_{t+2}\right)
$$
n-step TD target
$$
G_{t: t+n} \doteq R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1} R_{t+n}+\gamma^{n} V_{t+n-1}\left(S_{t+n}\right)
$$
State-value learning algorithm for using n-step returns is
$$
V_{t+n}\left(S_{t}\right) \doteq V_{t+n-1}\left(S_{t}\right)+\alpha\left[G_{t: t+n}-V_{t+n-1}\left(S_{t}\right)\right], \quad 0 \leq t&amp;lt;T
$$&lt;/p>
&lt;img src="../../figures/lecture4.5/image-20220212162104250.png" alt="image-20220212162104250" style="zoom:67%;" />
&lt;p>The n-step TD methods thus form a family of sound methods, with one-step TD methods and Monte Carlo methods as extreme members.&lt;/p>
&lt;h3 id="n-step-sarsa">$n$-step Sarsa&lt;/h3>
&lt;p>$$
G_{t: t+n} \doteq R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1} R_{t+n}+\gamma^{n} Q_{t+n-1}\left(S_{t+n}, A_{t+n}\right), n \geq 1,0 \leq t&amp;lt;T-n
$$&lt;/p>
&lt;p>$$
Q_{t+n}\left(S_{t}, A_{t}\right) \doteq Q_{t+n-1}\left(S_{t}, A_{t}\right)+\alpha\left[G_{t: t+n}-Q_{t+n-1}\left(S_{t}, A_{t}\right)\right], \quad 0 \leq t&amp;lt;T
$$&lt;/p>
&lt;h3 id="n-step-off-policy-learning">$n$-step Off-policy Learning&lt;/h3>
&lt;h3 id="n-step-tree-backup-algorithm">$n$-step Tree Backup Algorithm&lt;/h3></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/categories/cs234/" term="cs234" label="cs234"/></entry><entry><title type="text">The Big Data Newsvendor: Practical Insights from Machine Learning</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/9/"/><id>https://allenz-me.github.io/posts/papers/9/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-12T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Operations Research, 2019. DOI: https://doi.org/10.1287/opre.2018.1757. Subject Classiﬁcations: inventory/production; stochastic: programming; stochastic: statistics; estimation Area of Review: Operations and Supply Chains Keywords: big data • newsvendor……</summary><content type="html">&lt;p>发表在 Operations Research, 2019. DOI: &lt;a href="https://doi.org/10.1287/opre.2018.1757">https://doi.org/10.1287/opre.2018.1757&lt;/a>.&lt;/p>
&lt;p>Subject Classiﬁcations: inventory/production; stochastic: programming; stochastic: statistics; estimation&lt;/p>
&lt;p>Area of Review: Operations and Supply Chains&lt;/p>
&lt;p>Keywords: big data • newsvendor • machine learning • sample average approximation • statistical learning theory • quantile regression&lt;/p>
&lt;hr>
&lt;p>文章是2015年初写的，2018年初接收。这篇文章把对需求的刻画，从统计学跃进到了数据科学。明天的需求，可能跟方方面面的因素有关系（天气、汇率、节假日等等），这汇聚成了 big data。文章提出了一种 &lt;strong>distribution-free, one-step&lt;/strong> machine-learning 的算法来解决大数据时代下的 newsvendor 问题。&lt;/p>
&lt;blockquote>
&lt;p>We investigate the data-driven newsvendor problem when one has n observations of p features related to the demand as well as historical demand data.&lt;/p>
&lt;/blockquote>
&lt;p>文章围绕四个问题展开：&lt;/p>
&lt;ol>
&lt;li>How should the DM use a feature-demand data set to solve the newsvendor problem?&lt;/li>
&lt;li>What is the value of incorporating features in newsvendor decision making in the ﬁrst place?&lt;/li>
&lt;li>What theoretical guarantees does the DM using such data have, and how do these scale with the various problem parameters?&lt;/li>
&lt;li>How do newsvendor decisions based on the feature-demand data set compare with other benchmarks in practice?&lt;/li>
&lt;/ol>
&lt;p>传统的 newsvendor 问题是：
$$
\begin{array}{c}
C(q ; D):=b(D-q)^{+}+h(q-D)^{+} \\
\displaystyle\min _{q \geq 0} E C(q):=\mathbb{E}[C(q ; D)] \\
\end{array}
$$
这是一个随机规划问题，SAA方法给出的解是：
$$
\hat{q}_{n}=\inf \left\{y: \hat{F}_{n}(y) \geq \frac{b}{b+h}\right\}
$$
而 big-data newsvendor 问题是：
$$
\min _{q(\cdot) \in \mathscr{L},\{q: \mathscr{x} \rightarrow \mathbb{R}\}} \mathbb{E}[C(q(\mathbf{x}) ; D(\mathbf{x})) \mid \mathbf{x}]
$$
其中 $\mathbf{x}$ 是 high-dimensional features。优化的目的是找到一个规则，输入特征，输出订货量，并使得期望的损失最小。$\mathscr{L}$ 是一族可行的规则。&lt;/p>
&lt;p>说白了，就是直接训练一个模型，输入特征，输出订货量。&lt;/p>
&lt;p>关于这类问题的求解，文章给出了参数和非参数式两种方法。&lt;/p>
&lt;p>假定有数据集 $S_n = [(\mathbf{x}_1, d_1), \dots, (\mathbf{x}_n, d_n)]$.&lt;/p>
&lt;h4 id="empirical-risk-minimization">Empirical Risk Minimization&lt;/h4>
&lt;p>假定这个可行集 $\mathscr{L}$ 是线性参数化的：
$$
\mathscr{L}=\left\{q: \mathscr{X} \rightarrow \mathbb{R}: q(\mathbf{x})=\mathbf{q}^{\prime} \mathbf{x}=\sum_{j=1}^{p} q^{j} x^{j}\right\}
$$
并且以经验风险作为优化目标：
$$
\min _{q(\cdot) \in \mathscr{L} \;\{q: \mathscr{x} \rightarrow \mathbb{R}\}} \hat{R}\left(q(\cdot) ; S_{n}\right) =\frac{1}{n} \sum_{i=1}^{n}\left[b\left(d_{i}-q\left(\mathbf{x}_{i}\right)\right)^{+}+h\left(q\left(\mathbf{x}_{i}\right)-d_{i}\right)^{+}\right]
$$
最终这个问题归结为线性参数的线性规划问题。&lt;/p>
&lt;p>我们还可以把特征的交叉项，甚至参数的正则化考虑进去。&lt;/p>
&lt;h4 id="kernel-optimization-method">Kernel Optimization Method&lt;/h4>
&lt;p>随后，文章列举了一些相近的文献并 argue 了本文的优点。&lt;/p>
&lt;hr>
&lt;p>在下一部分，文章举了两个示例，并证明了 SAA 方法不是一致的，而 ERM 方法是一致的。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Two-Population Model&lt;/strong>
$$
D=D_{0}(1-x)+D_{1} x
$$&lt;/p>
&lt;/blockquote>
&lt;p>最后，文章做了一个 case study&lt;/p>
&lt;blockquote>
&lt;p>Although some analytical comparisons are possible under assumptions about the true demand model, the ultimate test of data-driven methods must be on real data sets.&lt;/p>
&lt;/blockquote>
&lt;p>研究的问题是医院在某一天应该安排多少个护士，数据显示病人的数量与时间有一定的相关性：&lt;/p>
&lt;img src="../../figures/9/image-20220213230314335.png" alt="image-20220213230314335" style="zoom:50%;" />
&lt;p>文章设定了一些特征，如星期几、时间、前几天的病人数量，然后比较了多种方法的效果。&lt;/p>
&lt;img src="../../figures/9/image-20220214000418419.png" alt="image-20220214000418419" style="zoom: 50%;" />
&lt;p>论文提出的方法在效果上可圈可点。&lt;/p>
&lt;!--
因为 newsvendor 问题是 zero lead-time 的，如果把它应用到 inventory control 上，如何设计好的特征将是一个重要的问题！
算法层面，每天都能新增一个训练样本，怎样设计计算效率高的算法。
--></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/or/" term="OR" label="OR"/><category scheme="https://allenz-me.github.io/tags/newsvendor/" term="Newsvendor" label="Newsvendor"/><category scheme="https://allenz-me.github.io/tags/machine-learning/" term="Machine Learning" label="Machine Learning"/><category scheme="https://allenz-me.github.io/tags/prescriptive-analytics/" term="Prescriptive Analytics" label="Prescriptive Analytics"/></entry><entry><title type="text">Operations Research Algorithms Drive Intelligent Warehouse Robots to Work</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/jd/"/><id>https://allenz-me.github.io/posts/papers/jd/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-09T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Informs Journal on Applied Analytics, 2021. DOI: https://doi.org/10.1287/inte.2021.1100 Key words: intelligent warehouse • robotic system • automatic guided vehicle (AGV) • integer program • cutting planes • dispatching • e-commerce • order picking • order fulﬁ……</summary><content type="html">&lt;p>发表在 Informs Journal on Applied Analytics, 2021. DOI: &lt;a href="https://doi.org/10.1287/inte.2021.1100">https://doi.org/10.1287/inte.2021.1100&lt;/a>&lt;/p>
&lt;p>Key words: intelligent warehouse • robotic system • automatic guided vehicle (AGV) • integer program
• cutting planes • dispatching • e-commerce • order picking • order fulﬁllment • Edelman Award&lt;/p>
&lt;hr>
&lt;p>在这篇文章里面，京东的团队叙述了智能仓库的建设历程，并在附录里面简要介绍了涉及到的一个核心算法的简化版本。&lt;/p>
&lt;p>首先，文章以“亚洲一号”起笔，叙述了京东物流的发展目标、发展历程和发展现状。&lt;/p>
&lt;p>文章重点介绍了京东无人仓库建设期遇到的种种问题。不仅是技术上（运筹、机器人、软件、网络）的问题，&lt;strong>财务上&lt;/strong>、（公司）政策也存在很多问题。整个项目历经立项、研究可行性、模拟、部门批准、实地建设、算法改进、压测等多个环节。&lt;/p>
&lt;p>中间插叙了智能仓库的核心组成部分和遇到的问题。&lt;/p>
&lt;img src="../../figures/7/jd.png" style="zoom: 67%;" />
&lt;p>传统的人工仓库是 picker-to-parts 模式，工人走到货架上取到货物再拿回打包中心装箱。而无人仓库采用 parts-to-picker 模式，AGVs 把 racks 移动到 workstation，再由 workstation 的机器手臂取下 racks 上的货物进行组装打包。&lt;/p>
&lt;blockquote>
&lt;p>An AGV picks up a rack within the picking area and brings it to a workstation, where some requested SKUs are retrieved.&lt;/p>
&lt;/blockquote>
&lt;p>文章在附录将以上过程建模为一个纯整数规划问题，并设计了近似算法进行求解。&lt;/p>
&lt;p>【此处挖坑待填】&lt;/p>
&lt;!-- 用到了 Lagrange multiplier --></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/applied-or/" term="Applied OR" label="Applied OR"/></entry><entry><title type="text">Lecture 4: Model Free Control</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cs234/lecture4/"/><id>https://allenz-me.github.io/posts/cs234/lecture4/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-05T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Lecture 4 主要介绍无模型的 control，包含 MC control 和 TD control。 On-policy learning Direct experience Learn to estimate and evaluate a……</summary><content type="html">&lt;p>Lecture 4 主要介绍无模型的 control，包含 MC control 和 TD control。&lt;/p>
&lt;ul>
&lt;li>On-policy learning
&lt;ul>
&lt;li>Direct experience&lt;/li>
&lt;li>Learn to estimate and evaluate a policy from experience obtained from following that policy&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Off-policy learning
&lt;ul>
&lt;li>Learn to estimate and evaluate a policy using experience gathered from following a different policy&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="monte-carlo-control">Monte Carlo Control&lt;/h2>
&lt;h3 id="monte-carlo-with-exploring-starts">Monte Carlo with Exploring Starts&lt;/h3>
&lt;img src="../../figures/lecture4/mces.png" alt="" style="zoom:67%;" />
&lt;p>A Blackjack game is presented to elucidate MCES.&lt;/p>
&lt;h3 id="on-policy-mc-control">On-policy MC Control&lt;/h3>
&lt;p>Maintain an $\epsilon$-greedy policy&lt;/p>
&lt;img src="../../figures/lecture4/mc-onpolicy.png" alt="" style="zoom:67%;" />
&lt;p>Only achieve the best policy among the $\epsilon$-greedy policies.&lt;/p>
&lt;p>&lt;strong>Greedy in the Limit of Infinite Exploration (GLIE)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>All state-action pairs are visited an infinite number of times
$$
\lim _{i \rightarrow \infty} N_{i}(s, a) \rightarrow \infty
$$&lt;/li>
&lt;li>Behavior policy (policy used to act in the world) converges to greedy policy
$\lim _{i \rightarrow \infty} \pi(a \mid s) \rightarrow \arg \max _{a} Q(s, a)$ with probability 1&lt;/li>
&lt;/ul>
&lt;p>A simple GLIE strategy is $\epsilon$-greedy where $\epsilon$ is reduced to 0 with the following rate: $\epsilon_{i}=1 / i$&lt;/p>
&lt;p>GLIE Monte-Carlo control converges to the optimal state-action value function $Q(s, a) \to Q^\ast(s, a)$.&lt;/p>
&lt;h3 id="off-policy-mc-control">Off-policy MC Control&lt;/h3>
&lt;p>Require that the behavior policy be soft, to ensure each pair of state and action be visited.&lt;/p>
&lt;h2 id="td-control">TD Control&lt;/h2>
&lt;h3 id="on-policy-sarsa">On-policy SARSA&lt;/h3>
&lt;p>Quintuple of events $(S_t, A_t, R_t, S_{t+1}, A_{t+1}) \to \text{SARSA}$&lt;/p>
&lt;p>$$
Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_{t}, A_{t}\right)\right]
$$&lt;/p>
&lt;p>SARSA for finite-state and finite-action MDPs converges to the optimal action-value, $Q(s, a) \rightarrow Q^{\ast}(s, a)$, under the following conditions (&lt;em>Robbins-Munro sequence&lt;/em>)&lt;/p>
&lt;ol>
&lt;li>The policy sequence $\pi_{t}(a \mid s)$ satisfies the condition of GLIE&lt;/li>
&lt;li>The step-sizes $\alpha_{t}$ satisfy the Robbins-Munro sequence such that
$$
\begin{aligned}
&amp;amp;\sum_{t=1}^{\infty} \alpha_{t}=\infty \\
&amp;amp;\sum_{t=1}^{\infty} \alpha_{t}^{2}&amp;lt;\infty
\end{aligned}
$$&lt;/li>
&lt;/ol>
&lt;p>A typical selection is $\alpha_t = o(1/t)$ .&lt;/p>
&lt;h3 id="off-policy-q-learning">Off-policy Q-learning&lt;/h3>
&lt;p>$$
Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right]
$$&lt;/p>
&lt;p>Directly approximates $q^\ast$, the optimal state-action value function.&lt;/p>
&lt;p>Converges to optimal $q^\ast$ if visit all $(s, a)$ pairs infinitely often and $\alpha_t$ satisfy Robbins-Munro sequence.&lt;/p>
&lt;h3 id="expected-sarsa">Expected Sarsa&lt;/h3>
&lt;p>$$
\begin{aligned}
Q\left(S_{t}, A_{t}\right) &amp;amp; \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \mathbb{E}_{\pi}\left[Q\left(S_{t+1}, A_{t+1}\right) \mid S_{t+1}\right]-Q\left(S_{t}, A_{t}\right)\right] \\
&amp;amp; \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \sum_{a} \pi\left(a \mid S_{t+1}\right) Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right]
\end{aligned}
$$&lt;/p>
&lt;p>Expected Sarsa eliminates the variance due to the random selection of $A_{t+1}$, thus outperforms Sarsa by and large.&lt;/p>
&lt;p>It can safely set $\alpha=1$ without suffering any degradation of asymptotic performance.&lt;/p>
&lt;p>If $\pi$ is a greedy policy, expected sarsa is exactly Q-learning.&lt;/p>
&lt;p>&lt;strong>In a nutshell, expected Sarsa subsumes and generalizes Q-learning while reliably improving over Sarsa.&lt;/strong>&lt;/p>
&lt;h3 id="maximization-bias">Maximization Bias&lt;/h3>
&lt;p>Consider single-state MDP $(|S|=1)$ with 2 actions, and both actions have 0-mean random rewards, ie. $\mathbb{E}\left(r \mid a=a_{1}\right)=\mathbb{E}\left(r \mid a=a_{2}\right)=0$.&lt;/p>
&lt;p>Then $Q\left(s, a_{1}\right)=Q\left(s, a_{2}\right)=0=V(s)$ for any policy.&lt;/p>
&lt;p>However, the esimate can be biased&lt;/p>
&lt;p>$$
\hat{V}^{\hat{\pi}}(s)=\mathbb{E}\left[\max \{ \hat{Q}\left(s, a_{1}\right), \hat{Q}\left(s, a_{2}\right)\} \right] &amp;gt; \max \left\{ \mathbb{E}\left[\hat{Q}\left(s, a_{1}\right)\right],\left[\hat{Q}\left(s, a_{2}\right)\right]\right\} =\max [0,0]=V^{\pi}
$$&lt;/p>
&lt;p>The greedy policy w.r.t. estimated $Q$ values can yield a maximization bias during finite-sample learning.&lt;/p>
&lt;h3 id="double-q-learning">Double Q-learning&lt;/h3>
&lt;img src="../../figures/lecture4/image-20220305161630917.png" alt="" style="zoom:50%;" /></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/categories/cs234/" term="cs234" label="cs234"/><category scheme="https://allenz-me.github.io/tags/" term="Tags" label="Tags"/></entry><entry><title type="text">Lecture 3: Model Free Policy Evaluation</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cs234/lecture3/"/><id>https://allenz-me.github.io/posts/cs234/lecture3/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-04T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Lecture3 主要介绍当我们不知道模型的各个参数的时候，如何评价一个 policy. Recall Deﬁnition of Return D……</summary><content type="html">&lt;p>Lecture3 主要介绍当我们不知道模型的各个参数的时候，如何评价一个 policy.&lt;/p>
&lt;h3 id="recall">Recall&lt;/h3>
&lt;ul>
&lt;li>Deﬁnition of Return&lt;/li>
&lt;li>Deﬁnition of State Value Function&lt;/li>
&lt;li>Deﬁnition of State-Action Value Function&lt;/li>
&lt;/ul>
&lt;p>Dynamic programming for policy evaluation&lt;/p>
&lt;p>$$
V^{\pi}(s) \leftarrow \mathbb{E}_{\pi}\left[r_{t}+\gamma V_{k-1} \mid s_{t}=s\right]
$$&lt;/p>
&lt;img src="../../figures/lecture3/bts.png" alt="" style="zoom: 60%;" />
&lt;h2 id="policy-evaluation-without-a-model">Policy Evaluation without a Model&lt;/h2>
&lt;h3 id="monte-carlo-policy-evaluation">Monte Carlo Policy Evaluation&lt;/h3>
&lt;ul>
&lt;li>If trajectories are all finite, sample set of trajectories &amp;amp; average returns&lt;/li>
&lt;li>Does not require MDP dynamics/rewards&lt;/li>
&lt;li>No bootstrapping&lt;/li>
&lt;li>Does not assume state is Markov (handles non-Markovian domains)&lt;/li>
&lt;li>Can only be applied to episodic MDPs&lt;/li>
&lt;li>Averaging over returns from a complete episode&lt;/li>
&lt;li>Requires each episode to terminate&lt;/li>
&lt;/ul>
&lt;p>Monte Carlo methods can be incremental in an episode-by-episode sense, but not in a step-by-step (online) sense.&lt;/p>
&lt;p>Monte Carlo is particularly useful when a subset of states is required. One can generate many sample episodes starting from the states of interest, averaging returns from only these states, ignoring all others.&lt;/p>
&lt;h4 id="first-visit">First-Visit&lt;/h4>
&lt;p>Initialize $N(s)=0, G(s)=0 \;\; \forall s \in S$
Loop&lt;/p>
&lt;ul>
&lt;li>Sample episode $i=s_{i, 1}, a_{i, 1}, r_{i, 1}, s_{i, 2}, a_{i, 2}, r_{i, 2}, \ldots, s_{i, T_{i}}$&lt;/li>
&lt;li>Define $G_{i, t}=r_{i, t}+\gamma r_{i, t+1}+\gamma^{2} r_{i, t+2}+\cdots \gamma^{T_{i}-1} r_{i, T_{i}}$ as return from time
step $t$ onwards in $i$ th episode&lt;/li>
&lt;li>For each time step $t$ till the end of the episode $i$
&lt;ul>
&lt;li>If this is the &lt;strong>first&lt;/strong> time $t$ that state $s$ is visited in episode $i$
&lt;ul>
&lt;li>Increment counter of total first visits: $N(s)=N(s)+1$&lt;/li>
&lt;li>Increment total return $G(s)=G(s)+G_{i, t}$&lt;/li>
&lt;li>Update estimate $V^{\pi}(s)=G(s) / N(s)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Properties&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Unbiased&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Consistent&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>By SLLN, the sequence of averages of the estimates converges to the expected value.&lt;/p>
&lt;h4 id="every-visit">Every-Visit&lt;/h4>
&lt;p>Initialize $N(s)=0, G(s)=0 \; \forall s \in S$
Loop&lt;/p>
&lt;ul>
&lt;li>Sample episode $i=s_{i, 1}, a_{i, 1}, r_{i, 1}, s_{i, 2}, a_{i, 2}, r_{i, 2}, \ldots, s_{i, T_{i}}$&lt;/li>
&lt;li>Define $G_{i, t}=r_{i, t}+\gamma r_{i, t+1}+\gamma^{2} r_{i, t+2}+\cdots \gamma^{T_{i}-1} r_{i, T_{i}}$ as return from time
step $t$ onwards in $i$ th episode&lt;/li>
&lt;li>For each time step $t$ till the end of the episode $i$
&lt;ul>
&lt;li>state $s$ is the state visited at time step $t$ in episodes $i$&lt;/li>
&lt;li>Increment counter of total visits: $N(s)=N(s)+1$&lt;/li>
&lt;li>Increment total return $G(s)=G(s)+G_{i, t}$&lt;/li>
&lt;li>Update estimate $V^{\pi}(s)=G(s) / N(s)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Properties&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Biased&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Consistent, and better MSE&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="incremental-monte-carlo">Incremental Monte Carlo&lt;/h4>
&lt;p>A more computationally efficient way is:
$$
V^{\pi}(s)=V^{\pi}(s) \frac{N(s)-1}{N(s)}+\frac{G_{i, t}}{N(s)}=V^{\pi}(s)+\frac{1}{N(s)}\left(G_{i, t}-V^{\pi}(s)\right)
$$&lt;/p>
&lt;p>$$
V^{\pi}(s)=V^{\pi}(s)+\alpha\left(G_{i, t}-V^{\pi}(s)\right)
$$&lt;/p>
&lt;p>Incremental MC with $\alpha&amp;gt;\displaystyle\frac{1}{N\left(s\right)}$ could help in non-stationary domains.&lt;/p>
&lt;p>&lt;strong>Monte Carlo Policy Evaluation Key Limitations&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Generally high variance estimator
&lt;ul>
&lt;li>Reducing variance can require a lot of data&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Requires episodic settings
&lt;ul>
&lt;li>Episode must end before data from that episode can be used to update the value function&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Problem of maintaining exploration&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Many state–action pairs may never be visited&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Monte Carlo with Exploring Starts&lt;/strong>&lt;/p>
&lt;p>Specify that the episodes start in a state–action pair, and that every pair has a nonzero probability of being selected as the start.&lt;/p>
&lt;h3 id="mc-off-policy-evaluation">MC off-policy evaluation&lt;/h3>
&lt;p>Aim: estimate &lt;em>target policy&lt;/em> $\pi$ given episodes generated under &lt;em>behavior policy&lt;/em> $b$&lt;/p>
&lt;p>Requirement
$$
\pi(a \mid s)&amp;gt;0 \Longrightarrow b(a\mid s) &amp;gt; 0 \tag{coverage}
$$
&lt;em>Importance-sampling ratio&lt;/em>
$$
\rho_{t: T-1} \doteq \frac{\prod_{k=t}^{T-1} \pi\left(A_{k} \mid S_{k}\right) p\left(S_{k+1} \mid S_{k}, A_{k}\right)}{\prod_{k=t}^{T-1} b\left(A_{k} \mid S_{k}\right) p\left(S_{k+1} \mid S_{k}, A_{k}\right)}=\prod_{k=t}^{T-1} \frac{\pi\left(A_{k} \mid S_{k}\right)}{b\left(A_{k} \mid S_{k}\right)}
$$
Given episodes from $b$
$$
\mathbb{E}\left[\rho_{t: T-1} G_{t} \mid S_{t}=s\right]=v_{\pi}(s)
$$
Unbiased and consistent.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Ordinary importance sampling — uausally unbiased; &lt;strong>may not converge&lt;/strong>&lt;/p>
&lt;p>$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t: T(t)-1} G_{t}}{|\mathcal{T}(s)|}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Weighted importance sampling — biased but lower variance
$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t: T(t)-1} G_{t}}{\sum_{t \in \mathcal{T}(s)} \rho_{t: T(t)-1}}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The estimates of ordinary importance sampling will typically have inﬁnite variance, and thus unsatisfactory convergence properties, whenever the scaled returns have inﬁnite variance.&lt;/p>
&lt;h3 id="temporal-difference-learning">Temporal Difference Learning&lt;/h3>
&lt;blockquote>
&lt;p>“If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning.” – Sutton and Barto 2017&lt;/p>
&lt;/blockquote>
&lt;p>Incremental MC&lt;/p>
&lt;p>$$
V^{\pi}(s)=V^{\pi}(s)+\alpha\left(G_{i, t}-V^{\pi}(s)\right)
$$
Replace $G_{i,t}$ by bootstraping $r_t + \gamma V^\pi(s_{t+1})$ .
$$
V^{\pi}\left(s_{t}\right)=V^{\pi}\left(s_{t}\right)+\alpha(\underbrace{\left[r_{t}+\gamma V^{\pi}\left(s_{t+1}\right)\right]}_{\text {TD target }}-V^{\pi}\left(s_{t}\right))
$$&lt;/p>
&lt;ul>
&lt;li>
&lt;p>TD error
$$
\delta_{t}=r_{t}+\gamma V^{\pi}\left(s_{t+1}\right)-V^{\pi}\left(s_{t}\right)
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Can immediately update value estimate after $\left(s, a, r, s^{\prime}\right)$ tuple&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Don't need episodic setting&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Biased, but generally less high variance than MC&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>TD methods are often more efficient than Monte Carlo methods.&lt;/p>
&lt;p>&lt;strong>Conplex convergence property&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>TD(0) converges in the mean for a small constant $\alpha$&lt;/li>
&lt;li>TD(0) converges a.s. if $\alpha$ decreases accordingly&lt;/li>
&lt;li>TD(0) does not always converge with function approximation&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>TD(0) converges to DP policy $V^\pi$ for the MDP with the maximum likelihood model estimates&lt;/strong> if there is available only a ﬁnite amount of experience.&lt;/p>
&lt;blockquote>
&lt;p>Maximum likelihood Markov decision process model
$$
\begin{gathered}
\hat{P}\left(s^{\prime} \mid s, a\right)=\frac{1}{N(s, a)} \sum_{k=1}^{K} \sum_{t=1}^{L_{k}-1} \mathbb{1}\left(s_{k, t}=s, a_{k, t}=a, s_{k, t+1}=s^{\prime}\right) \\
\hat{r}(s, a)=\frac{1}{N(s, a)} \sum_{k=1}^{K} \sum_{t=1}^{L_{k}-1} \mathbb{1}\left(s_{k, t}=s, a_{k, t}=a\right) r_{t, k}
\end{gathered}
$$&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>TD exploits Markov structure.&lt;/strong> As in the AB example&lt;/p>
&lt;blockquote>
&lt;p>A, 0, B, 0&lt;/p>
&lt;p>B, 1&lt;/p>
&lt;p>B, 1&lt;/p>
&lt;p>B, 1&lt;/p>
&lt;p>B, 1&lt;/p>
&lt;p>B, 1&lt;/p>
&lt;p>B, 1&lt;/p>
&lt;p>B, 0&lt;/p>
&lt;/blockquote></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/categories/cs234/" term="cs234" label="cs234"/><category scheme="https://allenz-me.github.io/tags/" term="Tags" label="Tags"/></entry><entry><title type="text">Lecture 2: Making Sequences of Good Decisions Given a Model of the World</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cs234/lecture2/"/><id>https://allenz-me.github.io/posts/cs234/lecture2/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-03T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Lecture2 主要介绍了 MRP、MDP 的概念，以及在 model-based 情况下的策略评估、策略改进（PI + VI）。……</summary><content type="html">&lt;p>Lecture2 主要介绍了 MRP、MDP 的概念，以及在 model-based 情况下的策略评估、策略改进（PI + VI）。&lt;/p>
&lt;p>对应到 Sutton 的书是 Chaper 3 Finite Markov Decision Processes 和 Chapter 4 Dynamic Programming。&lt;/p>
&lt;hr>
&lt;h2 id="basics">Basics&lt;/h2>
&lt;ul>
&lt;li>Model: mathematical models of dynamics and reward&lt;/li>
&lt;li>Policy: function mapping agent’s states to actions&lt;/li>
&lt;li>Value function: future rewards from being in a state and/or action when following a particular policy&lt;/li>
&lt;/ul>
&lt;p>一个 Markov Process 由状态集 $\mathcal{S}$ 和转移概率 $P$ 组成，如果状态集是有限的，那么转移概率 $P$ 可以用一个矩阵来表示。&lt;/p>
&lt;p>Markov Reward Process is a Markov Chain + rewards&lt;/p>
&lt;p>定义：&lt;/p>
&lt;p>discounted sum of rewards&lt;/p>
&lt;p>$$
\begin{aligned}
G_t &amp;amp;= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots\\
&amp;amp;= r_t + \gamma G_{t+1}\\
\end{aligned} \qquad (0 \leq\gamma \leq1)
$$&lt;/p>
&lt;p>expected returen from starting in state $s$&lt;/p>
&lt;p>$$
V(s) = \mathbb{E} [G_t \mid s_t = s]
$$&lt;/p>
&lt;p>Value function estimates that how good it is to be in a given state.&lt;/p>
&lt;p>对于一个 MRP，我们能够计算出在一个状态的期望收益的折现：
$$
V(s)=\underbrace{R(s)}_{\text {Immediate reward }}+\underbrace{\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)}_{\text {Discounted sum of future rewards }}
$$
对于有限状态的 MRP，可以用更简洁的矩阵表示：
$$
\left(\begin{array}{c}
V\left(s_{1}\right) \\
\vdots \\
V\left(s_{N}\right)
\end{array}\right)=\left(\begin{array}{c}
R\left(s_{1}\right) \\
\vdots \\
R\left(s_{N}\right)
\end{array}\right)+\gamma\left(\begin{array}{ccc}
P\left(s_{1} \mid s_{1}\right) &amp;amp; \cdots &amp;amp; P\left(s_{N} \mid s_{1}\right) \\
P\left(s_{1} \mid s_{2}\right) &amp;amp; \cdots &amp;amp; P\left(s_{N} \mid s_{2}\right) \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
P\left(s_{1} \mid s_{N}\right) &amp;amp; \cdots &amp;amp; P\left(s_{N} \mid s_{N}\right)
\end{array}\right)\left(\begin{array}{c}
V\left(s_{1}\right) \\
\vdots \\
V\left(s_{N}\right)
\end{array}\right)
$$
也就是：
$$
V = R + \gamma P V
$$
从解析的角度来说，有：
$$
V = (I - \gamma P)^{-1} R
$$&lt;/p>
&lt;blockquote>
&lt;p>因为 $I- \gamma P$ 的特征值都非0，所以它一定是可逆的。&lt;/p>
&lt;/blockquote>
&lt;p>计算一个矩阵的逆，从数学上来说是很麻烦的，可能不稳定而且计算量大。上面这个问题本质上还是解一个线性方程组，所以通常使用迭代法。&lt;/p>
&lt;blockquote>
&lt;p>数学上，解线性方程组有两类方法：矩阵分解法和迭代法。矩阵分解法（如 LU 分解）适用于小规模的稠密矩阵，而迭代法（如 Jacobi 迭代法）适用于大规模稀疏矩阵。&lt;/p>
&lt;/blockquote>
&lt;p>Markov Decision Process is Markov Reward Process + actions&lt;/p>
&lt;p>这时候转移概率 $P(s^\prime\mid s, a)$ 就变成了现在的状态和行为的函数。It defines the dynamics of the MDP.&lt;/p>
&lt;p>MDP 是一个六元组 $(T, \mathcal{S}, \mathcal{A}, R, P, \gamma)$&lt;/p>
&lt;p>$T= \infty$, infinite horizon; $T &amp;lt; \infty$, finite horizon.&lt;/p>
&lt;p>A trajectory of a MDP begins like:&lt;/p>
&lt;p>$$
S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, R_{3}, \ldots
$$&lt;/p>
&lt;p>Policy specifies what action to take in each state. It can be deterministic or stochastic.&lt;/p>
&lt;p>MDP + $\pi(a \mid s)$ = MRP&lt;/p>
&lt;p>where
$$
\begin{aligned}
R^{\pi}(s) &amp;amp;=\sum_{a \in A} \pi(a \mid s) R(s, a) \\
P^{\pi}\left(s^{\prime} \mid s\right) &amp;amp;=\sum_{a \in A} \pi(a \mid s) P\left(s^{\prime} \mid s, a\right)
\end{aligned}
$$
这说明，评估 MDP 的 policy 可以用计算 MRP 的状态值一样的方法。&lt;/p>
&lt;p>State-value function for policy $\pi$
$$
v^{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]
$$
Action-value function for policy $\pi$
$$
q^{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s, A_{t}=a\right]
$$
Their relations
$$
\begin{aligned}
&amp;amp; v^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a\mid s) q_\pi(s, a) \\
&amp;amp; q^\pi(s, a) = r(s, a) + \sum_{s^{\prime} \in \mathcal{S}} p(s^{\prime} \mid s, a) v_\pi(s^\prime) \\
\end{aligned} \tag{$\\ast$}
$$&lt;/p>
&lt;h2 id="mdp-control">MDP control&lt;/h2>
&lt;p>Value functions deﬁne a partial ordering over policies.
$$
\pi_1 \geq \pi_2 :\; v^{\pi_1}(s) \geq v^{\pi_2}(s) \;\;\forall s \in \mathcal{S}
$$
最优的 policy 不一定是唯一的，但是最优的值函数是唯一的。&lt;/p>
&lt;p>Optimal policy for a MDP in an infinite horizon problem is&lt;/p>
&lt;ul>
&lt;li>Deterministic&lt;/li>
&lt;li>Stationary&lt;/li>
&lt;/ul>
&lt;h3 id="policy-iteration">Policy Iteration&lt;/h3>
&lt;p>分为两步&lt;/p>
&lt;ol>
&lt;li>Policy evaluation: evaluate the state value functions of a given policy&lt;/li>
&lt;/ol>
&lt;p>Dynamic programming for policy evaluation&lt;/p>
&lt;p>$$
V_k^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a| s) \left( r(s, a) + \gamma \sum_{a\in \mathcal{A}} p(s^\prime| s, a) V_{k-1}^\pi(s^\prime) \right)
$$
$V_k^\pi(s)$ is exact value of $k$-horizon value of state $s$ under policy $\pi$.&lt;/p>
&lt;ol start="2">
&lt;li>Policy improvement: find a better policy&lt;/li>
&lt;/ol>
&lt;img src="../../figures/lecture2/pi.png" alt="Policy Iteration" style="zoom:67%;" />
&lt;p>The core step of improvement:
$$
\pi_{i+1}(s) = \underset{a}{\operatorname{argmax}} \; q^{\pi_i}(s, a)
$$&lt;/p>
&lt;h3 id="value-iteration">Value Iteration&lt;/h3>
&lt;img src="../../figures/lecture2/vi.png" alt="" style="zoom:67%;" />
&lt;p>PI converges faster than VI.&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/categories/cs234/" term="cs234" label="cs234"/><category scheme="https://allenz-me.github.io/tags/" term="Tags" label="Tags"/></entry><entry><title type="text">Dynamic Inventory Allocation with Demand Learning for Seasonal Goods</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/5/"/><id>https://allenz-me.github.io/posts/papers/5/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-02-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Production and Operations Management, 2021. DOI: https://doi.org/10.1111/poms.13315 Key words: multi-echelon inventory; demand learning; dynamic programming 这篇文章研究的是 inventory allocation 的问题. Two-echelon network. 要把一个仓库里的……</summary><content type="html">&lt;p>发表在 Production and Operations Management, 2021. DOI: &lt;a href="https://doi.org/10.1111/poms.13315">https://doi.org/10.1111/poms.13315&lt;/a>&lt;/p>
&lt;p>Key words: multi-echelon inventory; demand learning; dynamic programming&lt;/p>
&lt;hr>
&lt;p>这篇文章研究的是 inventory &lt;strong>allocation&lt;/strong> 的问题. Two-echelon network.&lt;/p>
&lt;p>要把一个仓库里的货运给 $N$ 个零售商，零售商之间无法转运货物；总的货物量是 $w_0$；lead time 为0；历史需求信息集 $\mathscr{H}_t$（censored or uncensored）。&lt;/p>
&lt;p>定义 cost:&lt;/p>
&lt;p>$$
L_{i, t}\left(y_{i, t}\right):=p_{i, t}\left[D_{i, t}-y_{i, t}\right]^{+}+h_{i, t}\left[y_{i, t}-D_{i, t}\right]^{+}
$$&lt;/p>
&lt;p>文章把这种库存分配问题建模成了一个动态规划：&lt;/p>
&lt;p>$$
\begin{aligned}
&amp;amp; G_{t}\left(\mathbf{y}, w_{t}, \mathbf{x}_{t}, \mathscr{H}_{t}\right)=\sum_{i=1}^{N} \mathrm{E}\left[L_{i, t}(\mathbf{y}) \mid \mathscr{H}_{t}\right] +\gamma \mathrm{E}\left[V_{t+1}\left(w_{t}-\mathbf{1}^{\mathrm{T}}\left(\mathbf{y}-\mathbf{x}_{t}\right),\left[\mathbf{y}-\mathbf{D}_{t}\right]^{+}, \mathscr{H}_{t+1}\right) \mid \mathscr{H}_{t}\right] \\
&amp;amp; \qquad V_{t}\left(w_{t}, \mathbf{x}_{t}, \mathscr{H}_{t}\right)=\min _{\mathbf{y} \geq \mathbf{x}_{t} \atop \mathbf{1}^{\mathrm{T}} \mathbf{y} \leq \mathbf{1}^{\mathrm{T}} \mathbf{x}_{t}+w_{t}} G_{t}\left(\mathbf{y}, w_{t}, \mathbf{x}_{t}, \mathscr{H}_{t}\right) \\
&amp;amp; \qquad V_{\mathrm{T}+1}\left(w_{0, T+1}, \mathbf{x}_{T+1}, \mathscr{H}_{\mathrm{T}+1}\right)=0 .
\end{aligned}
$$&lt;/p>
&lt;p>它等价于：&lt;/p>
&lt;p>$$
\begin{aligned}
\min\; &amp;amp; \sum_{t=1}^{T} \sum_{i=1}^{N} \gamma^{t-1} \mathrm{E}\left[\mathrm{E}\left[L_{i, t}\left(y_{i, t}\right) \mid \mathscr{H}_{t}\right]\right] \\
\text { s.t. } &amp;amp; y_{i, t}=x_{i, t}+a_{i, t} \forall i, t, \mathscr{H}_{t} \\
&amp;amp; x_{i, t+1}=\left[y_{i, t}-D_{i, t}\right]^{+} \forall i, t, \mathscr{H}_{t} \\
&amp;amp; \sum_{i=1}^{N} \sum_{t=1}^{T} a_{i, t} \leq w_{0} \text { a.s. } \\
&amp;amp; a_{i, t} \geq 0 \;\; \forall i, t, \mathscr{H}_{t} .
\end{aligned}
$$&lt;/p>
&lt;p>在这个过程中，可以以任意的方式进行 demand learning。&lt;/p>
&lt;p>Two types of demand learning models that &lt;em>this framework&lt;/em> can capture are Bayesian methods, where the decision maker updates her beliefs on the unknown demand model parameter distributions with time, and time series models (e.g., ARMA or ARIMA).&lt;/p>
&lt;p>Bayesian methods:&lt;/p>
&lt;p>$$
\mathbb{P}\left[D_{i, t}=d \mid \mathscr{H}_{t}\right]=\int_{\boldsymbol{\theta} \in \Theta} p_{i, t}(d \mid \boldsymbol{\theta}) f\left(\boldsymbol{\theta} \mid \mathscr{H}_{t}\right) \mathrm{d} \boldsymbol{\theta}
$$&lt;/p>
&lt;p>ARMA/ARIMA models:&lt;/p>
&lt;p>$$
D_{i, t}=\mu_{i}+\alpha_{1} D_{i, t-1}+\cdots+\alpha_{p} D_{i, t-p}+\varepsilon_{i, t}+\beta_{1} \varepsilon_{i, t-1}+\cdots+\beta_{q} \varepsilon_{i, t-q}
$$&lt;/p>
&lt;blockquote>
&lt;p>这两种方法都要求 uncensored demand information.&lt;/p>
&lt;/blockquote>
&lt;p>文章设计了一种 Lagrangian Relaxation 的近似方法，并说明了这种方法的渐进最优性。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/pom/" term="POM" label="POM"/><category scheme="https://allenz-me.github.io/tags/inventory/" term="Inventory" label="Inventory"/></entry><entry><title type="text">On Implications of Demand Censoring in the Newsvendor Problem</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/archives/6/"/><id>https://allenz-me.github.io/posts/archives/6/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-31T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Management Science, 2013. DOI: https://doi.org/10.1287/mnsc.1120.1654 Key words: demand censoring; inventory management; newsvendor; estimation; nonparametric Area of review: stochastic models and simulation 这篇文章以 multi-period newsvendor 为背景，研究了 demand censoring 的影……</summary><content type="html">&lt;p>发表在 Management Science, 2013. DOI: &lt;a href="https://doi.org/10.1287/mnsc.1120.1654">https://doi.org/10.1287/mnsc.1120.1654&lt;/a>&lt;/p>
&lt;p>Key words: demand censoring; inventory management; newsvendor; estimation; nonparametric&lt;/p>
&lt;p>Area of review: stochastic models and simulation&lt;/p>
&lt;hr>
&lt;p>这篇文章以 multi-period newsvendor 为背景，研究了 demand censoring 的影响。&lt;/p>
&lt;blockquote>
&lt;p>Therefore, the ﬁrm may only have records of past sales, as opposed to actual demand. This restriction, commonly referred to as &lt;strong>demand censoring&lt;/strong>, inevitably comes at a cost to the ﬁrm, and the purpose of this paper is to further one’s understanding of the implications of censoring.&lt;/p>
&lt;/blockquote>
&lt;p>很多时候，我们只能知道销售额，而不能知道真实的需求信息。&lt;/p>
&lt;p>文章考虑了三种设定：&lt;/p>
&lt;ol>
&lt;li>observable demand （uncensored demand）&lt;/li>
&lt;li>censored demand&lt;/li>
&lt;li>partially censored demand （知道需求是否超过库存）&lt;/li>
&lt;/ol>
&lt;p>记 $a=u, c, pc$，$\pi \in \mathscr{P}^a$ 是一个 nonanticipating policy.&lt;/p>
&lt;p>$C\left(F, x_{t}\right)=h \mathbb{E}\left[\left(x_{t}-D_{t}\right)^{+}\right]+b \mathbb{E}\left[\left(D_{t}-x_{t}\right)^{+}\right]$, $\beta=\displaystyle\frac{b}{h+b}$.&lt;/p>
&lt;p>最优订货量 $x_{F}^{\ast}=\min \{x \in \mathscr{S}: F(x) \geq \beta\}$； $\mathcal{C}^\ast (F, T) = T \cdot \mathcal{C}(F, x_F^\ast)$&lt;/p>
&lt;p>用 regret 衡量 policy 的表现：&lt;/p>
&lt;p>$$
\mathscr{R}^{a}(\mathscr{F}, T)=\inf _{\pi \in \mathscr{P} a} \sup _{F \in \mathscr{F}}\left\{\mathcal{C}^{\pi}(F, T)-\mathcal{C}^{\ast}(F, T)\right\},
$$&lt;/p>
&lt;blockquote>
&lt;p>We measure performance in terms of regret: the difference between the cumulative costs of a policy and the optimal cumulative costs with knowledge of the demand distribution.&lt;/p>
&lt;p>This objective is well posed and can be seen as a game between the decision maker and “nature.” 我们想尽办法设计最好的通用算法，nature 拿出最千奇百怪的现实的例子。&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>对于需求分布连续的情况，文章证明了：&lt;/p>
&lt;p>$$
\sup _{F \in \mathscr{F}}\left\{\mathcal{C}^{\pi}(F, T)-\mathcal{C}^{\ast}(F, T)\right\} \geq \underline{K}_{u}[M+\log T] \quad \forall \pi \in \mathscr{P}^u
$$&lt;/p>
&lt;blockquote>
&lt;p>policies that can achieve a minimax regret of order $\log T$ in the censored case have been developed by, e.g., Huh and Rusmevichientong (2009, §3.5).&lt;/p>
&lt;/blockquote>
&lt;p>这说明连续需求分布情况下，最好也就只能得到 $O(\log T)$ 的 regret。&lt;/p>
&lt;hr>
&lt;p>对于需求分布离散的情况，文章首先在 observable demand 下给出了一个 sample quantile policy $\pi^u$. 每次从经验分布里选取最优的 $\beta$-quantile（即 optimal ordering quantity），$\pi^u$ 的表现是很好的。&lt;/p>
&lt;p>$$
\pi^u: \qquad \begin{aligned}
&amp;amp; q_{t}=\inf \left\{k:(t-1)^{-1} \sum_{i=1}^{t-1} 1\left\{D_{i} \leq k\right\} \geq \beta\right\} \quad[\mathrm{sample \; \beta-quantile}] \\
&amp;amp; x_{t}=\min \left\{q_{t}, M\right\}
\end{aligned}
$$&lt;/p>
&lt;p>它的 regret 有这样一个上界：&lt;/p>
&lt;p>$$
\sup _{F \in \mathscr{F}}\left\{\mathcal{C}^{\pi^{u}}(F, T)-\mathcal{C}^{\ast}(F, T)\right\} \leq \bar{K}_{u} M
$$&lt;/p>
&lt;p>这个上界不依赖于时间 $T$！&lt;strong>这有赖于离散分布的经验分布的分位数是以指数速度收敛于其真实的分位数的。&lt;/strong> 或者说，在一些条件下，经验分布的分位数 收敛速度是比较快的。这是这篇文章用到的一个核心的理论，同时这也揭示了为什么连续分布和离散分布在理论上会形成差异。&lt;/p>
&lt;hr>
&lt;p>接下来文章根据 $\pi^u$ 提出了 $\pi^c$，这是一个用于 cencored demand 情况下的算法：对于先前估计的 $\beta$-quantile做 exploitation，计算出这段时期的 $\beta$-quantile，如果新的分位数比现在的分位数小，说明现在的订货量偏大，以新的 $\beta$-quantile 作为订货量继续 exploitation；如果新的分位数比现在的分位数大，说明订少了，就开启一段时间的 exploration，加大订货量探索真实的需求分布。&lt;/p>
&lt;p>随后，证明了 $\pi^c$ 满足：&lt;/p>
&lt;p>$$
\sup _{F \in \mathscr{F}}\left\{\mathcal{C}^{\pi^{c}}(F, T)-\mathcal{C}^{\ast}(F, T)\right\} \leq \bar{K}_{c} \log M[M \log M+\log T]
$$&lt;/p>
&lt;p>说明了 $\pi^c$ 是 near-optimal 的。&lt;/p>
&lt;p>在 partially censored demand 情况下设计了一个接近 $\pi^c$ 的算法 $\pi^{pc}$ 并证明了：&lt;/p>
&lt;p>$$
\sup _{F \in \mathscr{F}}\left\{\mathcal{C}^{\not^{p c}}(F, T)-\mathcal{C}^{\ast}(F, T)\right\} \leq \bar{K}_{p c}(\log M)^{2} M
$$&lt;/p>
&lt;p>注意到右边的常数是与 $T$ 无关的。&lt;/p>
&lt;p>综上，需求分布是离散/连续 对于该问题有着本质的不同。&lt;/p>
&lt;p>最后，文章以数值实验验证了 regret 的界。&lt;/p>
&lt;img src="../../figures/6/image-20220214095752564.png" alt="image-20220214095752564" style="zoom:50%;" /></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/ms/" term="MS" label="MS"/><category scheme="https://allenz-me.github.io/tags/newsvendor/" term="Newsvendor" label="Newsvendor"/></entry><entry><title type="text">Robust Approximation to Multiperiod Inventory Management</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/archives/4/"/><id>https://allenz-me.github.io/posts/archives/4/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-30T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Operations Research, 2010. DOI: https://doi.org/10.1287/opre.1090.0746 Subject classiﬁcations: robust optimization; inventory control. Area of review: Optimization. 这篇文章提出了用鲁棒……</summary><content type="html">&lt;p>发表在 Operations Research, 2010. DOI: &lt;a href="https://doi.org/10.1287/opre.1090.0746">https://doi.org/10.1287/opre.1090.0746&lt;/a>&lt;/p>
&lt;p>Subject classiﬁcations: robust optimization; inventory control.&lt;/p>
&lt;p>Area of review: Optimization.&lt;/p>
&lt;hr>
&lt;p>这篇文章提出了用鲁棒优化解决多阶段库存管理的方法。文章假设需求分布是未知的，但是知道一些统计信息（support, mean...)。&lt;/p>
&lt;p>文章提到了一个概念，tractable replenishment policy，指可以在多项式时间内计算出来的库存管理策略。使用动态规划来得到库存管理策略有可能不是多项式内可计算的。&lt;/p>
&lt;blockquote>
&lt;p>For instance, the celebrated optimum base-stock policy may not necessarily be a tractable one.&lt;/p>
&lt;/blockquote>
&lt;p>这篇文章用时间序列来考虑需求存在相关性的情况，并且使用了因子模型。&lt;/p>
&lt;blockquote>
&lt;p>Our proposed robust optimization approximation is based upon a comprehensive factor-based demand model that can capture correlations such as the autoregressive nature of demand, the effect of external factors, as well as trends and seasonality, among others.&lt;/p>
&lt;/blockquote>
&lt;p>文章首先给出了 Stochastic Inventory Model：
$$
\begin{aligned}
Z_{\text {STOC }}=&amp;amp;\min \sum_{t=1}^{T}\left(\mathrm{E}\left(c_{t} x_{t}\left(\tilde{\mathbf{d}}_{t-1}\right)\right)+\mathrm{E}\left(h_{t}\left(y_{t+1}\left(\tilde{\mathbf{d}}_{t}\right)\right)^{+}\right)+\mathrm{E}\left(b_{t}\left(y_{t+1}\left(\tilde{\mathbf{d}}_{t}\right)\right)^{-}\right)\right) \\
&amp;amp;\text {s.t. } y_{t+1}\left(\tilde{\mathbf{d}}_{t}\right)=y_{t}\left(\tilde{\mathbf{d}}_{t-1}\right)+x_{t-L}\left(\tilde{\mathbf{d}}_{t-L-1}\right)-\tilde{d}_{t}, t=1, \ldots, T \\
&amp;amp; 0 \leqslant x_{t}\left(\tilde{\mathbf{d}}_{t-1}\right) \leqslant S_{t} \quad t=1, \ldots, T-L .
\end{aligned}
$$&lt;/p>
&lt;p>这里的 lead time = $L$。需求由多个因子决定：&lt;/p>
&lt;p>$$
d_{t}(\widetilde{\mathbf{z}}) \triangleq \tilde{d}_{t}=d_{t}^{0}+\sum_{k=1}^{N} d_{t}^{k} \tilde{z}_{k}, \quad t=1, \ldots, T
$$&lt;/p>
&lt;p>并且有时间序列上的相关性：&lt;/p>
&lt;p>$$
d_{t}(\widetilde{\mathbf{z}})= \begin{cases}d_{t}^{0} &amp;amp; \text { if } t \leqslant 0, \\ \sum_{j=1}^{p} \phi_{j} d_{t-j}(\widetilde{\mathbf{z}})+\tilde{z}_{t}+\sum_{j=1}^{\min \{q, t-1\}} \theta_{j} \tilde{z}_{t-j} &amp;amp; \text { otherwise }\end{cases}
$$&lt;/p>
&lt;p>接着提出用 directional deviations 来表达分布信息：&lt;/p>
&lt;p>Given a random variable $\tilde{z}$, the &lt;strong>forward deviation&lt;/strong> is defined as&lt;/p>
&lt;p>$$
\sigma_{f}(\tilde{z}) \triangleq \sup _{\theta&amp;gt;0}\left\{\sqrt{2 \ln \left(\mathrm{E}(\exp (\theta(\tilde{z}-\mathrm{E}(\tilde{z})))) / \theta^{2}\right.}\right\}
$$&lt;/p>
&lt;p>and &lt;strong>backward deviation&lt;/strong> is defined as&lt;/p>
&lt;p>$$
\sigma_{b}(\tilde{z}) \triangleq \sup _{\theta&amp;gt;0}\left\{\sqrt{2 \ln \left(\mathrm{E}(\exp (-\theta(\tilde{z}-\mathrm{E}(\tilde{z})))) / \theta^{2}\right.}\right\}
$$&lt;/p>
&lt;hr>
&lt;p>文章还提到一个有用的命题：&lt;/p>
&lt;p>(Scarf 1958). Let $\tilde{z}$ be a random variable in $[-\mu, \infty)$ with mean $\mu$ and standard deviation $\sigma$; then, for all $a \geqslant-\mu$,&lt;/p>
&lt;p>$$
\mathrm{E}\left((\tilde{z}-a)^{+}\right) \leqslant\left\{\begin{array}{ll}
\frac{1}{2}\left(-a+\sqrt{\sigma^{2}+a^{2}}\right) &amp;amp; \text { if } a \geqslant \frac{\sigma^{2}-\mu^{2}}{2 \mu} \\
-a \frac{\mu^{2}}{\mu^{2}+\sigma^{2}}+\mu \frac{\sigma^{2}}{\mu^{2}+\sigma^{2}} &amp;amp; \text { if } a&amp;lt;\frac{\sigma^{2}-\mu^{2}}{2 \mu}
\end{array} .\right.
$$&lt;/p>
&lt;p>Moreover, the bound is achievable.&lt;/p>
&lt;hr>
&lt;p>文章在最后讨论了各种 policy 的求解，如&lt;/p>
&lt;p>Static Replenishment Policy:&lt;/p>
&lt;p>$$
x_{t}\left(\tilde{\mathbf{d}}_{t-1}\right)=x_{t}^{0}
$$&lt;/p>
&lt;p>Linear Replenishment Policy:&lt;/p>
&lt;p>$$
x_{t}^{\mathrm{LRP}}\left(\tilde{\mathbf{d}}_{t-1}\right)=x_{t}^{0}+\mathbf{x}_{t}^{\prime} \tilde{\mathbf{z}}
$$&lt;/p>
&lt;p>提出 Truncated Linear Replenishment Policy：&lt;/p>
&lt;p>$$
x_{t}^{\mathrm{TLRP}}\left(\tilde{\mathbf{d}}_{t-1}\right)=\min \left\{\max \left\{x_{t}^{0}+\mathbf{x}_{t}^{\prime} \widetilde{\mathbf{z}}, 0\right\}, S_{t}\right\}
$$&lt;/p>
&lt;p>Policy 是与因子 $\tilde{z}$ 相关联的。并且可以由鲁棒优化高效计算。&lt;/p>
&lt;p>总的来说，我觉得这篇文章写的比较乱，让人一眼看不清主线；&lt;strong>另外大量引用了作者之前做的工作&lt;/strong>。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/or/" term="OR" label="OR"/><category scheme="https://allenz-me.github.io/tags/inventory/" term="Inventory" label="Inventory"/><category scheme="https://allenz-me.github.io/tags/robust-optimization/" term="Robust Optimization" label="Robust Optimization"/></entry><entry><title type="text">Regret in the Newsvendor Model with Partial Information</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/3/"/><id>https://allenz-me.github.io/posts/papers/3/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-29T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Operations Research, 2008. DOI: https://doi.org/10.1287/opre.1070.0486 Subject classiﬁcations: distribution-free inventory policy; newsvendor model; robust optimization; entropy; value of information; semi-in……</summary><content type="html">&lt;p>发表在 Operations Research, 2008. DOI: &lt;a href="https://doi.org/10.1287/opre.1070.0486">https://doi.org/10.1287/opre.1070.0486&lt;/a>&lt;/p>
&lt;p>Subject classiﬁcations: distribution-free inventory policy; newsvendor model; robust optimization; entropy; value of information; semi-inﬁnite linear optimization.&lt;/p>
&lt;p>Area of review: Manufacturing, Service, and Supply Chain Operations.&lt;/p>
&lt;hr>
&lt;p>文章研究部分需求分布信息下的 Newsvender 问题。&lt;/p>
&lt;p>值得一提的贡献有两点：&lt;/p>
&lt;ol>
&lt;li>在经典的报童模型的基础上，假定需求分布虽然是未知的，但是可以获取到部分统计信息（矩、单峰等），给出了一个 robust 的解。&lt;/li>
&lt;li>在给定的统计信息下，最大化信息熵的分布，往往是鲁棒优化眼中的 worst case distribution.&lt;/li>
&lt;/ol>
&lt;p>设 $\Pi_F(y)$ 是订货量 $y$ 在需求分布为 $F$ 下的期望利润，经典的鲁棒优化的优化目标是让最坏情况下的利润最大：&lt;/p>
&lt;p>$$
\max_{y \geqslant 0} \min_{F \in \mathscr{D}} \Pi_F(y)
$$&lt;/p>
&lt;p>但是这样子做决策会导致过于保守。当需求的均值易知，缺货成本为0时，鲁棒优化给出的最优订货量是0。&lt;/p>
&lt;p>文章用的决策准则是 regret：&lt;/p>
&lt;p>$$
\rho^{\ast}=\min _{y \geqslant 0} \rho(y)=\min _{y \geqslant 0} \max _{F \in \mathscr{D}} \max _{z \geqslant 0}\left\{\Pi_{F}(z)\right\}-\Pi_{F}(y)
$$&lt;/p>
&lt;p>分布集 $\mathscr{D}$ 包含我们对需求分布的 initial beliefs。这种决策方式使最大的遗憾最小化。&lt;/p>
&lt;p>&lt;em>这提示我们建立鲁棒优化目标函数的时候也许可以跟 regret 结合起来。&lt;/em>&lt;/p>
&lt;p>文章提取出了一个内层的无穷维的最优化问题：&lt;/p>
&lt;p>$$
\begin{aligned}
\max _{F \in \mathscr{D}} &amp;amp; \int_{\Omega}(\min \{x, z\}-\min \{x, y\}) d F(x) \\
\text { s.t. } &amp;amp; \int_{\Omega} x^{i} d F(x)=q_{i} \quad \forall i=0, \ldots, n .
\end{aligned}
$$&lt;/p>
&lt;p>已知分布的各阶矩，要找到一个最优的分布 $F$.&lt;/p>
&lt;p>如果 Slater's condition 成立，那么强对偶性成立，且它的对偶是一个 semi-infinite 的线性规划：&lt;/p>
&lt;p>$$
\begin{aligned}
\min _{\alpha_{0}, \ldots, \alpha_{n}} &amp;amp; \sum_{i=0}^{n} \alpha_{i} q_{i} \\
\text { s.t. } &amp;amp; \sum_{i=0}^{n} \alpha_{i} x^{i}-(\min \{x, z\}-\min \{x, y\}) \in \mathscr{C}^{\ast},
\end{aligned}
$$&lt;/p>
&lt;blockquote>
&lt;p>Bertsimas and Popescu (2002, 2005) showed that problem, when $\mathscr{D}$ has only moment constraints, can be formulated as a semideﬁnite optimization problem and be therefore efﬁciently solved.&lt;/p>
&lt;/blockquote>
&lt;p>有文献说明了这类优化问题能够得到很好的求解。&lt;/p>
&lt;p>接下来，文章对各类需求分布的统计信息下的最优订货量做了具体而细致的推导和阐述：&lt;/p>
&lt;p>如果只知道需求分布的 support，那么 worst case 的需求分布恰好是均匀分布，这恰好是最大熵的情况。&lt;/p>
&lt;p>接着文章还讨论了，只知道需求分布的 mean、mean and median、mean and symmetry、unimodality, mode and median 等情况。证明过程在 e-companion 里，非常长。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/or/" term="OR" label="OR"/><category scheme="https://allenz-me.github.io/tags/newsvendor/" term="Newsvendor" label="Newsvendor"/></entry><entry><title type="text">Large-Scale Linear Optimization</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/large-scale-linear-opt/"/><id>https://allenz-me.github.io/posts/operations/large-scale-linear-opt/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-26T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">本文介绍大规模线性规划问题的求解思想。内容主要来源自 《Introduction to Linear O……</summary><content type="html">&lt;p>本文介绍大规模线性规划问题的求解思想。内容主要来源自 《Introduction to Linear Optimization》Chapter 6.&lt;/p>
&lt;h2 id="delayed-column-generation">Delayed column generation&lt;/h2>
&lt;p>考虑一个非退化的线性规划的标准问题：&lt;/p>
&lt;p>$$
\begin{array}{cl}
\min &amp;amp; c^T x \\
\text{s.t.} &amp;amp; Ax = b, x \geq 0
\end{array}\tag{1}
$$&lt;/p>
&lt;p>假如说矩阵 $A\in \mathrm{R}^{m \times n}$ 的列数非常大，即 $n \gg m$，这时候我们无法把所有的列都放入内存执行计算。但是，注意到问题只有 $m$ 个基变量，也就是说我们只需要找到 $n$ 列中特定的 $m$ 列，就可以完成单纯型法的迭代，找到最优解了！&lt;/p>
&lt;p>我们可以先随便选取一组基变量进行计算， 接着我们就要去找 entering variable，找进基变量的准则是 reduced cost $\bar{c}_j = c_j - c_B^T B^{-1} A_j &amp;lt; 0$。当然，一般会选择 $\bar{c}_j$ 最小的指标 $j$ 进基，这就归结于问题：&lt;/p>
&lt;p>$$
\min \;\bar{c}_j = c_j - c_B^T B^{-1} A_j
$$&lt;/p>
&lt;p>对于一些具有特殊结构的线性规划，只要如上的问题可以轻松解决，即找到进基指标 $j$ 和相应的列 $A_j$，那么原问题就可以得到高效地求解！&lt;/p>
&lt;blockquote>
&lt;p>注：矩阵 $B$ 可以借助 revised simplex 方法进行迭代，因此计算量也是小的。&lt;/p>
&lt;/blockquote>
&lt;p>下料问题是一个经典的、可以使用列生成算法来求解的问题。&lt;/p>
&lt;h2 id="cutting-stock-problem">Cutting-stock problem&lt;/h2>
&lt;p>下料问题由Kantorovich在1939年提出，是一个 NP-Hard 问题。&lt;/p>
&lt;p>假设一个造纸厂生产长度为 $W$ 的纸，然而，$m$ 个顾客需要的是 $b_i$ 卷长度为 $w_i &amp;lt; W$ 的纸（$i=1, \dots, m$）。那么，造纸厂最少需要多少卷纸来满足顾客的需求呢？&lt;/p>
&lt;p>为了解决这个问题，用一个列向量 $A_j = [a_{ij}]_{m \times 1}$ 表示一卷长度为 $W$ 的纸是如何切割成若干个长度为 $w_i$ 的纸的，其中 $a_{ij}$ 表示第 $j$ 种切法中切出长度为 $w_i$ 的纸的数量。这样的话，对于一个向量 $[a_{1j}, a_{2j}, \dots, a_{mj}]^T$ ，它成为一种可行的切法的充要条件是满足：&lt;/p>
&lt;p>$$
A_j^T {w} =\sum_{i=1}^m a_{ij} w_i \leq W, \quad a_{ij} \in \mathrm{N}
$$&lt;/p>
&lt;p>令矩阵 $A=[A_j]_{1\times n} \in \mathrm{R}^{m\times n}$ 表示所有可行的切割方法，注意到 $n$ 可能非常大，这会给问题带来困难。&lt;/p>
&lt;p>下料问题归结为一下优化问题：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \;&amp;amp; \sum_{j=1}^n x_j \\
\text{s.t.} \;&amp;amp; \sum_{j=1}^n a_{ij}x_j \geq b_i, \;\; i =1, 2, \dots, m \\
&amp;amp; \: x_j \geq 0, \;\; x_j \in \mathbb{N}, \;\; j = 1, 2, \dots, n
\end{aligned}
$$&lt;/p>
&lt;p>决策变量 $x_j$ 表示第 $j$ 种切割方法执行的数量。这是一个整数规划问题，把整数条件放松掉，我们求解一个线性规划问题能轻松得到原整数规划问题的一个上界。（向上取整即可），于是其线性松弛可以表示为：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \;&amp;amp; \sum_{j=1}^n x_j \\
\text{s.t.} \;&amp;amp; \sum_{j=1}^n a_{ij}x_j = b_i, \;\; i =1, 2, \dots, m \\
&amp;amp; \: x_j \geq 0, \;\; j = 1, 2, \dots, n
\end{aligned}
$$&lt;/p>
&lt;p>注意到 $n \gg m$，可行的切割方案数是非常大的，所以，这里可以使用 delayed column generation 的思想。不妨初始化设置 $B = I_{m\times m}$，接下来解优化问题：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \;&amp;amp; 1 - c_B^T B^{-1} A_j \\
\text{s.t.} \;&amp;amp; [w_1, \dots, w_m] A_j \leq W, \quad a_{ij} \in \mathbb{N} \\
\end{aligned}
$$&lt;/p>
&lt;p>找到合适的进基变量和对应的列 $A_j$. 这里的目标函数是变量 $x_j$ 在单纯型表中的系数。&lt;/p>
&lt;blockquote>
&lt;p>注意到这里是一个整数规划问题，但是可以用动态规划算法以伪多项式的时间内求解。&lt;/p>
&lt;/blockquote>
&lt;p>接下来用 revised simplex 更新矩阵 $B$. 迭代到上述问题的最优值大于等于0的时候，原问题达到最优，单纯型迭代停止。这样我们就解决了一个整数规划的线性松弛问题。&lt;/p>
&lt;p>最后，对线性松弛最优解向上取证，可得： IP-optimal cost $\leq$ LP-optimal cost + $m$. 在 $m$ 较小的时候，这是一个良好的近似解。&lt;/p>
&lt;h2 id="cutting-plane-method">Cutting-plane method&lt;/h2>
&lt;p>列生成算法针对的是线性规划中列数特别多的情况，而切平面方法针对的是约束条件特别多的情况。这两种方法的联系可以理解为，列生成针对的是原问题，而切平面解决的是对偶问题：&lt;/p>
&lt;p>$$
\begin{array}{cl}
\max &amp;amp; p^T b \\
\text{s.t.} &amp;amp; p^T A \leq c^T
\end{array} \tag{2}
$$&lt;/p>
&lt;p>注意到，如果 $A_{m\times n}$ 的列数 $n$ 非常大时，上述问题的约束条件非常之多，使得单纯型法的基变量个数也非常多。&lt;/p>
&lt;p>类似地，我们没有必要同时考虑所有的约束条件，如果我们考虑一个子集上的：&lt;/p>
&lt;p>$$
\begin{array}{cl}
\max &amp;amp; p^T b \\
\text{s.t.} &amp;amp; p^T A_i \leq c_i , \;\; i \in I
\end{array}
$$&lt;/p>
&lt;p>计算出的最优值 $\bar{p}$ 是问题(2)的可行解，那么 $\bar{p}$ 就一定是(2)的最优解！&lt;/p>
&lt;p>如果不可行，那么，求解&lt;/p>
&lt;p>$$
\min \;c_i - p^T A_i
$$&lt;/p>
&lt;p>找到一组使 $\bar{p}$ 不可行的约束条件和对应的 $A_i$ ，继续迭代就可以了。因此，cutting-plane method 能否应用归结于如上问题是否能高效求解！&lt;/p>
&lt;blockquote>
&lt;p>加入新的约束条件时，用对偶单纯型法继续迭代。&lt;/p>
&lt;/blockquote>
&lt;p>对原问题执行列生成等价于对对偶问题执行切平面！&lt;/p>
&lt;h2 id="dantzig-wolfe-decomposition">Dantzig-Wolfe decomposition&lt;/h2>
&lt;p>考虑如下的线性规划：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp; {c}_{1}^{\prime} {x}_{1}+ {c}_{2}^{\prime} {x}_{2} \\
\text { s.t. } &amp;amp; {D}_{1} {x}_{1}+ {D}_{2} {x}_{2}= {b}_{0} \\
&amp;amp; {F}_{1} {x}_{1}= {b}_{1} \\
&amp;amp; {F}_{2} {x}_{2}= {b}_{2} \\
&amp;amp; {x}_{1}, {x}_{2} \geq {0}
\end{aligned} \tag{3}
$$&lt;/p>
&lt;p>假设 $b_0, b_1, b_2$ 的维度分别是 $m_0, m_1, m_2$，对于 $m_1, m_2 \gg m_0$ 的情况，可以设计恰当的分解算法来高效求解。&lt;/p>
&lt;p>定义多面体&lt;/p>
&lt;p>$$
P_i = \{x \mid F_i x = b_i\}
$$&lt;/p>
&lt;p>于是原问题可以写成：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp; {c}_{1}^{\prime} {x}_{1}+ {c}_{2}^{\prime} {x}_{2} \\
\text { s.t. } &amp;amp; {D}_{1} {x}_{1}+ {D}_{2} {x}_{2}= {b}_{0} \\
&amp;amp; x_1 \in P_1, \;\; x_2 \in P_2
\end{aligned} \tag{4}
$$&lt;/p>
&lt;p>根据 Minkowski-Weyl 定理，一个 polyhedron 可以由若干个极点和极线构成，于是可以把 $x_1, x_2$ 改写成：&lt;/p>
&lt;p>$$
{x}_{i}=\sum_{j \in J_{i}} \lambda_{i}^{j} {x}_{i}^{j}+\sum_{k \in K_{i}} \theta_{i}^{k} {w}_{i}^{k}, \qquad \sum_{j \in J_{i}} \lambda_{i}^{j}=1, \;\; \lambda_i^j , \theta_i^k \geq 0 \quad i=1,2
$$&lt;/p>
&lt;p>其中 $J_i, K_i$ 分别表示 $P_1, P_2$ 的极点集，极线集。代入原问题，可得：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp; \sum_{j \in J_{1}} \lambda_{1}^{j} {c}_{1}^{\prime} {x}_{1}^{j}+\sum_{k \in K_{1}} \theta_{1}^{k} {c}_{1}^{\prime} {w}_{1}^{k}+\sum_{j \in J_{2}} \lambda_{2}^{j} {c}_{2}^{\prime} {x}_{2}^{j}+\sum_{k \in K_{2}} \theta_{2}^{k} {c}_{2}^{\prime} {w}_{2}^{k} \\
\text { s.t. } &amp;amp; \sum_{j \in J_{1}} \lambda_{1}^{j} {D}_{1} {x}_{1}^{j}+\sum_{k \in K_{1}} \theta_{1}^{k} {D}_{1} {w}_{1}^{k}+\sum_{j \in J_{2}} \lambda_{2}^{j} {D}_{2} {x}_{2}^{j} + \sum_{k\in K_2}\theta_{2}^{k} {D}_{2} {w}_{2}^{k}= {b}_{0} \\
&amp;amp; \sum_{j \in J_{1}} \lambda_{1}^{j}=1 \\
&amp;amp; \sum_{j \in J_{2}} \lambda_{2}^{j}=1 \\
&amp;amp; \lambda_{i}^{j} \geq 0, \theta_{i}^{k} \geq 0, \quad \forall i, j, k .
\end{aligned} \tag{DW-MP}
$$&lt;/p>
&lt;p>这个问题叫做 Dantzig-Wolfe master problem。注意到这个等价的问题只有 $m_0 + 2$ 个约束条件，当 $m_1, m_2$ 比较大的时候，它很好地降低了单纯型法的存储规模！但是呢，它的列数非常大（变量个数很多），一个多面体的极点、极线的个数是阶乘级别的；幸运的是，我们能够使用列生成的思想去解决它。&lt;/p>
&lt;p>首先我们没有必要同时考虑那么多个极点和极限，先只取一个很小的子集，构成一个 restricted master problem（RMP）。&lt;/p>
&lt;p>设 DW-RMP 的&lt;strong>对偶最优解&lt;/strong>是 $p = c_B^T B^{-1} = \begin{bmatrix} q &amp;amp; r_1 &amp;amp; r_2 \end{bmatrix}$，变量 $\lambda_1^j$ 的 reduced cost 是 $\left( {c}_{1}^{\prime}- {q}^{\prime} {D}_{1}\right) {x}_{1}^{j}-r_{1}$，变量 $\theta_1^k$ 的 reduced cost 是 $\left( {c}_{1}^{\prime}- {q}^{\prime} {D}_{1}\right) {w}_{1}^{k}$. 接下来我们要去检验这些变量的检验数是否小于0，这归结于一个&lt;strong>更小规模&lt;/strong>的线性规划问题：&lt;/p>
&lt;p>$$
\begin{aligned}
\min\; &amp;amp; \left( {c}_{1}^{\prime}- {q}^{\prime} {D}_{1}\right) {x}_{1} \\
\text { s.t. } &amp;amp; {x}_{1} \in P_{1}
\end{aligned} \tag{DW-SP}
$$&lt;/p>
&lt;p>这个问题叫做 Dantzig-Wolfe subproblem （DW-SP），&lt;strong>子问题用来检验最优性&lt;/strong>！&lt;/p>
&lt;p>如果最优值是 $-\infty$，我们能找到一条极线使得 $\left( {c}_{1}^{\prime}- {q}^{\prime} {D}_{1}\right) {w}_{i}^{k} &amp;lt; 0$，这说明 $\theta_i^k$ 应该进基。&lt;/p>
&lt;p>又或者最优值小于 $r_1$ ，也就是能找到一个极点 $x_i^j$ 使得 $\left( {c}_{1}^{\prime}- {q}^{\prime} {D}_{1}\right) {x}_{1}^{j}-r_{1} &amp;lt; 0$，这说明 $\lambda_i^j$ 应该进基。如果算出的最优值不小于 $r_1$，说明达到最优了~~！&lt;/p>
&lt;p>对另一组问题也是类似的：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp;\left( {c}_{2}^{\prime}- {q}^{\prime} {D}_{2}\right) {x}_{2} \\
\text { s.t. } &amp;amp; {x}_{2} \in P_{2},
\end{aligned}\tag{DW-SP}
$$&lt;/p>
&lt;p>列生成在 DW 分解中扮演着非常重要的作用，把求解一个大问题分解成若干个小问题，使计算得到了简化。&lt;/p>
&lt;p>这个方法可以推广到多个可分离变量上：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp; {c}_{1}^{\prime} {x}_{1}+ {c}_{2}^{\prime} {x}_{2}+\cdots+ {c}_{t}^{\prime} {x}_{t} \\
\text { s.t. } &amp;amp; {D}_{1} {x}_{1}+ {D}_{2} {x}_{2}+\cdots+ {D}_{t} {x}_{t}= {b}_{0} \\
&amp;amp; {F}_{i} {x}_{i}= {b}_{i}, \quad i=1,2, \ldots, t, \\
&amp;amp; {x}_{1}, {x}_{2}, \ldots, {x}_{t} \geq {0} .
\end{aligned}
$$&lt;/p>
&lt;p>甚至当 $t=1$ 的时候：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp; {c}^{\prime} {x} \\
\text { s.t. } &amp;amp; {D} {x}= {b}_{0} \\
&amp;amp; {F x} = {b} \\
&amp;amp; {x} \geq {0}
\end{aligned}
$$&lt;/p>
&lt;p>依然可以定义 $P = \{x \mid Fx = b\}$ ，再利用 $P$ 的极点、极线来简化问题。当然，DW 分解并不拘泥于等式约束，只要约束条件的“较难”的一部分能够表达成 polyhedron 的形式即可。&lt;/p>
&lt;p>经验上，DW 分解在迭代的一开始效果比较好，但是最优值的提升可能逐渐变慢，所以经常提前终止并输出一个次优解。&lt;/p>
&lt;p>此外，在迭代的过程中，我们还能逐步更新问题的上界和下界。&lt;/p>
&lt;p>DW-RMP 输出原问题的一个上界。&lt;/p>
&lt;h2 id="stochastic-programming-and-benders-decomposition">Stochastic programming and Benders decomposition&lt;/h2>
&lt;p>Benders 分解借用的是 cutting-plane 的思想。&lt;/p>
&lt;p>对如下的优化问题：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp; c^T x + f^T y \\
\text { s.t. } &amp;amp; Ax + By = b \\
&amp;amp; y \geq 0, x \in X \subseteq \mathrm{R}^n
\end{aligned} \tag{5}
$$&lt;/p>
&lt;p>其中 $X$ 是一个 polyhedron。鉴于 $x, y$ 是关联（coupling）着的，问题的难度会变大。如果确定了 $x$ 能方便地计算出 $y$，那么就可以用 Benders 分解来使问题得到简化。&lt;/p>
&lt;blockquote>
&lt;p>这里的 $x$ 还可以是整数变量，$y$ 是连续取值的。$x$ 确定了，求 $y$ 就是一个简单的线性规划了。&lt;/p>
&lt;/blockquote>
&lt;p>首先，改写为关于 $x$ 的优化问题：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp; c^T x + z \\
\text { s.t. } &amp;amp; x \in X \subseteq \mathrm{R}^n
\end{aligned} \tag{6}
$$&lt;/p>
&lt;p>式(6)称为 BD-Master Problem，其中：&lt;/p>
&lt;p>$$
z = z(x)= \left [ \begin{aligned}
\min \; &amp;amp;f^T y \\
\text { s.t. } &amp;amp; Ax + By = b, y \geq 0
\end{aligned}\right] = \left[
\begin{aligned}
\max \; &amp;amp; \;\alpha^T (b-Ax)\\
\text { s.t. } &amp;amp; B^T \alpha \leq f
\end{aligned} \right]
$$&lt;/p>
&lt;p>根据假定（原问题关于 $x$ 困难而关于 $y$ 容易），给出 $x$，计算 $g(x)$ 是一件轻松的事情。但是，我们还得从对偶问题来计算 $g(x)$，&lt;strong>注意到右侧的对偶问题的可行域是与 $x$ 无关的&lt;/strong>。记右侧这个对偶问题为 BD-Subproblem。&lt;/p>
&lt;p>记多面体 $P = \{\alpha \mid B^T \alpha \leq f\}$，如果 $P$ 是空集，那么，由线性规划的弱对偶性，原问题无解（不可行）或者最优值无界。以下假设 $P$ 是非空的。&lt;/p>
&lt;p>式(6)可以改写为：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp; c^T x + z \\
\text { s.t. } &amp;amp; x \in X \subseteq \mathrm{R}^n \\
&amp;amp; \alpha^T_i (b - Ax) \leq z,\;\; \forall \alpha_i \in J_P \\
&amp;amp; \alpha_j^T (b - Ax) \leq 0, \;\; \forall \alpha_j \in K_P
\end{aligned} \tag{7}
$$&lt;/p>
&lt;p>$J_P, K_P$ 分别表示 $P$ 的极点、极线组成的集合。注意到 Master Problem 的约束条件非常多。式(7)的最优值和式(5)是一样的。&lt;/p>
&lt;p>令 $J_P^\prime =K_P^\prime = \emptyset$，取一个 $x_0$ 开始迭代，如果对偶子问题无上界，那么这意味着原问题无解，解对偶子问题能得到一条极线 $w_0$，将 $w_0$ 加入到 $K_P^\prime$，这样的 $x_0$ 对于原始的问题(5)是不可行的！&lt;/p>
&lt;blockquote>
&lt;p>$w_0$ 导致了一个 Benders feasibility cut.&lt;/p>
&lt;/blockquote>
&lt;p>现在假定对偶子问题是有界的，那么说明 $x_0$ 是式(5)的可行解，于是我们能得到原始问题的一个上界 $\mathrm{UB} = c^T x_0 + z(x_0)$；同时，解 relaxed master problem：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp; c^T x + z \\
\text { s.t. } &amp;amp; x \in X \subseteq \mathrm{R}^n \\
&amp;amp; \alpha^T_i (b - Ax) \leq z,\;\; \forall \alpha_i \in J_P^\prime \\
&amp;amp; \alpha_j^T (b - Ax) \leq 0, \;\; \forall \alpha_j \in K_P^\prime
\end{aligned}
$$&lt;/p>
&lt;p>可以得到一个下界 $\mathrm{LB}$，如果 $\mathrm{LB} = \mathrm{UB}$，那么迭代停止，输出最优解。如果 $\mathrm{UB} \geq \mathrm{LB}$，就要向 BD-RMP 添加 Benders optimality cut，在计算用对偶子问题计算 $z(x_0)$ 的时候，得到的极点 $v_0$ 加入到 $J_P^\prime$，继续迭代即可。&lt;/p>
&lt;p>Benders 分解解决的是一类具有特殊形式的规划问题，大规模随机规划刚好具有这样的形式。
$$
\begin{aligned}
\min \; &amp;amp; c^T x &amp;amp; + \alpha_1 f^T y_1 &amp;amp; + \alpha_2 f^T y_2 + \dots &amp;amp; + \alpha_K f^T y_K \\
\text{ s.t. } &amp;amp; Ax &amp;amp; &amp;amp; &amp;amp;&amp;amp;=b \\
&amp;amp; B_1 x &amp;amp; + Dy_1 &amp;amp; &amp;amp; &amp;amp;= d_1 \\
&amp;amp; B_2 x &amp;amp; &amp;amp; + D y_2 &amp;amp; &amp;amp; = d_2 \\
&amp;amp; \vdots &amp;amp;&amp;amp;&amp;amp;&amp;amp; \vdots \\
&amp;amp; B_K x &amp;amp;&amp;amp;&amp;amp; + Dy_K &amp;amp; = d_K \\
&amp;amp; x, y_1, \dots, y_k \geq 0 \\
\end{aligned}
$$&lt;/p>
&lt;p>Scenario 的个数 $K$ 一般很大，而第一阶段的决策 $x$ 确定下来之后，每个 $y_k$ 的计算是若干个小的子问题。&lt;/p>
&lt;p>令：&lt;/p>
&lt;p>$$
\begin{aligned}
z_{\omega}( {x})= \left[
\begin{aligned}
\min \;&amp;amp; {f}^{\top} {y}_{\omega} \\
\text { s.t. } &amp;amp; {B}_{\omega} {x}+ {D} {y}_{\omega}= {d}_{\omega} \\
&amp;amp; {y}_{\omega} \geq {0} .
\end{aligned}\right]
\end{aligned} = \left[
\begin{aligned}
\max \;\; &amp;amp; {p}_{\omega}^{\top}\left( {d}_{\omega}- {B}_{\omega} {x}\right) \\
\text { s.t. } &amp;amp; {p}_{\omega}^{\top} {D} \leq {f}^{\top}
\end{aligned}
\right]
$$&lt;/p>
&lt;p>问题化为关于 $x$ 的：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\min &amp;amp; {c}^{\top} {x}+\sum_{w=1}^{K} \alpha_{\omega} z_{\omega}( {x}) \\
\text { s.t. } &amp;amp; {A x}= {b} \\
&amp;amp; {x} \geq {0} .
\end{array}
$$&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>本文总结了求解大规模线性规划问题的算法思路。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/tags/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/" term="线性规划" label="线性规划"/><category scheme="https://allenz-me.github.io/tags/%E9%9A%8F%E6%9C%BA%E8%A7%84%E5%88%92/" term="随机规划" label="随机规划"/></entry><entry><title type="text">Duality</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cvxopt/duality/"/><id>https://allenz-me.github.io/posts/cvxopt/duality/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-04T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">对偶是优化里非常重要的一个工具。 拉格朗日对偶函数 对偶问题是研究优化问题的有力武器。考……</summary><content type="html">&lt;p>对偶是优化里非常重要的一个工具。&lt;/p>
&lt;h2 id="拉格朗日对偶函数">拉格朗日对偶函数&lt;/h2>
&lt;p>对偶问题是研究优化问题的有力武器。考虑一个具有标准形式的优化问题：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f_{0}(x) \\
\text {subject to } &amp;amp; f_{i}(x) \leq 0, \quad i=1, \ldots, m \\
&amp;amp; h_{i}(x)=0, \quad i=1, \ldots, p
\end{array}
$$&lt;/p>
&lt;p>记整个的可行域为 $\mathcal{D}$，定义上述问题对应的 &lt;strong>拉格朗日函数（Lagrangian）&lt;/strong> 为&lt;/p>
&lt;p>$$
L(x, \lambda, \nu)=f_{0}(x)+\sum_{i=1}^{m} \lambda_{i} f_{i}(x)+\sum_{i=1}^{p} \nu_{i} h_{i}(x)
$$&lt;/p>
&lt;p>其中 $\lambda_i, \nu_i$ 称为&lt;strong>拉格朗日乘子（Lagrange multiplier）&lt;/strong>，或者对偶变量。 $\operatorname{dom} L=\mathcal{D} \times \mathbf{R}^{m} \times \mathbf{R}^{p}$&lt;/p>
&lt;p>接着，我们定义&lt;strong>拉格朗日对偶函数（Lagrange dual function）&lt;/strong> $g: \mathbf{R}^{m} \times \mathbf{R}^p\to \mathbf{R}$：&lt;/p>
&lt;p>$$
g(\lambda, \nu)=\inf _{x \in \mathcal{D}} L(x, \lambda, \nu)=\inf _{x \in \mathcal{D}}\left(f_{0}(x)+\sum_{i=1}^{m} \lambda_{i} f_{i}(x)+\sum_{i=1}^{p} \nu_{i} h_{i}(x)\right)
$$&lt;/p>
&lt;p>如果拉格朗日函数没有下界，就记 $g=-\infty$。$L(x, \lambda, \nu)$ 是关于 $\lambda,\nu$ 的仿射函数，从而，&lt;strong>对偶函数 $g$ 是一个凹函数，即使原来的优化问题不是凸的&lt;/strong>。&lt;/p>
&lt;p>假设 $\tilde{x}\in\mathcal{D}$，如果 $\lambda \succeq 0$ ，我们可以得到：&lt;/p>
&lt;p>$$
\sum_{i=1}^{m} \lambda_{i} f_{i}(\tilde{x})+\sum_{i=1}^{p} \nu_{i} h_{i}(\tilde{x}) \leq 0
$$&lt;/p>
&lt;p>进而：&lt;/p>
&lt;p>$$
g(\lambda, \nu)=\inf _{x \in \mathcal{D}} L(x, \lambda, \nu) \leq L(\tilde{x}, \lambda, \nu) \leq f_{0}(\tilde{x})
$$&lt;/p>
&lt;p>因为 $g(\lambda, \nu)\leq f_{0}(\tilde{x})$ 对 $\forall \tilde{x}\in\mathcal{D}$ 都成立，对两边取下确界，就有：&lt;/p>
&lt;p>$$
g(\lambda, \nu)\leq p^\ast
$$&lt;/p>
&lt;p>$p^\ast$ 是优化问题的最优值。&lt;/p>
&lt;p>这说明，&lt;strong>原优化问题的最优值是对偶函数的一个上界！&lt;/strong> 这个性质是接下来提出对偶问题的基本出发点。&lt;/p>
&lt;blockquote>
&lt;p>如果是一个 maximize 的问题，拉格朗日函数应为：
$$
L(x, \lambda, \nu)=f_{0}(x)-\sum_{i=1}^{m} \lambda_{i} f_{i}(x)+\sum_{i=1}^{p} \nu_{i} h_{i}(x)
$$
对偶函数为：
$$
g(\lambda, \nu)=\sup _{x \in \mathcal{D}} L(x, \lambda, \nu) \geq L(\tilde{x}, \lambda, \nu) \geq f_{0}(\tilde{x})
$$&lt;/p>
&lt;/blockquote>
&lt;p>对偶函数可以用来求一个困难问题的界。例如下面这个非凸最优化问题：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; x^{T} W x \\
\text {subject to } &amp;amp; x_{i}^{2}=1, \quad i=1, \ldots, n
\end{array}
$$&lt;/p>
&lt;p>它其实是一个组合问题，向量 $x$ 的每个分量要么是 $1$ 要么是 $-1$。写出它的拉格朗日函数：&lt;/p>
&lt;p>$$
\begin{aligned}
L(x, \nu) &amp;amp;=x^{T} W x+\sum_{i=1}^{n} \nu_{i}\left(x_{i}^{2}-1\right) \\
&amp;amp;=x^{T}(W+\operatorname{diag}(\nu)) x-\mathbf{1}^{T} \nu
\end{aligned}
$$&lt;/p>
&lt;p>对偶函数：&lt;/p>
&lt;p>$$
\begin{aligned}
g(\nu) &amp;amp;=\inf _{x} x^{T}(W+\operatorname{diag}(\nu)) x-\mathbf{1}^{T} \nu \\
&amp;amp;=\left\{\begin{array}{ll}
-\mathbf{1}^{T} \nu &amp;amp; W+\operatorname{diag}(\nu) \succeq 0 \\
-\infty &amp;amp; \text { otherwise }
\end{array}\right.
\end{aligned}
$$&lt;/p>
&lt;p>为了得到原问题的一个下界，我们可以取一个特殊的对偶变量：&lt;/p>
&lt;p>$$
\nu=-\lambda_{\min }(W) \mathbf{1}
$$&lt;/p>
&lt;p>这时&lt;/p>
&lt;p>$$
W+\operatorname{diag}(\nu)=W-\lambda_{\min }(W) I \succeq 0
$$&lt;/p>
&lt;p>是对偶可行的。最终我们得到原问题的一个下界：&lt;/p>
&lt;p>$$
p^{\star} \geq-\mathbf{1}^{T} \nu=n \lambda_{\min }(W)
$$&lt;/p>
&lt;p>当然，这个下界也可以将原问题的约束条件放宽成 $\sum_{i=1}^{n} x_{i}^{2}=n$ 来得到。&lt;/p>
&lt;blockquote>
&lt;p>在这里，原问题是一个难解的组合问题，但是对偶问题是一个容易解的SDP。&lt;/p>
&lt;/blockquote>
&lt;h3 id="对偶函数与共轭函数">对偶函数与共轭函数&lt;/h3>
&lt;p>【重写】&lt;/p>
&lt;p>回想一下，$f$ 的共轭函数被定义为：&lt;/p>
&lt;p>$$
f^{\ast}(y)=\sup _{x \in \operatorname{dom} f}\left(y^{T} x-f(x)\right)
$$&lt;/p>
&lt;p>共轭函数与对偶函数具有紧密的关系。考虑以下问题：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f_{0}(x) \\
\text {subject to } &amp;amp; A x \preceq b \\
&amp;amp; C x=d
\end{array}
$$&lt;/p>
&lt;p>它的对偶函数：&lt;/p>
&lt;p>$$
\begin{aligned}
g(\lambda, \nu) &amp;amp;=\inf _{x}\left(f_{0}(x)+\lambda^{T}(A x-b)+\nu^{T}(C x-d)\right) \\
&amp;amp;=-b^{T} \lambda-d^{T} \nu+\inf _{x}\left(f_{0}(x)+\left(A^{T} \lambda+C^{T} \nu\right)^{T} x\right) \\
&amp;amp;=-b^T\lambda -d^T \nu - \sup_x\left(-\left(A^T\lambda +C^T v\right)^T x-f_0(x)\right)\\
&amp;amp;=-b^{T} \lambda-d^{T} \nu-f_{0}^{\ast}\left(-A^{T} \lambda-C^{T} \nu\right)
\end{aligned}
$$&lt;/p>
&lt;p>同时 $g$ 的定义域：&lt;/p>
&lt;p>$$
\operatorname{dom} g=\left\{(\lambda, \nu) |-A^{T} \lambda-C^{T} \nu \in \operatorname{dom} f_{0}^{\ast}\right\}
$$&lt;/p>
&lt;p>&lt;strong>一般地，对于某些特殊形式的优化问题，可以通过共轭函数的形式来表示对偶函数。&lt;/strong>&lt;/p>
&lt;h2 id="拉格朗日对偶问题">拉格朗日对偶问题&lt;/h2>
&lt;p>既然对偶函数的每一个函数值都是原问题的下界，那么是否可以通过计算对偶函数的最大值来逼近原问题的最优值呢？这就导出了&lt;strong>拉格朗日对偶问题&lt;/strong>：&lt;/p>
&lt;p>$$
\begin{array}{ll}
{\text { maximize }} &amp;amp; g(\lambda, \nu) \\
\text { subject to } &amp;amp; \lambda \succeq 0
\end{array}
$$&lt;/p>
&lt;p>其中 $\operatorname{dom} g=\{(\lambda, \nu) |\; g(\lambda, \nu)&amp;gt;-\infty\}$。&lt;/p>
&lt;p>求解拉格朗日对偶问题的好处在于，&lt;strong>无论原问题是不是凸问题，对偶问题都是凸优化类问题！&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>对偶问题的约束条件，往往来源于$\operatorname{dom} g=\{(\lambda, \nu) |\; g(\lambda, \nu)&amp;gt;-\infty\}$这个定义域的要求！&lt;/p>
&lt;/blockquote>
&lt;p>对偶问题同样会有最优值，我们将$(\lambda^\ast, \nu^\ast)$称为对偶变量最优值（&lt;em>dual optimal&lt;/em>）或者最优拉格朗日乘子（&lt;em>optimal Lagrange multipliers&lt;/em>）。&lt;/p>
&lt;h3 id="几类优化问题的对偶问题">几类优化问题的对偶问题&lt;/h3>
&lt;p>以下列举一些常见优化问题的对偶问题的形式&lt;/p>
&lt;h4 id="线性规划">线性规划&lt;/h4>
&lt;p>考虑不等式形式的线性规划：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; c^{T} x \\
\text {subject to } &amp;amp; A x \preceq b
\end{array}
$$&lt;/p>
&lt;p>它的对偶函数：&lt;/p>
&lt;p>$$
g(\lambda)=\inf_{x} L(x, \lambda)=-b^{T} \lambda+\inf _{x}\left(A^{T} \lambda+c\right)^{T} x
$$&lt;/p>
&lt;p>从而：&lt;/p>
&lt;p>$$
g(\lambda) =\left\{\begin{array}{ll}
-b^{T} \lambda &amp;amp; A^{T} \lambda+c=0 \\
-\infty &amp;amp; \text { otherwise }
\end{array}\right.
$$&lt;/p>
&lt;p>所以它的对偶问题是：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{maximize} &amp;amp; -b^{T} \lambda \\
\text {subject to } &amp;amp; A^T\lambda +c =0,\; \lambda \succeq 0.
\end{array}
$$&lt;/p>
&lt;p>以上是不等式形式的对偶。接下来考虑&lt;strong>标准形式&lt;/strong>的线性规划：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; c^{T} x \\
\text {subject to } &amp;amp; A x=b \\
&amp;amp; x \succeq 0
\end{array}
$$&lt;/p>
&lt;p>对偶函数：&lt;/p>
&lt;p>$$
g(\lambda, \nu)=\left\{\begin{array}{ll}
-b^{T} \nu &amp;amp; A^{T} \nu-\lambda+c=0 \\
-\infty &amp;amp; \text { otherwise }
\end{array}\right.
$$&lt;/p>
&lt;p>对偶问题：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\text { maximize } &amp;amp; -b^{T} \nu \\
\text { subject to } &amp;amp; A^{T} \nu-\lambda+c =0
,\; \lambda \succeq 0
\end{array}
$$&lt;/p>
&lt;p>也可以写成：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\text { maximize } &amp;amp; b^{T} \nu \\
\text { subject to } &amp;amp; A^{T} \nu +\lambda=c,\; \lambda \succeq 0
\end{array}
$$&lt;/p>
&lt;blockquote>
&lt;p>这也是锥形式下线性规划标准形式的对偶。&lt;/p>
&lt;/blockquote>
&lt;p>也可以写成不等式形式：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\text { maximize } &amp;amp; b^{T} \nu \\
\text { subject to } &amp;amp; A^{T} \nu \preceq c
\end{array}
$$&lt;/p>
&lt;p>线性规划的不等式形式的对偶问题的形式是等式，而线性规划的等式形式的对偶问题的形式可以写成不等式。&lt;/p>
&lt;h4 id="等式约束的二次规划">等式约束的二次规划&lt;/h4>
&lt;p>考虑问题：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; x^{T} x \\
\text {subject to } &amp;amp; A x=b
\end{array}
$$&lt;/p>
&lt;p>拉格朗日函数：$$L(x, \nu)=x^{T} x+\nu^{T}(A x-b)$$&lt;/p>
&lt;p>$$
\nabla_{x} L(x, \nu)=2 x+A^{T} \nu=0 \Rightarrow x=-(1 / 2) A^{T} \nu
$$&lt;/p>
&lt;p>从而有对偶函数：&lt;/p>
&lt;p>$$
g(\nu)=L\left(-(1 / 2) A^{T} \nu, \nu\right)=-(1 / 4) \nu^{T} A A^{T} \nu-b^{T} \nu
$$&lt;/p>
&lt;p>对偶问题：&lt;/p>
&lt;p>$$
\operatorname{maximize} \quad-(1 / 4) \nu^{T} A A^{T} \nu-b^{T} \nu
$$&lt;/p>
&lt;p>这是一个&lt;strong>无约束&lt;/strong>的凹函数最大化问题！（自然可以转化为无约束的凸优化问题）。从而我们知道，原问题是约束优化问题，对偶问题可能是无约束的问题！&lt;/p>
&lt;h4 id="qcqp">QCQP&lt;/h4>
&lt;p>考虑：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; (1 / 2) x^{T} P_{0} x+q_{0}^{T} x+r_{0} \\
\text {subject to } &amp;amp; (1 / 2) x^{T} P_{i} x+q_{i}^{T} x+r_{i} \leq 0, \quad i=1, \ldots, m
\end{array}
$$&lt;/p>
&lt;p>拉格朗日函数：&lt;/p>
&lt;p>$$
L(x, \lambda)=(1 / 2) x^{T} P(\lambda) x+q(\lambda)^{T} x+r(\lambda)
$$&lt;/p>
&lt;p>其中：&lt;/p>
&lt;p>$$
P(\lambda)=P_{0}+\sum_{i=1}^{m} \lambda_{i} P_{i}, \quad q(\lambda)=q_{0}+\sum_{i=1}^{m} \lambda_{i} q_{i}, \quad r(\lambda)=r_{0}+\sum_{i=1}^{m} \lambda_{i} r_{i}
$$&lt;/p>
&lt;p>在 $\lambda \succeq 0$ 时，$P(\lambda)$ 是正定的，从而：&lt;/p>
&lt;p>$$
g(\lambda)=\inf _{x} L(x, \lambda)=-(1 / 2) q(\lambda)^{T} P(\lambda)^{-1} q(\lambda)+r(\lambda)
$$&lt;/p>
&lt;p>所以有对偶问题：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{maximize} &amp;amp; -(1 / 2) q(\lambda)^{T} P(\lambda)^{-1} q(\lambda)+r(\lambda) \\
\text {subject to } &amp;amp; \lambda \succeq 0
\end{array}
$$&lt;/p>
&lt;p>【添加一些问题的对偶】&lt;/p>
&lt;h3 id="弱对偶性与强对偶性">弱对偶性与强对偶性&lt;/h3>
&lt;p>记 $d^\ast$ 是对偶问题的最优值，通过上面的推证，我们有：&lt;/p>
&lt;p>$$
d^\ast \leq p^\ast
$$&lt;/p>
&lt;p>这个性质称为&lt;strong>弱对偶性&lt;/strong>，它甚至不要求原问题是凸的。&lt;/p>
&lt;p>弱对偶性可以推广到取值为无穷的情况：&lt;/p>
&lt;ul>
&lt;li>如果 $p^\ast=-\infty$，原问题最优值是无界的，则必然有 $d^\ast=-\infty$，也就是对偶问题不可行。&lt;/li>
&lt;li>如果 $d^\ast=+\infty$，对偶问题最优值无界，也必然成立 $p^\ast=+\infty$，也就是说，原问题不可行。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>也可能出现情况：$d^\ast = -\infty, p^\ast = + \infty$，即原问题和对偶问题都不可行！&lt;/p>
&lt;p>如线性规划的原问题：
$$
\begin{aligned}
\min \; &amp;amp; x_{1}+2 x_{2} \\
\text { s.t. } &amp;amp; x_{1}+x_{2}=1 \\
&amp;amp; 2 x_{1}+2 x_{2}=3
\end{aligned}
$$
和对偶问题：
$$
\begin{aligned}
\max\; &amp;amp; y_{1}+3 y_{2} \\
\text { s.t. } &amp;amp; y_{1}+2 y_{2}=1 \\
&amp;amp; y_{1}+2 y_{2}=2
\end{aligned}
$$
都不可行！&lt;/p>
&lt;/blockquote>
&lt;p>$p^\ast-d^\ast$ 称作&lt;strong>对偶间隙（optimal duality gap）&lt;/strong>。如果 $p^\ast=d^\ast$，我们就称&lt;strong>强对偶性&lt;/strong>成立，这意味着，对偶问题的最优值恰好就是原问题的最优值。因此，如果对偶问题更易求解的话，我们往往能够方便地给原问题提供一个下界。&lt;/p>
&lt;p>弱对偶性是一定严格成立的，强对偶性就未必了。&lt;/p>
&lt;p>如果原问题是凸的，&lt;strong>通常&lt;/strong>来说强对偶性是会成立的。建立在凸性上的成立强对偶性的条件有很多，我们把这些条件称作：&lt;strong>constraint qualifications&lt;/strong>。&lt;/p>
&lt;p>&lt;strong>Slater's condition&lt;/strong> 是强对偶性成立的一个充分条件。如果问题是凸的，并且：
$$
\exists \tilde{x}\in \mathcal{D},\; \mathrm{s.t.\;} \,f_i(\tilde{x})&amp;lt;0, A \tilde{x}=b
$$&lt;/p>
&lt;p>这样的点有时候叫做&lt;em>严格可行（strictly feasible）&lt;/em>。在这个条件成立的同时，对偶可行解（如果有限）是可以取到的。&lt;/p>
&lt;h3 id="强对偶性成立的问题">强对偶性成立的问题&lt;/h3>
&lt;h3 id="minimax-定理">minimax 定理&lt;/h3>
&lt;p>对于两人零和博弈，可以只使用一个矩阵 $A$ 来表示，用向量 $x, y$ 分别表示两个人的策略，那么 payoff 就是 $\pm\, y^T Ax$，这里说明，强对偶性可以用来证明 minimax 定理的一种特殊情况：
$$
\max_x \:(\min_y y^TAx) = \min_y \: (\max_x y^T A x)
$$
其中 $x, y \geq 0, x^T\mathbf{1}=y^T\mathbf{1}=1$。&lt;/p>
&lt;blockquote>
&lt;p>在优化问题构建的过程中蕴含了博弈论里的 indifference principle&lt;/p>
&lt;/blockquote>
&lt;p>row player 面临如下的优化问题：
$$
\begin{array}{cl}
\min &amp;amp; \max_{i=1,\dots, n} \; (A^T y)_i \\
\text{s.t.} &amp;amp; y \geq 0, \, y^T \mathbf{1} = 1
\end{array}&lt;br>
$$
等价于：
$$
\begin{array}{cl}
\min &amp;amp; t \\
\text{s.t.} &amp;amp; y \geq 0, \, y^T \mathbf{1} = 1 \\
&amp;amp; A^T y \leq t \mathbf{1}&lt;br>
\end{array}
$$
不难写出它的对偶：
$$
\begin{array}{ll}
\max &amp;amp; \nu \\
\text { s.t. } &amp;amp; \lambda \geq 0, \quad \mathbf{1}^{T} \lambda=1 \\
&amp;amp; A \lambda \geq \nu \mathbf{1} .
\end{array}\tag{\ast}
$$
column player 面临的优化问题是：
$$
\begin{array}{cl}
\max &amp;amp; \min_{i=1,\dots, m} \; (A x)_i \\
\text{s.t.} &amp;amp; x \geq 0, \, x^T \mathbf{1} = 1
\end{array}&lt;br>
$$
实际上这个问题就是(*)式，根据强对偶性，两个问题的最优值相等。&lt;/p>
&lt;h2 id="几何解释">几何解释&lt;/h2>
&lt;p>记 $\mathcal{G}=\left\{\left(f_{1}(x), \ldots, f_{m}(x), h_{1}(x), \ldots, h_{p}(x), f_{0}(x)\right) \in \mathbf{R}^{m} \times \mathbf{R}^{p} \times \mathbf{R} | x \in \mathcal{D}\right\}$，这是一个把目标函数和所有约束都考虑进来的集合。&lt;/p>
&lt;p>原问题的最优解可以很容易用 $\mathcal{G}$ 来表示：
$$
p^{\star}=\inf \{t |(u, v, t) \in \mathcal{G}, u \preceq 0, v=0\}
$$
用 $\mathcal{G}$ 还能来表示对偶函数：
$$
g(\lambda, \nu)=\inf \left\{(\lambda, \nu, 1)^{T}(u, v, t) |(u, v, t) \in \mathcal{G}\right\}
$$
从而我们可以得到弱对偶性：
$$
\begin{aligned}
p^{\star} &amp;amp;=\inf \{t |(u, v, t) \in \mathcal{G}, u \preceq 0, v=0\} \\
&amp;amp; \geq \inf \left\{(\lambda, \nu, 1)^{T}(u, v, t) |(u, v, t) \in \mathcal{G}, u \preceq 0, v=0\right\} \\
&amp;amp; \geq \inf \left\{(\lambda, \nu, 1)^{T}(u, v, t) |(u, v, t) \in \mathcal{G}\right\} \\
&amp;amp;=g(\lambda, \nu)
\end{aligned}
$$&lt;/p>
&lt;h2 id="鞍点">鞍点&lt;/h2>
&lt;p>&lt;img src="../../figures/Duality/310.png" alt="saddle point">&lt;/p>
&lt;p>鞍点这个词，顾名思义，来自马鞍。如上图中标注出的点，它&lt;strong>在一个方向上使函数值达到最小，而在另一个方向上使函数达到最大&lt;/strong>。在光滑的条件下，判断鞍点的一个充分条件是&lt;strong>函数在一阶导数为零处（驻点）的Heisen矩阵为不定矩阵&lt;/strong>。&lt;/p>
&lt;p>且先考虑这么一个拉格朗日函数：&lt;/p>
&lt;p>$$
\begin{aligned}
\sup _{\lambda \succeq 0} L(x, \lambda) &amp;amp;=\sup _{\lambda \succeq 0}\left(f_{0}(x)+\sum_{i=1}^{m} \lambda_{i} f_{i}(x)\right) \\
&amp;amp;= \begin{cases}f_{0}(x) &amp;amp; f_{i}(x) \leq 0, \quad i=1, \ldots, m \\
+ \infty &amp;amp; \text { otherwise }\end{cases}
\end{aligned}
$$&lt;/p>
&lt;p>如果原优化问题不可行，那么 $\sup _{\lambda \succeq 0} L(x, \lambda)$ 可以取到 $+\infty$，而如果 $x$ 满足约束条件，就只能等于目标函数。所以：&lt;/p>
&lt;p>$$
p^{\star}=\inf_x \;\sup_{ \lambda \succeq 0} L(x, \lambda)
$$&lt;/p>
&lt;p>而根据对偶问题的定义，又有：&lt;/p>
&lt;p>$$
d^{\star}=\sup _{\lambda \succeq 0} \inf _{x} L(x, \lambda)
$$&lt;/p>
&lt;p>所以弱对偶性等价于：&lt;/p>
&lt;p>$$
\sup _{\lambda \succeq 0} \inf _{x} L(x, \lambda) \leq \inf _{x} \sup _{\lambda \succeq 0} L(x, \lambda)
$$&lt;/p>
&lt;p>强对偶性等价于：&lt;/p>
&lt;p>$$
\sup _{\lambda \succeq 0} \inf _{x} L(x, \lambda) = \inf _{x} \sup _{\lambda \succeq 0} L(x, \lambda)
$$&lt;/p>
&lt;blockquote>
&lt;p>事实上，
$$
\sup _{z \in Z} \inf _{w \in W} f(w, z) \leq \inf _{w \in W} \sup _{z \in Z} f(w, z)
$$
总是成立的！这叫做 maxmin-inequality.&lt;/p>
&lt;/blockquote>
&lt;p>称 $\tilde{w} \in W, \tilde{z} \in Z$ 是 $f$ 的鞍点（saddle point），如果&lt;/p>
&lt;p>$$
f(\tilde{w}, z) \leq f(\tilde{w}, \tilde{z}) \leq f(w, \tilde{z}), \qquad \forall w \in W, \forall z \in Z
$$&lt;/p>
&lt;p>换句话说， $\tilde{w}$ 最小化 $f(w, \tilde{z})$， $\tilde{z}$ 最大化 $f(\tilde{w}, z)($ over $z \in Z)$ :&lt;/p>
&lt;p>$$
f(\tilde{w}, \tilde{z})=\inf _{w \in W} f(w, \tilde{z}), \quad f(\tilde{w}, \tilde{z})=\sup _{z \in Z} f(\tilde{w}, z)
$$&lt;/p>
&lt;p>所以说，如果 $x^\ast$ 和 $\lambda^\ast$ 分别是原问题和对偶问题的最优点，那么 $(x^\ast, \lambda^\ast)$ 就是拉格朗日函数的鞍点！反之也是成立的。&lt;/p>
&lt;h2 id="optimality-conditions">Optimality conditions&lt;/h2>
&lt;h3 id="complementary-slackness">Complementary slackness&lt;/h3>
&lt;!-- 可以认为这一小节写完了。 -->
&lt;p>对于任意的问题，&lt;strong>如果强对偶性成立&lt;/strong>，记 $x^\star$ 和 $(\lambda^\star, \nu^\star)$ 分别是原问题和对偶问题的最优解，那么：&lt;/p>
&lt;p>$$
\begin{aligned}
f_{0}\left(x^{\star}\right) &amp;amp;=g\left(\lambda^{\star}, \nu^{\star}\right) \\
&amp;amp;=\inf _{x}\left(f_{0}(x)+\sum_{i=1}^{m} \lambda_{i}^{\star} f_{i}(x)+\sum_{i=1}^{p} \nu_{i}^{\star} h_{i}(x)\right) \\
&amp;amp; \leq f_{0}\left(x^{\star}\right)+\sum_{i=1}^{m} \lambda_{i}^{\star} f_{i}\left(x^{\star}\right)+\sum_{i=1}^{p} \nu_{i}^{\star} h_{i}\left(x^{\star}\right) \\
&amp;amp; \leq f_{0}\left(x^{\star}\right)
\end{aligned}
$$&lt;/p>
&lt;p>这意味着：&lt;/p>
&lt;p>$$
\sum_{i=1}^{m} \lambda_{i}^{\star} f_{i}\left(x^{\star}\right)=0
$$&lt;/p>
&lt;p>进而有：&lt;/p>
&lt;p>$$
\lambda_{i}^{\star} f_{i}\left(x^{\star}\right)=0, \quad i=1, \ldots, m
$$&lt;/p>
&lt;p>这就是&lt;strong>互补松弛&lt;/strong>性！(complementary slackness)&lt;/p>
&lt;p>如果 $\lambda_i^\ast&amp;gt;0$，就有 $f_i(x^\ast)=0$；如果 $f_i(x^\ast)&amp;lt;0$，那么 $\lambda_i^\ast=0$。每一个约束都有与它对应的对偶变量（乘子），如果乘子大于0，那么约束必然是紧的 (binding)；如果约束不是紧的 (unbinding)，那么乘子必然为0。&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>有解&lt;/em> 的线性规划必然成立互补松弛定理，它可以看成是强对偶性下的互补松弛的一个特例。&lt;/p>
&lt;p>并且，因为线性规划的对偶的对偶就是它本身，这两重对偶的互补松弛性可以揭示线性规划 primal and dual optimality。这是线性规划的 primal-dual method 的思想。&lt;/p>
&lt;/blockquote>
&lt;h3 id="kkt-optimality-conditions">KKT optimality conditions&lt;/h3>
&lt;p>进一步假设这些函数都是可微的（并没有凸性的假设）。记 $x^\ast$ 和 $(\lambda^\ast, \nu^\ast)$ 分别是原问题和对偶问题的最优解，且强对偶性成立，因为 $x^\ast$ 最小化了 $L(x, \lambda^\ast, \nu^\ast)$，所以：
$$
\nabla f_{0}\left(x^{\star}\right)+\sum_{i=1}^{m} \lambda_{i}^{\star} \nabla f_{i}\left(x^{\star}\right)+\sum_{i=1}^{p} \nu_{i}^{\star} \nabla h_{i}\left(x^{\star}\right)=0
$$
于是，加上互补松弛条件和问题本身的约束，我们有：&lt;/p>
&lt;p>$$
\begin{aligned}
f_{i}\left(x^{\star}\right) &amp;amp; \leq 0, \quad i=1, \ldots, m \\
h_{i}\left(x^{\star}\right) &amp;amp;=0, \quad i=1, \ldots, p \\
\lambda_{i}^{\star} &amp;amp; \succeq 0, \quad i=1, \ldots, m \\
\lambda_{i}^{\star} f_{i}\left(x^{\star}\right) &amp;amp;=0, \quad i=1, \ldots, m \\
\nabla f_{0}\left(x^{\star}\right)+\sum_{i=1}^{m} \lambda_{i}^{\star} \nabla f_{i}\left(x^{\star}\right)+\sum_{i=1}^{p} \nu_{i}^{\star} \nabla h_{i}\left(x^{\star}\right) &amp;amp;=0,
\end{aligned}
$$&lt;/p>
&lt;p>这就是KKT条件（&lt;em>Karush-Kuhn-Tucker conditions&lt;/em>）。总结一下，对任何有着可微目标函数与约束的优化问题，只要强对偶性成立，那么其最优值 $x^\ast$ 和对偶最优 $(\lambda^\ast, \nu^\ast)$ 就必然成立KKT条件。&lt;/p>
&lt;p>在这里，&lt;strong>KKT条件的证明，前提是强对偶性，针对的是全局最优点&lt;/strong>，利用了对偶函数的性质。&lt;/p>
&lt;blockquote>
&lt;p>简而言之，强对偶性（能取到最优解）$\Rightarrow$ 全局最优点成立KKT条件。&lt;strong>注意这里是全局最优而不是局部最优&lt;/strong>！&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>强对偶性成立下KKT条件是优化问题最优解的必要条件！&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>例如问题：$\min x\\ \text{s.t. }\, x^2\leq 0$ 就在最优点（$x=0$）处&lt;strong>不成立KKT条件&lt;/strong>。它的对偶问题是：
$\max ;-\frac{1}{4\lambda}\
st. \lambda &amp;gt; 0$虽然强对偶性还是成立的（$d^&lt;em>=p^&lt;/em>=0$），但是对偶问题没有办法取到最优解。&lt;/li>
&lt;/ul>
&lt;p>如果这个优化问题还是凸的，那么KKT条件还能提供充分性。&lt;/p>
&lt;p>利用KKT条件对最优解的充分性，可以把求凸优化的最优解转化为求满足KKT条件的点。&lt;/p>
&lt;p>比如等式约束的二次规划问题可以利用KKT条件求解。对
$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; (1 / 2) x^{T} P x+q^{T} x+r \\
\text {subject to } &amp;amp; A x=b
\end{array}
$$
它的KKT条件是：
$$
A x^{\star}=b, \quad P x^{\star}+q+A^{T} \nu^{\star}=0
$$
写成矩阵形式就是：
$$
\left[\begin{array}{cc}
P &amp;amp; A^{T} \\
A &amp;amp; 0
\end{array}\right]\left[\begin{array}{l}
x^{\star} \\
\nu^{\star}
\end{array}\right]=\left[\begin{array}{c}
-q \\
b
\end{array}\right]
$$&lt;/p>
&lt;p>关于KKT条件对局部最优解的论述，参见：&lt;a href="https://www.jianshu.com/p/c74bd51749e2">KKT的进一步论述&lt;/a>&lt;/p>
&lt;h2 id="替代定理theorem-of-alternatives">替代定理（Theorem of alternatives）&lt;/h2>
&lt;p>在这一部分，将要使用对偶理论来讨论一系列约束的可行性。&lt;/p>
&lt;p>约束的可行性，可以用优化的理论进行等价表述：&lt;/p>
&lt;p>$$
f_{i}(x) \leq 0, \quad i=1, \ldots, m, \quad h_{i}(x)=0, \quad i=1, \ldots, p
$$&lt;/p>
&lt;p>这个约束是否可行，可以链接到一个优化问题的最优值上。&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; 0 \\
\text {subject to } &amp;amp; f_{i}(x) \leq 0, \quad i=1, \ldots, m \\
&amp;amp; h_{i}(x)=0, \quad i=1, \ldots, p
\end{array} \tag{\ast}
$$&lt;/p>
&lt;p>且：&lt;/p>
&lt;p>$$
p^{\star}= \begin{cases}0 &amp;amp; (\ast) \text { is feasible } \\ \infty &amp;amp; (\ast) \text { is infeasible }\end{cases}
$$&lt;/p>
&lt;p>写出这个优化问题的对偶函数：&lt;/p>
&lt;p>$$
g(\lambda, \nu)=\inf _{x \in \mathcal{D}}\left(\sum_{i=1}^{m} \lambda_{i} f_{i}(x)+\sum_{i=1}^{p} \nu_{i} h_{i}(x)\right)
$$&lt;/p>
&lt;p>注意到 $g(\lambda, \nu)$ 是齐次函数，即 $g(t\lambda, t\nu)=tg(\lambda, \nu), \forall t &amp;gt; 0$。所以：对偶问题的最优值：&lt;/p>
&lt;p>$$
d^{\star}= \begin{cases}\infty &amp;amp; \lambda \succeq 0, \quad g(\lambda, \nu)&amp;gt;0 \text { is feasible } \\ 0 &amp;amp; \lambda \succeq 0, \quad g(\lambda, \nu)&amp;gt;0 \text { is infeasible. }\end{cases}
$$&lt;/p>
&lt;p>弱对偶性告诉我们，至少成立 $d^\ast\leq p^\ast$，所以，如果 $d^\ast=\infty$，那么必然 $p^\ast=\infty$；如果 $p^\ast=0$，那么必然 $d^\ast=0$！这说明，这里的原问题和对偶问题，&lt;strong>最多只有一个&lt;/strong>是可行（feasible）的！这叫做&lt;em>weak alternatives&lt;/em>。&lt;/p>
&lt;blockquote>
&lt;p>如果$p^\ast=\infty,d^\ast=0$，那么意味着都不可行。一个可行能推出另一个不可行，但一个不可行却不能推出另一个可行，所以称作&lt;em>&lt;strong>弱&lt;/strong>&lt;/em> 替代（以区别下面的强替代）&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>回想一下求最大的LP问题的对偶问题没有可行解，那么原问题无上界。&lt;/p>
&lt;/blockquote>
&lt;p>接下来，进一步考虑严格不等式约束的两组条件：&lt;/p>
&lt;p>$$
f_{i}(x)&amp;lt;0, \quad i=1, \ldots, m, \quad h_{i}(x)=0, \quad i=1, \ldots, p\tag{1}
$$&lt;/p>
&lt;p>$$
\lambda \succeq 0, \quad \lambda \neq 0, \quad g(\lambda, \nu) \geq 0\tag{2}
$$&lt;/p>
&lt;p>容易证明，它们满足弱替代性。&lt;/p>
&lt;p>有了弱替代性，自然会想到强替代性，能不能在某种条件下，&lt;strong>原问题和对偶问题的可行域有且只有一个可行&lt;/strong>呢？&lt;/p>
&lt;p>弱替代性不要求原约束条件的凸性。为了得到强替代性，凸性，自然是必须的！&lt;/p>
&lt;p>考虑一组“凸”的约束条件：&lt;/p>
&lt;p>$$
f_{i}(x)&amp;lt;0, \quad i=1, \ldots, m, \quad A x=b, \;\text{let}\: \mathcal{D}=\cap_i^m \mathrm{dom} f_i \tag{3}
$$&lt;/p>
&lt;p>和它的 &lt;em>alternative&lt;/em>：&lt;/p>
&lt;p>$$
\lambda \succeq 0, \quad \lambda \neq 0, \quad g(\lambda, \nu) \geq 0\tag{4}
$$&lt;/p>
&lt;p>这里还需要一个条件，就是式(3)的还要成立 $\exists \tilde{x} \in \mathrm{relint} \mathcal{D}, st. A\tilde{x}=b$，在这种条件下，强替代性是成立的！&lt;/p>
&lt;blockquote>
&lt;p>如果$\mathcal{D}=\mathbf{R}^n$，那么这个条件就总是成立的。&lt;/p>
&lt;/blockquote>
&lt;p>考虑关于(3)式的优化问题：
$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; s \\
\text {subject to } &amp;amp; f_{i}(x)-s \leq 0, \quad i=1, \ldots, m \\
&amp;amp; A x=b
\end{array}
$$
如果(3)式的可行域非空，那么上面的优化问题的最优值$p^\ast&amp;lt;0$。它的对偶问题是：
$$
\begin{array}{ll}
\operatorname{maximize} &amp;amp; g(\lambda, \nu) \\
\text {subject to } &amp;amp; \lambda \succeq 0, \quad \mathbf{1}^{T} \lambda=1
\end{array}
$$
Slater's 条件在这里是成立的，所以强对偶性是成立的！由此不难继续推导出强替代性的成立。&lt;/p>
&lt;p>另外，还有一种情况也成立强替代性：&lt;/p>
&lt;p>&lt;img src="https://upload-images.jianshu.io/upload_images/19526825-90bae7427cab0189.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">&lt;/p>
&lt;p>这里也需要对第一个约束，成立$\exists \tilde{x} \in \mathrm{relint} \mathcal{D}, st. A\tilde{x}=b$这个条件。&lt;/p>
&lt;p>线性关系是最容易导出强替代性的，下面列举一些例子：&lt;/p>
&lt;p>例1：&lt;strong>Farkas 引理&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>$Ax = b, x \geq 0$&lt;/li>
&lt;li>$A^T y \geq 0, b^T y &amp;lt; 0$&lt;/li>
&lt;/ol>
&lt;p>有且仅有一个可行。可以使用如下的一组对偶&lt;/p>
&lt;p>$$
(\mathcal{P})\quad
\begin{array}{ll}
\operatorname{minimize} &amp;amp; \quad c^T x \\
\text {subject to } &amp;amp; A x =b, x \geq 0
\end{array} \qquad
(\mathcal{D})\quad
\begin{array}{ll}
\operatorname { maximize } \; b^T y \\
\text {subject to } \; A^{T} y \leq c
\end{array}
$$&lt;/p>
&lt;p>例2：&lt;strong>Gordan 定理&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>$Ax=0, x\geq 0, x \neq 0$&lt;/li>
&lt;li>$A^T y &amp;gt; 0$&lt;/li>
&lt;/ol>
&lt;p>有且仅有一个可行。可以使用如下的一组对偶：
$$
(\mathcal{P})\quad
\begin{array}{ll}
\operatorname{minimize} &amp;amp; \mathbf{1}^{T} x \\
\text {subject to } &amp;amp; A x = 0, x \succeq 0
\end{array} \qquad
(\mathcal{D})\quad
\begin{array}{ll}
\operatorname { maximize } \quad 0 \\
\text {subject to } \; A^Ty \succeq \mathbf{1}
\end{array}
$$
例3：&lt;/p>
&lt;ol>
&lt;li>$Ax \prec b$&lt;/li>
&lt;li>$y \neq 0, y \succeq 0, A^Ty=0, y^Tb \leq 0$&lt;/li>
&lt;/ol>
&lt;p>有且仅有一个可行。可以使用如下的一组对偶：
$$
(\mathcal{P})\quad
\begin{array}{ll}
\operatorname{minimize} &amp;amp; 0 \\
\text {subject to } &amp;amp; A x \leq b
\end{array} \qquad
(\mathcal{D})\quad
\begin{array}{ll}
\operatorname { maximize } \quad -b^T y \\
\text {subject to } \; A^Ty=0 , y \succeq 0
\end{array}
$$&lt;/p>
&lt;h2 id="generalized-inequalities">Generalized inequalities&lt;/h2>
&lt;p>接下来我们讨论广义不等式下的对偶，这也叫做锥形式的对偶。这种优化问题具有标准形式：
$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f_{0}(x) \\
\text {subject to } &amp;amp; f_{i}(x) \preceq_{K_{i}} 0, \quad i=1, \ldots, m \\
&amp;amp; h_{i}(x)=0, \quad i=1, \ldots, p
\end{array}
$$
类似地，我们可以定义拉格朗日函数：
$$
L(x, \lambda, \nu)=f_{0}(x)+\lambda_{1}^{T} f_{1}(x)+\cdots+\lambda_{m}^{T} f_{m}(x)+\nu_{1} h_{1}(x)+\cdots+\nu_{p} h_{p}(x)
$$
和对偶函数：
$$
g(\lambda, \nu)=\inf _{x \in \mathcal{D}} L(x, \lambda, \nu)=\inf _{x \in \mathcal{D}}\left(f_{0}(x)+\sum_{i=1}^{m} \lambda_{i}^{T} f_{i}(x)+\sum_{i=1}^{p} \nu_{i} h_{i}(x)\right)
$$
注意，&lt;strong>这里的 $\lambda_i \succeq_{K^\ast} 0$ 在 $K$ 的对偶锥里&lt;/strong>，根据对偶锥的性质，我们就有：
$$
\lambda_i^T f_i(x) \leq 0
$$
**这一点保证了弱对偶性的成立！**因此对偶问题具有如下形式：
$$
\begin{array}{ll}
\operatorname{maximize} &amp;amp; g(\lambda, \nu) \\
\text {subject to } &amp;amp; \lambda_{i} \succeq_{K_{i}^{\ast}} 0, \quad i=1, \ldots, m
\end{array}
$$&lt;/p>
&lt;p>强对偶性的Slater's condition可以推广到锥形式的对偶下。即，如果问题是凸的，即 $f_0$ 凸、$f_i$ 关于 $K_i$ 凸，且存在一点 $\tilde x$ 严格可行，那么强对偶成立，对偶间隙为0。&lt;/p>
&lt;p>锥规划依然成立互补松弛条件、KKT条件、替代定理，在此从略。&lt;/p>
&lt;hr>
&lt;h3 id="minimax-theorem">minimax theorem&lt;/h3>
&lt;p>$$
\max _{\boldsymbol{z} \in \cap Z_{i}} \boldsymbol{x}^{\prime} \boldsymbol{z}=\min _{\boldsymbol{x}_{i}, i \in[I]}\left\{\sum_{i \in[I]} \max _{\boldsymbol{z} \in Z_{i}} \boldsymbol{x}_{i}^{\prime} \boldsymbol{z} \mid \sum_{i \in[I]} \boldsymbol{x}_{i}=\boldsymbol{x}\right\}
$$&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/" term="凸优化" label="凸优化"/></entry><entry><title type="text">Convex Optimization Problem</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cvxopt/convex-opt-problem/"/><id>https://allenz-me.github.io/posts/cvxopt/convex-opt-problem/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-03T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">凸集和凸函数，都是为了解决凸优化问题做的铺垫。当然，在这之前，我们还应当对整个优化问……</summary><content type="html">&lt;!-- # Convex optimization problem -->
&lt;p>凸集和凸函数，都是为了解决凸优化问题做的铺垫。当然，在这之前，我们还应当对整个优化问题的概念体系有一个大致的了解。&lt;/p>
&lt;h2 id="优化问题">优化问题&lt;/h2>
&lt;h3 id="基本术语">基本术语&lt;/h3>
&lt;p>一个标准的优化问题，通常都由：优化变量、目标函数、不等式约束、等式约束组成：&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>optimization variable, objective function, inequality constraints, equality constraints.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f_{0}(x) \\
\text {subject to } &amp;amp; f_{i}(x) \leq 0, \quad i=1, \ldots, m \\
&amp;amp; h_{i}(x)=0, \quad i=1, \ldots, p
\end{array}\tag{1}
$$&lt;/p>
&lt;p>满足等式约束和不等式约束的值叫做优化问题的&lt;strong>可行域（feasible set）&lt;/strong>。可行域也要包含在所有函数的定义域内。&lt;/p>
&lt;p>如果优化问题没有约束条件，就说这个问题是无约束的（unconstrained）。&lt;/p>
&lt;p>我们将可行域内 $f_0(x)$ 的&lt;strong>下确界&lt;/strong>定义为问题的 &lt;em>optimal value&lt;/em>。&lt;/p>
&lt;p>$$
p^{\ast}=\inf_{x\in D} f_0(x)
$$&lt;/p>
&lt;p>我们允许 $p^\ast$ 取值为 $\pm \infty$。如果可行域是空集，我们取 $p^\ast=+\infty$；如果存在点列 $x_k$ 使得 $f_0(x_k) \to -\infty \;(k\to \infty)$，就取 $p^\ast = -\infty$，并称这个问题是无界的（unbounded below）。&lt;/p>
&lt;p>如果 &lt;em>optimal value&lt;/em> 在问题的可行域内可以取到，即 $f_0(x^\ast)=p^\ast$，就称 $x^\ast$ 是问题的&lt;strong>最优解（optimal point）&lt;/strong>，注意不是所有问题都能取到最优解。所有的 optimal point 构成 &lt;strong>最优集（optimal set）&lt;/strong>。&lt;/p>
&lt;p>如果该优化问题的最优集不为空，那么就算这个问题是&lt;strong>可解的（solvable）&lt;/strong>，容易看到，并不是所有的可行域非空的优化问题都是可解的。大多数不可解的问题都是无界的问题。&lt;/p>
&lt;blockquote>
&lt;p>约束优化问题 $\displaystyle\min_{x\in \mathrm{R}_{+}}f_0(x) = \displaystyle\frac{1}{x}$ 的最优值 $p^\ast=0$，但是最优集为空，因此它是不可解的。&lt;/p>
&lt;/blockquote>
&lt;p>虽然有时候最优解不存在或者很难计算，但我们可以引入 $\epsilon$-&lt;em>suboptimal&lt;/em> 的解。使得 $f_0(x) \leq p^\ast + \epsilon$ 的 $x$ 可以作为一个 $\epsilon$ 近似的解。&lt;/p>
&lt;p>除去以上所说的全局最优（globally optimal）解的概念，还可以引入一种局部最优（locally optimal）的概念。$x$ 是局部最优仅需要它在一个足够小的领域内是最优的。&lt;/p>
&lt;p>对于不等式约束条件，如果 $f_i(x)&amp;lt;0$，我们就说约束条件 $f_i(x)\leq 0$ 在 $x$ 处是 &lt;em>inactive&lt;/em> 的。不等式约束条件到底有没有起作用，在优化理论中被广泛研究。&lt;/p>
&lt;p>如果令优化问题的目标函数恒等于0，那么这个问题的最优解要么是0，要么是 $+\infty$。研究类似的优化的问题的意义是，最优解可以揭示问题的可行域是否非空。
$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; 0 \\
\text {subject to } &amp;amp; f_{i}(x) \leq 0, \quad i=1, \ldots, m \\
&amp;amp; h_{i}(x)=0, \quad i=1, \ldots, p
\end{array}
$$&lt;/p>
&lt;h3 id="优化问题的标准形式">优化问题的标准形式&lt;/h3>
&lt;p>优化问题的标准形式如本文式(1)。事实上，很多实际问题被提出的时候并不是标准形式，但是我们总能够将它们化为标准形式。&lt;/p>
&lt;p>比如最大值问题添加一个负号就能变成最小值问题。因为凸函数具有全局的最小值点，所以习惯上我们还是考虑最小值问题。&lt;/p>
&lt;h3 id="问题的等价形式equivalent-problems">问题的等价形式（equivalent problems）&lt;/h3>
&lt;p>通过一定的变换，我们可以把一个优化问题变成与它等价的另一个优化问题。有时候，这样的操作可以帮助我们简化问题的求解。我们称两个优化问题等价，当且仅当解决其中一个问题能够立马得到另一个问题的最优解。&lt;/p>
&lt;ul>
&lt;li>变量代换&lt;/li>
&lt;/ul>
&lt;p>如果现在有一个 $\mathrm{R}^n \to \mathrm{R}^n$ 的双射 $\phi$，将 $x=\phi(z)$ 代入标准形式(1)得到的另一个关于 $z$ 的优化问题仍然与原问题等价。&lt;/p>
&lt;ul>
&lt;li>对目标函数或约束函数做变换&lt;/li>
&lt;/ul>
&lt;p>设 $\psi_0(x)$ 是单调递增的函数，那么 $\displaystyle\min_{x \in \mathcal{D}} f_0(x)$ 等价于 $\displaystyle\min_{x \in \mathcal{D}} \psi_0(f_0(x))$。&lt;/p>
&lt;p>例：&lt;/p>
&lt;p>$\min \|A x - b\|_2$ 等价于 $\min \|A x - b \|_2^2 = (Ax - b)^T (Ax - b)$&lt;/p>
&lt;p>对约束条件做类似的等价变换也是可行的。&lt;/p>
&lt;ul>
&lt;li>松弛变量&lt;/li>
&lt;/ul>
&lt;p>比如，在线性规划的单纯形法中，我们通过添加松弛变量将问题化为线性规划的标准形式。添加松弛变量得到的新问题仍然与原问题等价。
$$
f_i(x) \leq 0 \Leftrightarrow f_i(x) + s_i = 0,\; s_i \geq 0
$$&lt;/p>
&lt;ul>
&lt;li>Epigraph 形式&lt;/li>
&lt;/ul>
&lt;p>引入新变量 $t \in \mathrm{R}$，优化问题(1)可等价地转换为：
$$
\begin{aligned}
&amp;amp;\text {minimize }\;\;\quad t\\
&amp;amp;\begin{array}{ll}
\text {subject to } &amp;amp; f_{0}(x)-t \leq 0 \\
&amp;amp; f_{i}(x) \leq 0, \quad i=1, \ldots, m \\
&amp;amp; h_{i}(x)=0, \quad i=1, \ldots, p
\end{array}
\end{aligned}
$$&lt;/p>
&lt;p>许多优化问题都会使用这种处理方式。&lt;/p>
&lt;blockquote>
&lt;p>一点点欠缺&lt;/p>
&lt;/blockquote>
&lt;h3 id="黑盒式优化问题oracle-model">黑盒式优化问题（oracle model）&lt;/h3>
&lt;p>教科书上的优化问题往往都是参数式的优化问题，目标函数 $f_0(x)$ 有一个给定的形式。但也有时候，目标函数会是一个黑盒子（black box），它由一个程序给出，我们输入一个 $x$，程序返回 $f_0(x)$。&lt;/p>
&lt;p>二次规划、二阶锥规划，它们有明确的形式，因此可以对此设计高效的算法；而一些形式不特殊或者是黑盒式的目标函数（比如神经网络），就很难为此设计独特的解法。如求解神经网络的梯度下降法，就是一种通用型的方法。&lt;/p>
&lt;h3 id="最优性条件">最优性条件&lt;/h3>
&lt;h4 id="可行方向">可行方向&lt;/h4>
&lt;p>对于给定的 $x \in \mathcal{D}$，如果存在 $\bar{\alpha}$，使得 $\forall \alpha\; (0 \leq \alpha \leq \bar{\alpha})$，都有 $x + \alpha d \in \mathcal{D}$，就称向量 $d$ 是在 $x$ 处的可行方向（feasible direction）。&lt;/p>
&lt;p>以下条件都假设目标函数二次可微。&lt;/p>
&lt;h4 id="一阶必要条件">一阶必要条件&lt;/h4>
&lt;p>如果 $x^\ast$ 是 $f(x)$ 在 $\mathcal{D}$ 上的局部极小值，那么对于 $x^\ast$ 处任意一个可行方向 $d$ ，都有 $\nabla f(x^\ast) ^T d \geq 0$。这意味着沿着 $x^\ast$ 的任意一个可行方向，函数值都是上升的。&lt;/p>
&lt;p>一个重要的特殊情形是 $x^\ast$ 位于 $\mathcal{D}$ 内部时，此时所有的方向都是可行方向，所以必须成立 $\nabla f(x^\ast) = 0$。这也是无约束最优化局部极小值的必要条件。&lt;/p>
&lt;h4 id="二阶必要条件">二阶必要条件&lt;/h4>
&lt;p>如果 $x^\ast$ 是 $f(x)$ 在 $\mathcal{D}$ 上的局部极小值，那么对于 $x^\ast$ 处任意一个可行方向 $d$ ，有：&lt;/p>
&lt;ol>
&lt;li>$\nabla f(x^\ast)^T d \geq 0$&lt;/li>
&lt;li>如果 $\nabla f(x^\ast)^T d = 0$，那么 $d^T \nabla^2 f(x^\ast) d \geq 0$&lt;/li>
&lt;/ol>
&lt;p>二阶条件是通过 Taylor 展开来证明的。对应的无约束或内点情形，就是 $\nabla f(x^\ast) = 0, \nabla^2 f(x^\ast) \succeq 0$。即梯度为0，Hessian 矩阵半正定。&lt;/p>
&lt;h4 id="二阶充分条件">二阶充分条件&lt;/h4>
&lt;p>极小点在可行域边界处达到的充分条件较难刻画。&lt;/p>
&lt;p>但如果 $x^\ast$ 是 $\mathcal{D}$ 的内点，只要 $\nabla f(x^\ast) = 0, \nabla^2 f(x^\ast) \succ 0$，即可保证 $x^\ast$ 是局部极小点。&lt;/p>
&lt;h2 id="凸优化">凸优化&lt;/h2>
&lt;p>通俗地说，凸优化问题，就是目标函数是凸函数，并且可行域是凸集的优化问题。&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f_{0}(x) \\
\text {subject to } &amp;amp; f_{i}(x) \leq 0, \quad i=1, \ldots, m \\
&amp;amp; a_{i}^{T} x=b_{i}, \quad i=1, \ldots, p
\end{array}
$$&lt;/p>
&lt;p>凸优化问题的标准形式，与一般优化问题的相比，&lt;strong>要求目标函数 $f_0(x)$ 和不等式约束函数 $f_1(x), ..., f_m(x)$ 都是凸函数，并且等式约束都是线性的。&lt;/strong>&lt;/p>
&lt;p>这样的约束条件，保证了&lt;strong>问题的可行域是凸集&lt;/strong>！&lt;/p>
&lt;p>如果目标函数不是凸的，是拟凸的，那么这个问题就是一个拟凸优化问题（&lt;em>quasiconvex optimization problem&lt;/em>）&lt;/p>
&lt;p>当目标函数和不等式约束都变成凹函数并且是求最大值，这个问题叫做凹优化问题。凹优化问题和凸优化问题本质是一样的。&lt;/p>
&lt;h3 id="凸优化的最优性">凸优化的最优性&lt;/h3>
&lt;p>凸优化，相比与一般的优化问题，有一个非常好的性质，那就是：&lt;strong>任何一个局部最优点（locally optimal）都是全局最优点（globally optimal）&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>对于拟凸优化问题而言，局部最优不一定导致全局最优。&lt;/p>
&lt;/blockquote>
&lt;p>如果目标函数是可微的，那么还有一个判断最优点的准则：
&lt;strong>设 $X$ 是可行域，$x$ 最优 $\Longleftrightarrow\nabla f_{0}(x)^{T}(y-x) \geq 0\;\;, \forall y \in X$。&lt;/strong>&lt;/p>
&lt;p>这个命题有着非常好的几何解释：$-\nabla f$ 与 $y-x$ 成钝角
同时 &lt;strong>$\nabla f$ 定义了一个过点 $x$ 的对可行域的支撑超平面。&lt;/strong>&lt;/p>
&lt;p>验证 $x^\ast$ 是 $f_0(x)$ 的最优值，只需要再做一个优化问题 $ \min \nabla f_0 (x^\ast)^T (x - x^\ast)$ 即可。&lt;/p>
&lt;p>如果问题仅包含等式约束 $Ax=b$，那么 $x$ 最优 $\Longleftrightarrow \nabla f\left(x\right)+A^{T} \mu=0, A x=b$。这个可以用之后介绍的KKT条件进行证明。&lt;/p>
&lt;p>如果问题只是变量的非负约束，那么 $x$ 最优 $\Longleftrightarrow \nabla f\left(x\right)_{i} x_{i}=0, x \geq 0, \nabla f\left(x\right) \geq 0$&lt;/p>
&lt;p>如果凸优化问题没有约束条件（Unconstrained problems），那么上面的命题，归结为一个广为人知的充分条件：&lt;/p>
&lt;p>$$
\nabla f_0(x)=0
$$&lt;/p>
&lt;h3 id="常见的凸问题">常见的凸问题&lt;/h3>
&lt;p>很多实际问题都可以归结于或者转化为几类经典的凸优化问题。包括&lt;strong>线性规划（LP）、二次规划（QP）、二次约束二次规划（QCQP）、二阶锥规划（SOCP）、几何规划（GP）&lt;/strong>，接下来依次介绍它们。&lt;/p>
&lt;h4 id="线性规划linear-program">线性规划（Linear Program）&lt;/h4>
&lt;p>线性规划应该是最简单、人们最熟悉的一种凸优化问题了。线性规划问题具有如下的典型形式：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; c^{T} x+d \\
\text {subject to } &amp;amp; G x \preceq h \\
&amp;amp; A x=b
\end{array}
$$&lt;/p>
&lt;p>通过一些变换，如添加松弛变量，引入正部、负部等方法，可以化为&lt;strong>标准形式&lt;/strong>：&lt;/p>
&lt;p>$$
\begin{array}{cl}
\operatorname{minimize} &amp;amp; c^{T} x \\
\;\text { subject to } &amp;amp; A x=b \\
&amp;amp; x \succeq 0
\end{array}
$$&lt;/p>
&lt;p>对于标准形式的线性规划问题，本科的运筹学课程应当会介绍&lt;strong>单纯形法&lt;/strong>。这是根据线性规划可行域的特点提出的一种求解方法，因为线性规划的最优值如果存在那么必然取在可行域的极点上。&lt;/p>
&lt;blockquote>
&lt;p>线性规划的单纯形法并不是一个最坏时间复杂度在多项式级别的算法，在历史上椭球法第一次证明了线性规划的存在时间复杂度为多项式级别，单纯形法只是在&lt;strong>平均时间复杂度&lt;/strong>上具有优势。&lt;/p>
&lt;/blockquote>
&lt;p>有几类问题可以转化为LP问题。&lt;/p>
&lt;h6 id="多面体的-chebyshev-中心">多面体的 Chebyshev 中心&lt;/h6>
&lt;p>给定一个多面体&lt;/p>
&lt;p>$$
\mathcal{P}=\left\{x \in \mathbf{R}^{n} | a_{i}^{T} x \leq b_{i}, i=1, \dots, m\right\}
$$&lt;/p>
&lt;p>我们想知道这个多面体能包含的最大球的半径和球心。这个球心我们叫做该多面体的 chebyshev 中心。&lt;/p>
&lt;p>我们假设这个球是 $\mathcal{B}=\{x_c+u|\; \lVert u\rVert_2 \leq r\}$，如果这个球在半平面 $a_i^T x\leq b_i$ 内，那么一定有：&lt;/p>
&lt;p>$$
\sup_{\lVert u\rVert_2\leq r} a_i^T (x_c+u)=a_i^T x_c + r\lVert a_i\rVert_2\leq b_i
$$&lt;/p>
&lt;p>最后我们得到相应的LP问题：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\text { maximize } &amp;amp; r \\
\text { subject to } &amp;amp; a_{i}^{T} x_{c}+r\left\|a_{i}\right\|_{2} \leq b_{i}, \quad i=1, \ldots, m
\end{array}
$$&lt;/p>
&lt;p>$\vec{x_c}， r$ 是LP问题的变量。&lt;/p>
&lt;h5 id="linear-fractional-programming">Linear-fractional programming&lt;/h5>
&lt;p>如果线性规划的目标函数不是线性函数，而是一个线性分式函数，这个问题就成了线性分式规划。它也可以转化成线性规划。&lt;/p>
&lt;p>$$
f_{0}(x)=\frac{c^{T} x+d}{e^{T} x+f}, \quad \operatorname{dom} f_{0}=\left\{x | e^{T} x+f&amp;gt;0\right\}
$$&lt;/p>
&lt;p>先做一个换元：&lt;/p>
&lt;p>$$
y=\frac{x}{e^{T} x+f}, \quad z=\frac{1}{e^{T} x+f}&amp;gt;0,\quad x=y/z
$$&lt;/p>
&lt;p>将上式代入约束条件，就顺利转化成线性规划了。&lt;/p>
&lt;h4 id="二次规划qp和-二次约束二次规划qcqp">二次规划（QP）和 二次约束二次规划（QCQP）&lt;/h4>
&lt;p>当LP中的目标函数是一个二次函数的时候，这个问题就成了&lt;strong>二次规划（quadratic program）&lt;/strong>。注意这个时候，约束条件仍然要求是线性的。
如果不等式约束条件中的函数再变成二次函数，那么这就是&lt;strong>二次约束二次规划（quadratically constrained quadratic program ）&lt;/strong>。
它们分别具有标准形式：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; (1 / 2) x^{T} P x+q^{T} x+r \\
\text {subject to } &amp;amp; G x \preceq h \\
&amp;amp; A x=b
\end{array}
$$&lt;/p>
&lt;p>和：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; (1 / 2) x^{T} P_{0} x+q_{0}^{T} x+r_{0} \\
\text {subject to } &amp;amp; (1 / 2) x^{T} P_{i} x+q_{i}^{T} x+r_{i} \leq 0, \quad i=1, \ldots, m \\
&amp;amp; A x=b
\end{array}
$$&lt;/p>
&lt;p>需要注意，这样的二次规划问题，都需要矩阵$P\in S_+^n$至少是半正定的。&lt;/p>
&lt;p>有一些问题可以利用QP进行求解：&lt;/p>
&lt;h6 id="最小二乘法系数的确定">最小二乘法系数的确定&lt;/h6>
&lt;p>$$
\min_{x} \;\|A x-b\|_{2}^{2}=x^{T} A^{T} A x-2 b^{T} A x+b^{T} b
$$&lt;/p>
&lt;p>$A^TA$ 一定是半正定的。这是一个无约束的QP问题。&lt;/p>
&lt;h5 id="两个多面体之间的距离">两个多面体之间的距离&lt;/h5>
&lt;p>两个多面体 $\mathcal{P}_{1}=\left\{x | A_{1} x \preceq b_{1}\right\}$ 和$\mathcal{P}_{2}=\left\{x | A_{1} x \preceq b_{2}\right\}$，想要找到它们之间的最小距离&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; \left\|x_{1}-x_{2}\right\|_{2}^{2} \\
\text {subject to } &amp;amp; A_{1} x_{1} \preceq b_{1}, \quad A_{2} x_{2} \preceq b_{2},
\end{array}
$$
这也是一个QP问题。&lt;/p>
&lt;h5 id="markowitz-投资组合">Markowitz 投资组合&lt;/h5>
&lt;p>这也是一个经典的QP问题，在此从略。&lt;/p>
&lt;h4 id="二阶锥规划socp">二阶锥规划（SOCP）&lt;/h4>
&lt;p>&lt;strong>二阶锥规划（second-order cone program）&lt;/strong> 具有典型形式：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f^{T} x \\
\text {subject to } &amp;amp; \left\|A_{i} x+b_{i}\right\|_{2} \leq c_{i}^{T} x+d_{i}, \quad i=1, \ldots, m \\
&amp;amp; F x=g
\end{array}
$$&lt;/p>
&lt;p>乍一看，不等式约束两边同时平方一下，就能变成QCQP了。确实如此，SOCP可以看做是QCQP的推广。&lt;/p>
&lt;p>椭球不确定集上的鲁棒线性优化，和高斯分布的线性机会约束，最后都转化成了一个SOCP。&lt;/p>
&lt;h5 id="几何规划gp">几何规划（GP）&lt;/h5>
&lt;p>&lt;strong>几何规划（geometric program）&lt;strong>是一类&lt;/strong>可以转化成凸优化问题&lt;/strong>的非凸问题。在引入GP之前，我们还需要厘清一些概念。我们称$$f(x)=cx^{\alpha_1}x^{\alpha_2}\cdots x^{\alpha_n}, x\succeq 0, c&amp;gt;0$$是一个单项式（monomial），几个单项式的和，叫做正项式（posynomial）。&lt;/p>
&lt;blockquote>
&lt;p>尽管我的初中课本把若干个单项式的和叫做多项式，但是一般来说多项式特制未知数的指数是非负整数的情况，而且多项式的英文是 polynomial。&lt;/p>
&lt;/blockquote>
&lt;p>一个标准的GP问题具有如下形式：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f_{0}(x) \\
\text {subject to } &amp;amp; f_{i}(x) \leq 1, \quad i=1, \ldots, m \\
&amp;amp; h_{i}(x)=1, \quad i=1, \ldots, p
\end{array}
$$&lt;/p>
&lt;p>其中 $f_i, h_i$ 都定义在 $x\succeq 0$ 上。很显然，单项式并不一定是凸的，这并不是一个凸优化问题。作换元 $y_i=\log x_i, x_i=e^{y_i}$，对于新元 $y$，原问题具有如下形式：&lt;/p>
&lt;p>$$
\begin{aligned}
\operatorname{ minimize} &amp;amp; \sum_{k=1}^{K_{0}} e^{a_{0 k}^{T} y+b_{0 k}} \\
\text { subject to } &amp;amp; \sum_{k=1}^{K_{i}} e^{a_{i k} y+b_{i k}} \leq 1, \quad i=1, \ldots, m \\
&amp;amp; e^{g_{i}^{T} y+h_{i}}=1, \quad i=1, \ldots, p
\end{aligned}
$$&lt;/p>
&lt;p>如果GP的目标函数和不等式约束都是单项式的话，我们还可以通过换元将它变成LP。所以LP也可以看做是GP的一种特殊情况。&lt;/p>
&lt;h3 id="广义不等式下的凸优化问题">广义不等式下的凸优化问题&lt;/h3>
&lt;p>在前一章凸函数的末尾，我们通过广义不等式成功将凸函数推广到向量值函数。我们称：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; f_{0}(x) \\
\text {subject to } &amp;amp; f_{i}(x) \preceq_{K_{i}} 0, \quad i=1, \ldots, m \\
&amp;amp; A x=b
\end{array}
$$&lt;/p>
&lt;p>为广义不等式约束下的凸优化问题的标准形式。正如一般凸优化问题的要求，这里还要求 $f_i$ 在 $K_i$ 上是 $K-convex$ 的。&lt;/p>
&lt;p>数学的美，在于它能用精妙的理论，将许多看似没有关联的问题抽象地统一在一起。正如我们即将看到的，proper cone 的概念仿佛神来之笔，将整个优化理论的问题统一起来了。&lt;/p>
&lt;h4 id="conic-form-problems">Conic form problems&lt;/h4>
&lt;p>锥形式的优化问题是一种很 general 的情况。在数学里面，我们认为一般性的结论是要好于特殊性的结论的。锥规划就是把很多经典的优化问题的形式抽象出来的一种表示方法。&lt;/p>
&lt;p>锥规划一般都具有如下的典型形式：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; c^{T} x \\
\text {subject to } &amp;amp; F x+g \preceq_{K} 0 \\
&amp;amp; A x=b
\end{array}
$$&lt;/p>
&lt;p>线性规划显然是锥规划的一种特殊情况。&lt;/p>
&lt;h6 id="socp">SOCP&lt;/h6>
&lt;p>SOCP可以用锥规划的形式表示：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; c^{T} x \\
\text {subject to } &amp;amp; -\left(A_{i} x+b_{i}, c_{i}^{T} x+d_{i}\right) \preceq_{K_{i}} 0, \quad i=1, \ldots, m \\
&amp;amp; F x=g
\end{array}
$$&lt;/p>
&lt;p>其中 $K_i$ 是一个二阶锥：$K_{i}=\left\{(y, t) \in \mathbf{R}^{k_{i}+1} \mid\|y\|_{2} \leq t\right\}$。&lt;/p>
&lt;h6 id="半定规划semidefinite-programming">半定规划（semidefinite programming）&lt;/h6>
&lt;p>半定规划（SDP）是一类非常非常重要的凸优化问题！在 Conic form problems 的基础上，令$K=\mathrm{S}^n_+$为半正定矩阵锥，因为 $\operatorname{tr}(C X)=\sum_{i, j=1}^{n} C_{i j} X_{i j}$，可以将 $\operatorname{tr}(C X)$ 看做 $X\in \mathrm{S}^n$ 的线性函数。继而，我们有&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; \operatorname{tr}(C X) \\
\text {subject to } &amp;amp; \operatorname{tr}\left(A_{i} X\right)=b_{i}, \quad i=1, \ldots, p \\
&amp;amp; X \succeq 0
\end{array}
$$&lt;/p>
&lt;p>这称为SDP的&lt;strong>标准形式&lt;/strong>。SDP也有如下的形式：&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize} &amp;amp; c^{T} x \\
\text {subject to } &amp;amp; x_{1} F_{1}+\cdots+x_{n} F_{n}+G \preceq 0 \\
&amp;amp; A x=b
\end{array}
$$&lt;/p>
&lt;p>关于矩阵的线性不等式我们叫做 &lt;strong>linear matrix inequality&lt;/strong>，在很多文献上简写为 &lt;strong>LMI&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>二阶锥规划和半定规划都属于锥线性规划，其标准形式和典型形式的联系见：&lt;a href="https://www.jianshu.com/p/172a08ca86dd">Conic Linear Programming&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>广义不等式不仅可以作用在约束条件上，还能作用在目标函数上。令 $f_0: \mathrm{R}^n \to \mathrm{R}^p$ 是一个多元向量值函数，在 $\mathrm{R}^n$的一个凸集上$C$，我们希望找到在一个广义不等式下的 $C$ 的最小值/极小值。&lt;/p>
&lt;p>$$
\begin{array}{ll}
\operatorname{minimize}(\text { with respect to } K) &amp;amp; f_{0}(x) \\
\text {subject to } &amp;amp; f_{i}(x) \leq 0, \quad i=1, \ldots, m \\
&amp;amp; h_{i}(x)=0, \quad i=1, \ldots, p
\end{array}
$$&lt;/p>
&lt;p>这样的问题叫做向量优化问题。这样子的目的就在于，如果目标是多维的，可以通过定义一个 proper cone $K$，来表达 $f_0(x) \preceq f_0(y)$，$x$ 至少不比 $y$ 差。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/" term="凸优化" label="凸优化"/></entry><entry><title type="text">Data-Driven Inventory Control with Shifting Demand</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/2/"/><id>https://allenz-me.github.io/posts/papers/2/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-03T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Production and Operations Management, 2021. DOI: https://doi.org/10.1111/poms.13326 Key words: inventory control; shifting demand; nonparametric learning; censored demand; asymptotic optimality 文章研究数据驱动下需求分布会发生变化的库……</summary><content type="html">&lt;p>发表在 Production and Operations Management, 2021. DOI: &lt;a href="https://doi.org/10.1111/poms.13326">https://doi.org/10.1111/poms.13326&lt;/a>&lt;/p>
&lt;p>Key words: inventory control; shifting demand; nonparametric learning; censored demand; asymptotic optimality&lt;/p>
&lt;hr>
&lt;p>文章研究数据驱动下&lt;strong>需求分布会发生变化&lt;/strong>的库存管理。&lt;/p>
&lt;blockquote>
&lt;p>Over a planning horizon of T periods, demand distributions can change up to O( log T) times, but the ﬁrm does not know the demand distributions before or after each change, the time periods when changes occur, or the number of changes.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>lead-time 为0&lt;/strong>，需求分布是离散、轻尾的，期望存在，方差有限，整体设定与通俗的库存管理相同。需求可能会变化 $V$ 次，目标是最小化：&lt;/p>
&lt;p>$$
\sum_{v=0}^{V} \sum_{t=l^{v}}^{l^{v+1}-1}\left(h\, \mathbb{E}\left[\left(y_{t}-D_{t}^{v}\right)^{+}\right]+b\left[\left(D_{t}^{v}-y_{t}\right)^{+}\right]\right)
$$&lt;/p>
&lt;p>其中 $y_t$ 是当期的库存。不知道 lost sales，需求未满足时是不知道准确的需求的。&lt;/p>
&lt;p>用 regret 衡量 policy 的表现，研究 Regret(admissible policy) - Regret(best policy)。&lt;/p>
&lt;blockquote>
&lt;p>As it can be seen, regret measures the cost increase due to lack of demand information, and a smaller value of regret means a more cost-efﬁcient policy .&lt;/p>
&lt;/blockquote>
&lt;p>best policy 是假定需求分布和需求变化都已知的时候做出的最优 policy。&lt;/p>
&lt;p>这里 regret 的定义与 MAB 里的 external regret 非常接近。&lt;/p>
&lt;p>文章首先证明了一个 lower bound:&lt;/p>
&lt;p>$$
R^{\pi}(\Lambda, T) \geq \frac{1}{40 e} \sqrt{T}
$$&lt;/p>
&lt;p>思路是构造 instance 并利用 KL 散度。跟 MAB 的 lower bound 证明有一点点类似？&lt;/p>
&lt;p>接着，文章给出了一个 Learning Algorithm for Inventory Control with Shifting Demand (LAIS) 算法并证明其 upper bound 是：&lt;/p>
&lt;p>$$
R^{L A I S}(\Lambda, T) \leq 2 \max \{b, h\} \sqrt{T}(\log T)(\log \log T)^{4}
$$&lt;/p>
&lt;p>这个算法把时间 $T$ 划分成 $O(\sqrt{T})$ 个 batch，每个 batch 首先花 $L=\left\lceil\log I(\log \log I)^{3}\right\rceil$ 时间做 exploration，剩下的时间做 exploitation。exploration 阶段把订货量设定的稍高，学习需求分布，接着再拿需求的经验分布当做真实分布的估计，做出一个样本上最优的 policy。&lt;/p>
&lt;p>文章还提到这个算法不适用于需求分布是连续的情况。&lt;/p>
&lt;!-- 这就像 bandits 问题，如果 arm 的 reward distribution 会变化。 --></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/pom/" term="POM" label="POM"/><category scheme="https://allenz-me.github.io/tags/inventory/" term="Inventory" label="Inventory"/></entry><entry><title type="text">A practical inventory control policy using operational statistics</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/papers/1/"/><id>https://allenz-me.github.io/posts/papers/1/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-02T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">发表在 Operations Research Letters，2005. DOI: https://doi.org/10.1016/j.orl.2004.08.003 文章较简短，但是对于 newsvendor 问题给出了一个较为深刻……</summary><content type="html">&lt;p>发表在 Operations Research Letters，2005. DOI: &lt;a href="https://doi.org/10.1016/j.orl.2004.08.003">https://doi.org/10.1016/j.orl.2004.08.003&lt;/a>&lt;/p>
&lt;p>文章较简短，但是对于 newsvendor 问题给出了一个较为深刻的结论：&lt;/p>
&lt;blockquote>
&lt;p>The traditional approach of separating the parameter estimation and the maximization of the expected proﬁt leads to a suboptimal inventory policy.&lt;/p>
&lt;/blockquote>
&lt;p>需求服从一个确定但是未知的分布，其它设定与经典的报童模型一致。&lt;/p>
&lt;p>经典的做法可能是积累需求的样本，然后用参数式/非参数统计拟合出一个分布，再代入到报童模型求解 $q^\ast = \bar{F}^{-1}(c/p).$&lt;/p>
&lt;p>但是，这样做可能导致一个严格非最优的订货量。&lt;/p>
&lt;p>文章以指数分布为例：&lt;/p>
&lt;p>假定需求服从均值 $\theta$ 的指数分布，则 $x^\ast(\theta) = \theta \ln (p/c)$.&lt;/p>
&lt;p>$\theta$ 的无偏估计由需求样本的均值给出：$\bar{D}=\displaystyle\frac{1}{n} \sum_{k=1}^{n} D_{k}$.&lt;/p>
&lt;p>所以经典统计方法给出的订货量是：&lt;/p>
&lt;p>$$
\hat{X}_{\mathrm{sm}}=\bar{D} \ln \left(p/c\right) \tag{sample mean}
$$&lt;/p>
&lt;p>然而，如果把样本均值的线性函数 $\hat{X}(z)=z \bar{D}$ 代入到利润函数里面，对 $z$ 进行优化：&lt;/p>
&lt;p>$$
\begin{aligned}
\eta(z)
&amp;amp;=E\left[p \theta\left(1-\exp \left\{-\frac{z \bar{D}}{\theta}\right\}\right)-c z \bar{D}\right] \\
&amp;amp;=p \theta\left(1-\left(\frac{n}{n+z}\right)^{n}\right)-c z \theta, \quad z \geqslant 0 .
\end{aligned}
$$&lt;/p>
&lt;p>可以得到：&lt;/p>
&lt;p>$$
\hat{X}_{\mathrm{os}: \mathrm{sm}}=\hat{X}\left(z^{\ast}\right)=n\left(\left(\frac{p}{c}\right)^{1 /(n+1)}-1\right) \bar{D} \tag{operational statistics}
$$&lt;/p>
&lt;blockquote>
&lt;p>当 $p \gg c$ 时，$\hat{X}_{\mathrm{os}: \mathrm{sm}} &amp;gt; \hat{X}_{\mathrm{sm}}$&lt;/p>
&lt;p>在我看来，估计需求分布的过程中没有考虑进多订、少订带来的不一样的影响，可能是 suboptimality 的原因。&lt;/p>
&lt;/blockquote>
&lt;p>直接把订货量当成需求样本的函数来进行计算的这种方法，文章称作是 operational statistics.&lt;/p>
&lt;p>可以严格证明 $\hat{X}_{\text{os:sm}}$ 产生的利润严格大于 $\hat{X}_{\text{sm}}$ ！&lt;/p>
&lt;p>类似的影响出现于具有尺度参数（scale parameter）的分布族中。&lt;/p>
&lt;blockquote>
&lt;p>注：文章中售价使用的是字母 $s$，而本文使用的是 $p$。&lt;/p>
&lt;/blockquote>
&lt;!--
一个思路：对于正态分布的需求样本
1. 可以拟合一个神经网络（100 -> 2）来计算正态分布的均值和方差，由此得到一个最优订货量
2. 以经验分布为目标分布，容易计算出一个最优订货量，由此训练一个神经网络（100->1）。
比较两者的优劣！
--></content><category scheme="https://allenz-me.github.io/categories/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB/" term="论文简读" label="论文简读"/><category scheme="https://allenz-me.github.io/tags/newsvendor/" term="Newsvendor" label="Newsvendor"/></entry><entry><title type="text">Convex Function</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cvxopt/convex-function/"/><id>https://allenz-me.github.io/posts/cvxopt/convex-function/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-02T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">说完了凸集，下一个要介绍的肯定就是凸函数啦~ 凸函数的相关性质在优化中的地位不言而喻~……</summary><content type="html">&lt;!-- #! https://zhuanlan.zhihu.com/p/452821674 -->
&lt;p>说完了凸集，下一个要介绍的肯定就是凸函数啦~&lt;/p>
&lt;p>凸函数的相关性质在优化中的地位不言而喻~！&lt;/p>
&lt;h2 id="凸函数">凸函数&lt;/h2>
&lt;p>$f: \mathrm{R}^n \to \mathrm{R}$ 是凸函数，如果 $f$ 的定义域是凸集，并且 $\forall x, y, \theta \in [0, 1]$ 成立：&lt;/p>
&lt;p>$$
f(\theta x+(1-\theta )y) \leq \theta f(x) + (1-\theta) f(y)\\
$$&lt;/p>
&lt;p>如果 $\theta \in (0, 1), x\neq y$ 时上面的不等号严格成立，那么就说这个函数是&lt;strong>严格凸&lt;/strong>（strict convex）的。&lt;/p>
&lt;p>几何上看，凸函数要求 $(x, f(x))$ 和 $(y, f(y))$ 这条线段位于函数图形的上方。&lt;/p>
&lt;p>对应的，我们还有定义“凹函数”（concave function），当 $-f$ 是凸函数时，$f$ 被称为凹函数。&lt;/p>
&lt;p>对于仿射函数，它是既凸又凹的。同时，&lt;strong>既凸又凹的函数只有仿射函数&lt;/strong>。&lt;/p>
&lt;p>如果 $f$ 是凸函数，那么 $g(t)=f(x+tv)$ 也是凸函数，反过来的结论也成立。&lt;strong>这说明，凸函数限制在任何一条直线上都是凸的！&lt;/strong> 凸函数的概念完全可以从欧式空间推广到一般的线性空间，&lt;strong>在一般的线性空间上，这条性质成为我们判断凸函数的重要依据。&lt;/strong>&lt;/p>
&lt;p>凸函数还具有良好的分析性质，比如，凸函数在它定义域的相对内点集上是连续的；凸函数的不连续点只可能出现在它的相对边界上。&lt;/p>
&lt;h3 id="凸函数定义域延拓">凸函数定义域延拓&lt;/h3>
&lt;p>有时候我们会把一个凸函数的定义域延拓到整个 $R^n$ 空间中：&lt;/p>
&lt;p>$$
\tilde{f}(x)=\left\{\begin{array}{ll}f(x) &amp;amp; x \in \operatorname{dom} f \\ \infty &amp;amp; x \notin \operatorname{dom} f\end{array}\right.\\
$$&lt;/p>
&lt;p>可以证明，这样延拓的凸函数也满足凸函数的定义。（在定义好关于$\infty$的运算后）。这样的定义在函数表示上有一定的意义。&lt;/p>
&lt;h3 id="indicator-function">Indicator function&lt;/h3>
&lt;p>设 $C$ 是一个凸集，令函数 $I_C(x) = 0, \forall x \in C$，即在集合 $C$ 上的取值为0，此时：
$$
\tilde{I}_{C}(x)= \begin{cases}0 &amp;amp; x \in C \\ \infty &amp;amp; x \notin C\end{cases}\\
$$
$\tilde{I}_C$ 叫做集合 $C$ 的 indicator function。&lt;/p>
&lt;blockquote>
&lt;p>Indicator function 的共轭函数是 support function：
$$
S_{C}(y) = \sup \,\{ y^T x \mid x \in C\}\\
$$&lt;/p>
&lt;/blockquote>
&lt;h3 id="一阶条件first-order-condition">一阶条件（first order condition）&lt;/h3>
&lt;p>可微的凸函数满足一阶条件：&lt;/p>
&lt;p>$$
f(y) \geq f(x)+\nabla f(x)^{T}(y-x)\quad\forall x,y \in \mathrm{dom}f\\
$$&lt;/p>
&lt;p>这个不等式揭示了凸函数的局部特性，那就是在一点的切平面是整个函数的 &lt;strong>global underestimate&lt;/strong> 。&lt;/p>
&lt;p>如果上面的不等号严格成立，那么这个函数是严格凸的。这里的条件是&lt;strong>充分必要&lt;/strong>的。&lt;/p>
&lt;h3 id="二阶条件second-order-condition">二阶条件（second order condition）&lt;/h3>
&lt;p>如果定义在&lt;strong>开凸集&lt;/strong>上的二阶可微函数 $f$ 满足 $\nabla^2f\succeq0$，那么 $f$ 是凸函数。&lt;/p>
&lt;p>如果 $\nabla^2f\succ0$，那么 $f$ 是严格凸的。&lt;/p>
&lt;blockquote>
&lt;p>当 $f$ 严格凸时，不一定能推出 $\nabla^2f$ 正定。比如 $f(x)=x^4$，二阶导数在 $x=0$ 处为 $0$。&lt;/p>
&lt;/blockquote>
&lt;p>关于一阶条件和二阶条件的证明，要用到泰勒展开。在此从略。&lt;/p>
&lt;p>例：&lt;/p>
&lt;p>$$
f(x) = (1/2) x^T P x + q^T x + r\\
$$&lt;/p>
&lt;p>因为 $\nabla^2 f = P$，所以，当 $P \succeq 0$ 正定时，$f$ 是凸函数；当 $P \preceq 0$ 负定时，$f$ 是凹函数。&lt;/p>
&lt;h3 id="凸函数的例子">凸函数的例子&lt;/h3>
&lt;ul>
&lt;li>指数函数、多项式函数，绝对值函数都是凸的；对数函数是凹的。&lt;/li>
&lt;li>最大值函数 $f(x)=\max _{i} x_{i}$ 是凸的。&lt;/li>
&lt;/ul>
&lt;p>$$
\begin{aligned}f(\theta x+(1-\theta) y) &amp;amp;=\max _{i}\left(\theta x_{i}+(1-\theta) y_{i}\right) \\&amp;amp; \leq \theta \max _{i} x_{i}+(1-\theta) \max _{i} y_{i} \\&amp;amp;=\theta f(x)+(1-\theta) f(y) , \quad \theta \in [0,1]\end{aligned}\\
$$&lt;/p>
&lt;blockquote>
&lt;p>$f(x)=\log \left(e^{x_{1}}+\cdots+e^{x_{n}}\right)$ 在 $\mathbf{R}^{n}$ 上是凸的。有估计式：
$$
\max \left\{x_{1}, \ldots, x_{n}\right\} \leq f(x) \leq \max \left\{x_{1}, \ldots, x_{n}\right\}+\log n \\
$$
这个函数是最大值函数的一个光滑近似。&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>$f(x,y) = \displaystyle\frac{x^2}{y} (\operatorname{dom} f=\mathbf{R} \times \mathbf{R}_{++}=\left\{(x, y) \in \mathbf{R}^{2} \mid y&amp;gt;0\right\})$ 是一个 &lt;em>Quadratic-over-linear function&lt;/em>， 它是凸的。&lt;/li>
&lt;li>$f(x)=(\Pi_{i=1}^n x_i)^{\frac{1}{n}},x_i&amp;gt;0$ 是凹函数，容易证明 $\nabla^2f$ 是半负定的。&lt;/li>
&lt;li>$f(X)=\log \det X$ 是 $\mathrm{S}_{++}^n$ 上的凹函数。它是一个定义在矩阵空间上的函数。它的证明要用到线性空间中凸性的性质，即凸函数限制在任意一条直线都是凸的。&lt;/li>
&lt;/ul>
&lt;h3 id="下水平集sublevel-sets">下水平集（sublevel sets）&lt;/h3>
&lt;p>定义 $f: \mathbf{R}^n \to \mathbf{R}$ 的 $\alpha-\text{sublevel\; set}$ 为：&lt;/p>
&lt;p>$$
C_{\alpha}=\{x \in \operatorname{dom} f \mid f(x) \leq \alpha\}\\
$$&lt;/p>
&lt;p>易证 $f$ 是凸函数的时候 $C_\alpha$ 是个凸集。（该性质说明凸函数是拟凸的）&lt;/p>
&lt;p>从而这里给出了判断凸集的另一个方法：&lt;strong>能被写成某个凸函数的 &lt;em>$\alpha$-sublevel set&lt;/em> 的集合是凸集&lt;/strong>。反之，一个函数的 sublevel set 是凸的，并不能反推出它是凸函数（事实上这个函数是拟凸的）。&lt;/p>
&lt;blockquote>
&lt;p>例：$S=\{x \mid x^T Ax+c^T x+ b \leq 0, A\succeq 0\}$ 是凸集。&lt;/p>
&lt;/blockquote>
&lt;p>对于 $f$ 是凹函数有相应的结论：$\{x \in \operatorname{dom} f \mid f(x) \geq \alpha\}$ 是凸集。&lt;/p>
&lt;h3 id="epigraph">Epigraph&lt;/h3>
&lt;p>一个函数的 $f:\mathbf{R}^n\to \mathbf{R}$ 的 &lt;em>epigraph&lt;/em> 是指：&lt;/p>
&lt;p>$$
\text { epi } f=\{(x, t) \mid x \in \operatorname{dom} f, f(x) \leq t\}\\
$$&lt;/p>
&lt;p>$\text{epi} f$ 是 $\mathbf{R} ^{n+1}$ 的子集，是函数图形的上方。&lt;/p>
&lt;p>&lt;strong>$\text{epi} f$ 是凸集当且仅当 $f$ 是凸函数。&lt;/strong> 所以 epigraph 也是一种主要的判断凸函数的方法。这个证明非常容易，只需要反复使用定义即可。&lt;/p>
&lt;p>对应于凹函数我们定义 &lt;em>hypograph&lt;/em>：&lt;/p>
&lt;p>$$
\text { hypo } f=\{(x, t) \mid t \leq f(x)\}\\
$$&lt;/p>
&lt;p>&lt;strong>$\text{hypo} \;f$ 是凸集当且仅当 $f$ 是凹函数。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>例：$f(X)=\lambda_{\max} (X), X \in \mathrm{S}_n$ 的 epigraph $\{(X, t)\mid tI - X \succeq 0\}$ 是凸集（由定义证），从而 $f$ 是凸函数。类似可得 $f(X)=\lambda_{\min} (X), X \in \mathrm{S}_n$ 是凹函数。&lt;/p>
&lt;/blockquote>
&lt;h3 id="琴生不等式jesen-inequality">琴生不等式（Jesen inequality）&lt;/h3>
&lt;p>琴生不等式是凸函数的重要性质。&lt;/p>
&lt;p>对 $x_{1}, \ldots, x_{k} \in \operatorname{dom} f$ 和 $\theta_{1}, \ldots, \theta_{k} \geq 0, \theta_{1}+\cdots+\theta_{k}=1$ 成立：&lt;/p>
&lt;p>$$
f\left(\theta_{1} x_{1}+\cdots+\theta_{k} x_{k}\right) \leq \theta_{1} f\left(x_{1}\right)+\cdots+\theta_{k} f\left(x_{k}\right)\\
$$&lt;/p>
&lt;p>这是有限个点的情况。该不等式还能扩展到无限和、积分等情况。&lt;/p>
&lt;blockquote>
&lt;p>If $p(x) \geq 0$ on $S \subseteq \operatorname{dom} f, \int_{S} p(x) \mathrm{d} x=1$, then:
$$
f \left( \int _ { S } p ( x ) x \mathrm{d} x \right) \leq \int _ { &amp;gt; S } f ( x ) p ( x ) \mathrm{d} x
$$&lt;/p>
&lt;/blockquote>
&lt;p>对某些凸函数应用琴生不等式可以得到许多著名的不等式，比如 Holder 不等式：&lt;/p>
&lt;p>$$
\sum_{i=1}^{n} x_{i} y_{i} \leq\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^{1 / p}\left(\sum_{i=1}^{n}\left|y_{i}\right|^{q}\right)^{1 / q}\\
$$&lt;/p>
&lt;h2 id="保持函数凸性的操作">保持函数凸性的操作&lt;/h2>
&lt;h3 id="若干凸函数非负的加权和">若干凸函数非负的加权和&lt;/h3>
&lt;p>$f=w_1f_1 + w_2 f_2 + \cdots + w_n f_n$， $f_i$ 凸，$w_i &amp;gt; 0 \Longrightarrow f$ 凸。&lt;/p>
&lt;p>这意味着所有的凸函数形成了一个凸锥！&lt;/p>
&lt;p>可以推广到无穷和的情况，如果 $f(x,y)$ 对 $x$ 是凸的，并且 $w(y)&amp;gt;0$，那么 $g(x)=\displaystyle\int f(x,y)w(y) \mathrm{d}y$ 是凸的。&lt;/p>
&lt;h3 id="与仿射函数的复合">与仿射函数的复合&lt;/h3>
&lt;p>如果 $A$ 是一个矩阵，那么 $g(x)=f(A x+b)$ 与 $f$ 有相同的凹凸性。&lt;/p>
&lt;p>如果加上可微的条件，那么根据复合函数的求导法则，就有：
$$
\nabla g = A^T \nabla f, \quad \nabla^2 g = A^T \nabla^2 f A\\
$$
所以显然 $g$ 与 $f$ 会有相同的凹凸性。&lt;/p>
&lt;p>例：$e^{ax+by+c}$ 是凸函数。&lt;/p>
&lt;h3 id="逐点取最大值上确界">逐点取最大值（上确界）&lt;/h3>
&lt;p>$f(x)=\max \left\{f_{1}(x), \ldots, f_{m}(x)\right\}$，$f_i$ 凸，则 $f$ 凸。对于凹函数，则是取最小值（下确界）。&lt;/p>
&lt;p>在 &lt;em>infinite&lt;/em> 的情况下，如果 $f(x,y)$ 对 $x$ 是凸的，那么 $g(x) = \displaystyle\sup_y f(x,y)$ 也是凸的。&lt;/p>
&lt;blockquote>
&lt;p>证明要用到 epigraph，因为 $\operatorname{epi} g=\displaystyle\bigcap_{y \in \mathcal{A}} \operatorname{epi} f(\cdot, y)$，且凸集的交仍然是凸的，所以 $\operatorname{epi} g$ 也是凸的。&lt;/p>
&lt;/blockquote>
&lt;p>例：&lt;/p>
&lt;p>支撑函数 $S_C(x) = \sup \{x^T y \mid y \in C\}$，是一族线性函数的上确界，所以不管 $C$ 是不是凸集，$S_C$ 都是凸函数。&lt;/p>
&lt;p>$f(X) = \lambda_{\max} (X)$ 可以表示为 $\sup \,\{y^TX y \mid \|y \|_2 = 1\}$，是一列线性函数的上确界，所以 $\lambda_{\max}$ 是凸函数。&lt;/p>
&lt;p>事实上，绝大多数的凸函数，都能够表示成一族仿射函数的上确界函数，这种方法也是判断凸函数最常用的方法。这与凸集可以表示成一族半平面的交，是一样的。（凸函数可以通过 epigraph 转换成凸集。）&lt;/p>
&lt;blockquote>
&lt;p>In other words, a convex function is the pointwise supremum of the set of all aﬃne global underestimators of it.&lt;/p>
&lt;/blockquote>
&lt;h3 id="函数复合">函数复合&lt;/h3>
&lt;p>考虑一般函数复合的情况：$f(x) = h(g(x))$&lt;/p>
&lt;p>在一元的情况下，求导可得：&lt;/p>
&lt;p>$$
f^{\prime \prime}(x)=h^{\prime \prime}(g(x)) g^{\prime}(x)^{2}+h^{\prime}(g(x)) g^{\prime \prime}(x)\\
$$&lt;/p>
&lt;p>能够得到如下的判断法则：&lt;/p>
&lt;ul>
&lt;li>$f$ is convex if $h$ is convex and nondecreasing, and $g$ is convex.&lt;/li>
&lt;li>$f$ is convex if $h$ is convex and nonincreasing, and $g$ is concave.&lt;/li>
&lt;li>$f$ is concave if $h$ is concave and nondecreasing, and $g$ is concave.&lt;/li>
&lt;li>$f$ is concave if $h$ is concave and nonincreasing, and $g$ is convex.&lt;/li>
&lt;/ul>
&lt;p>简单来说，&lt;strong>凸函数批上一件单增且凸的外衣仍然是凸的&lt;/strong>。当 $g$ 是多元函数的时候，上述法则仍然成立，只不过需要把 $h$ 换成延拓定义域之后的 $\tilde{h}$&lt;/p>
&lt;p>这个结论可以直接用定义证明。&lt;/p>
&lt;p>$g$ 是凸函数：$g(\theta x+(1-\theta) y) \leq \theta g(x)+(1-\theta) g(y)$&lt;/p>
&lt;p>$h$ 单调递增：$h(g(\theta x+(1-\theta) y)) \leq h(\theta g(x)+(1-\theta) g(y))$&lt;/p>
&lt;p>$h$ 是凸函数：$h(\theta g(x)+(1-\theta) g(y)) \leq \theta h(g(x))+(1-\theta) h(g(y))$&lt;/p>
&lt;p>结合起来有：$h(g(\theta x+(1-\theta) y)) \leq \theta h(g(x))+(1-\theta) h(g(y))$&lt;/p>
&lt;h3 id="特殊情况下的下确界函数">特殊情况下的下确界函数&lt;/h3>
&lt;p>如果 $f(x,y)$ 对 $(x,y)$ 是凸的，$C$ 是一个非空凸集，那么 $g(x)=\displaystyle\inf_{y \in C} \;f(x, y)$ 是凸的。&lt;/p>
&lt;p>$$
\begin{aligned}g\left(\theta x_{1}+(1-\theta) x_{2}\right) &amp;amp;=\inf _{y \in C} f\left(\theta x_{1}+(1-\theta) x_{2}, y\right) \\&amp;amp; \leq f\left(\theta x_{1}+(1-\theta) x_{2}, \theta y_{1}+(1-\theta) y_{2}\right) \\&amp;amp; \leq \theta f\left(x_{1}, y_{1}\right)+(1-\theta) f\left(x_{2}, y_{2}\right) \\&amp;amp; \leq \theta g\left(x_{1}\right)+(1-\theta) g\left(x_{2}\right)+\epsilon\end{aligned}\\
$$&lt;/p>
&lt;p>另外，也可以通过 $\mathrm{epi }\;g$ 来证明凸性：
$$
\text { epi } g=\{(x, t) \mid(x, y, t) \in \text { epi } f \text { for some } y \in C\}\\
$$
是某个凸集的投影。&lt;/p>
&lt;p>例：&lt;/p>
&lt;p>点 $x$ 到集合$S$ 的距离：
$$
\operatorname{dist}(x, S)=\inf _{y \in S}\|x-y\|\\
$$
当 $S$ 是凸集时，$\operatorname{dist}(x, S)$ 是凸函数。&lt;/p>
&lt;h3 id="函数的透视">函数的透视&lt;/h3>
&lt;p>透视操作是保持凸（凹）性的。&lt;/p>
&lt;p>如果 $f: \mathrm{R}^n \to \mathrm{R}$， $f$ 的透视（perspective）被定义为 $g : \mathrm{R}^{n+1} \to \mathrm{R}$：
$$
g(x, t) = t f(x/t)\\
$$
$g$ 的定义域为 $\operatorname{dom} g=\{(x, t) \mid x / t \in \operatorname{dom} f, t&amp;gt;0\}$&lt;/p>
&lt;p>$g$ 的凸性可以轻松由 epigraph 和凸集的透视仍然是凸的 来得到：
$$
\begin{aligned}(x, t, s) \in \mathbf{e p i} g &amp;amp; \Longleftrightarrow \quad t f(x / t) \leq s \\&amp;amp; \Longleftrightarrow \quad f(x / t) \leq s / t \\&amp;amp; \Longleftrightarrow \quad(x / t, s / t) \in \mathbf{e p i} f\end{aligned}\\
$$
例：&lt;/p>
&lt;p>$f = -\log x$ 是 $\mathrm{R}_{++}$ 上的凸函数，则它的透视：
$$
g(x, t)=-t \log (x / t)=t \log (t / x)=t \log t-t \log x\\
$$
也是凸的。&lt;/p>
&lt;h2 id="共轭函数conjugate-function">共轭函数（Conjugate Function）&lt;/h2>
&lt;p>这是一个非常重要的概念，在凸分析里，共轭函数有至关重要的地位！&lt;/p>
&lt;p>定义函数$f(x)$的共轭函数为：&lt;/p>
&lt;p>$$
f^{\ast}(y)=\sup _{x \in \operatorname{dom} f}\left(y^{T} x-f(x)\right)\\
$$&lt;/p>
&lt;p>共轭函数是多个仿射函数的上确界，&lt;strong>因此是一定是凸函数&lt;/strong>。共轭函数的定义域是上确界值有限的 $y$ 的值。&lt;/p>
&lt;p>一些例子：&lt;/p>
&lt;ul>
&lt;li>$f(x)=ax+b$，注意到 $xy-ax-b$ 只在 $y=a$ 时有界，因此共轭函数只在 $y=a$ 处有定义，并且 $f^\ast(y)=-b$&lt;/li>
&lt;li>$f(x)=x^2$，$\displaystyle\sup_{x\in R} xy - x^2 = \frac{y^2}{4}=f^\ast(y)$&lt;/li>
&lt;li>$f(x)=\frac{1}{2}x^T Qx,(Q\in \mathrm{S}^n_{++})$ 的共轭函数是 $f^\ast(y)=\frac{1}{2}y^T Q^{-1}y$&lt;/li>
&lt;li>$f(x)=|x|$ 的共轭函数是 $f^{\star}(y)=\left\{\begin{array}{ll}0 &amp;amp; \text { if }|y| \leq 1 \\ \infty &amp;amp; \text { if }|y|&amp;gt;1\end{array}\right.$&lt;/li>
&lt;/ul>
&lt;p>共轭函数具有鲜明的几何意义：&lt;/p>
&lt;p>&lt;img src="../../figures/Convex-function/4308d11eac841b613a0a935ca6a0f26b.png" alt="">&lt;/p>
&lt;p>当 $f(x)$ 是一元函数的时候，如上图所示，$f^\ast(y)$ 表示以 $y$ 为斜率且过原点的直线，与$f(x)$ 的图像的最大距离（或者其负数）。&lt;/p>
&lt;p>当 $f(x)$ 是 $n$ 元函数的时候，$f^\ast(y)$ 表示以 $(-y, 1)$ 为法向量（n+1维）且过原点的平面，与 $f(x)$ 的图像的最大距离（或者其负数）。&lt;/p>
&lt;p>设$f(x)=\|x\|$代表$\mathbf{R}^n$中的一种范数，其对偶范数为$\|\cdot \|_\ast$，我们能得到共轭函数：
$$
f^{\ast}(y)=\left\{\begin{array}{ll}0 &amp;amp; \|y\|_\ast \leq 1 \\ \infty &amp;amp; \text { otherwise }\end{array}\right.\\
$$&lt;/p>
&lt;h3 id="fenchels-inequality">Fenchel’s inequality&lt;/h3>
&lt;p>根据共轭函数的定义，下式是显然的：&lt;/p>
&lt;p>$$
f(x)+f^\ast(y)\ge x^T y\\
$$&lt;/p>
&lt;p>应用到上面的例子，还能得到：&lt;/p>
&lt;p>$$
x^{T} y \leq(1 / 2) x^{T} Q x+(1 / 2) y^{T} Q^{-1} y\\
$$&lt;/p>
&lt;h3 id="共轭函数的共轭">共轭函数的共轭&lt;/h3>
&lt;p>如果 $f$ 是凸函数，并且$\mathbf{epi} f$是闭集，那么 $f^{\ast\ast}=f$。&lt;/p>
&lt;h3 id="可微分函数的共轭">可微分函数的共轭&lt;/h3>
&lt;p>如果 $f$ 是凸函数并且一阶可微，那么根据凸函数的极值理论，容易得到，使得 $y^T x-f(x)$ 最大的 $x^\ast$ 满足：&lt;/p>
&lt;p>$$
y=\nabla f(x^\ast)\\
$$&lt;/p>
&lt;p>从而我们有：&lt;/p>
&lt;p>$$
f^{\ast}(y)=x^{\ast T} \nabla f\left(x^{\ast}\right)-f\left(x^{\ast}\right),\quad(y=\nabla f(x^\ast))\\
$$&lt;/p>
&lt;p>欲求 $f^\ast(y)$，只需要解 $y=\nabla f(z)$ 得到向量 $z$&lt;/p>
&lt;blockquote>
&lt;p>可微函数$f$的共轭，也叫做$f$的 &lt;strong>Legendre变换&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;h3 id="其他性质">其他性质&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>对 $a &amp;gt; 0, b \in \mathbf{R}$，$g(x)=af(x)+b$ 的共轭函数是 $g^{\ast}(y) = af^{\ast}(y/a) - b$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果$f(u, v)=f_{1}(u)+f_{2}(v)$，且$f_1, f_2$都是凸函数，那么：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>$$
f^{\ast}(w, z)=f_{1}^{\ast}(w)+f_{2}^{\ast}(z)\\
$$&lt;/p>
&lt;h2 id="拟凸函数quasiconvex-functions">拟凸函数（Quasiconvex functions）&lt;/h2>
&lt;blockquote>
&lt;p>拟凸这节写的不是特别完善，挖坑以后填了。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>如果一个函数的下水平集是凸的，那么就称这个函数是拟凸的。&lt;/strong>&lt;/p>
&lt;p>$$
S_{\alpha}=\{x \in \operatorname{dom} f \mid f(x) \leq \alpha\} \;\text{ convex} \Longleftrightarrow f \;\text{ quasiconvex}\\
$$&lt;/p>
&lt;p>$f$ 拟凹（quasiconcave）当且仅当 $-f$ 是拟凸的。即，对于拟凹的函数 $f$ 来说，上水平集 $\{x \mid f(x) \geq \alpha\}$ 是凸的。&lt;/p>
&lt;p>既拟凸又拟凹的函数叫做拟线性函数（quasilinear），拟线性函数的所有水平集 $\{x \mid f(x) = \alpha\}$ 都是凸的。&lt;/p>
&lt;p>&lt;strong>凸函数一定是拟凸函数，但拟凸函数不一定是凸函数。&lt;/strong> 比如 $f(x)=\sqrt{|x|}$ 就不是凸函数，但是是拟凸的。&lt;/p>
&lt;p>很多凸函数具有的良好性质，可以推广到拟凸函数上。&lt;/p>
&lt;p>一个定义在凸集上的函数是拟凸函数，&lt;strong>当且仅当&lt;/strong> $\forall x, y \in \mathrm{dom} f, 0\le \theta \le 1$，成立：
$$
f(\theta x+(1-\theta) y) \leq \max \{f(x), f(y)\}\\
$$
这意味着，&lt;strong>线段上的函数值，一定小于等于两个端点函数值最大的那一个&lt;/strong>。这个既可以当做拟凸函数的性质，也能当做拟凸函数的定义。&lt;/p>
&lt;blockquote>
&lt;p>拟凸（quasiconvex）两种等价定义的证明：&lt;/p>
&lt;p>$$
\forall \alpha , L_{\alpha}=\{x| f(x)\le \alpha\} \text{ convex}\Leftrightarrow f(\lambda x + (1-\lambda)y)\le \max\{f(x), f(y)\}
$$&lt;/p>
&lt;ul>
&lt;li>如果 $f(\lambda x + (1-\lambda)y)\le \max\{f(x), f(y)\}$，那么 $\forall \alpha,$ 设 $x, y \in L_{\alpha}$，就有 $f(x)\le \alpha, f(y) \le \alpha$，从而&lt;/li>
&lt;/ul>
&lt;p>$$
f(\lambda x+(1-\lambda )y)\le \max\{f(x), f(y)\}\le\alpha\\
$$&lt;/p>
&lt;p>所以 $\lambda x+ (1-\lambda)y \in L_{\alpha}$，所以 $L_{\alpha}$ 是凸集。&lt;/p>
&lt;ul>
&lt;li>如果 $L_{\alpha}$ 是凸集，$\forall x, y$， 设 $\alpha=\max \{f(x), f(y)\}$，从而 $x, y \in L_{\alpha}$，那么 $\lambda x + (1-\lambda) y \in L_{\alpha}$ 并且&lt;/li>
&lt;/ul>
&lt;p>$$
f(\lambda x + (1-\lambda)y)\le\alpha= \max\{f(x), f(y)\}\\
$$&lt;/p>
&lt;/blockquote>
&lt;p>有时候，这个定义也叫做 Jensen's inequality for quasiconvex functions.&lt;/p>
&lt;p>针对这个性质还有另一个版本：
$$
f(x)\le f(y) \Rightarrow f(\theta x+(1-\theta)y)\le f(y)\\
$$&lt;/p>
&lt;blockquote>
&lt;p>一些资料上把这个当做拟凸函数的定义，并且当不等号严格成立时，称$f$是&lt;strong>严格拟凸&lt;/strong>的。&lt;/p>
&lt;/blockquote>
&lt;p>来看一些例子：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>$f(x_1, x_2)=x_1x_2, (x_1, x_2 &amp;gt; 0)$，容易看到 $\nabla^2 f$ 是不定的，因此既不是凸函数也不是凹函数。但是 $\{x\mid x_1x_2\ge\alpha\}$ 是凸集，所以 $f$ 是拟凹函数。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>线性分式函数 $f(x)=\displaystyle\frac{a^{T} x+b}{c^{T} x+d} \; (\operatorname{dom} f=\left\{x \mid c^{T} x+d&amp;gt;0\right\})$ 是拟线性的，因为&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>$$
\begin{aligned}S_{\alpha} &amp;amp;=\left\{x \mid c^{T} x+d&amp;gt;0,\left(a^{T} x+b\right) /\left(c^{T} x+d\right) \leq \alpha\right\} \\&amp;amp;=\left\{x \mid c^{T} x+d&amp;gt;0, a^{T} x+b \leq \alpha\left(c^{T} x+d\right)\right\}\end{aligned}\\
$$&lt;/p>
&lt;p>是凸集&lt;/p>
&lt;ul>
&lt;li>因为&lt;/li>
&lt;/ul>
&lt;p>$$
\operatorname{rank}(X+Y) \geq \min \{\operatorname{rank} X, \operatorname{rank} Y\}\\
$$&lt;/p>
&lt;p>所以 $f(X)=\operatorname{rank}(X)$ 是 $\mathrm{S}_+^n$ 上的拟凹函数。&lt;/p>
&lt;ul>
&lt;li>向上取整函数 $\operatorname{ceil}(x)=\inf \{z \in \mathbf{Z} \mid z \geq x\}$ 是拟凸的。&lt;/li>
&lt;/ul>
&lt;h3 id="mathrmr-上的拟凸函数">$\mathrm{R}$ 上的拟凸函数&lt;/h3>
&lt;p>结合上述的性质，可以给出 $\mathrm{R}$ 上的拟凸函数的特性：&lt;/p>
&lt;p>$\mathrm{R}$ 上的拟凸函数，要么是单调的，要么在一个点左边单调递减，右边单调递增。&lt;/p>
&lt;p>所以 $\mathrm{R}$ 上的拟凸函数也满足至多是单峰的（unimodal）这一特点。&lt;/p>
&lt;h3 id="可微分的拟凸函数">可微分的拟凸函数&lt;/h3>
&lt;p>类似于凸函数，当函数可微时，可以推导出拟凸函数需要满足的一阶条件和二阶条件。&lt;/p>
&lt;h4 id="一阶条件">一阶条件&lt;/h4>
&lt;p>定义在凸集上的函数 $f$ 拟凸， 当且仅当 $\forall x, y \in \operatorname{dom} f$，有：&lt;/p>
&lt;p>$$
f(y) \leq f(x) \Longrightarrow \nabla f(x)^{T}(y-x) \leq 0\\
$$&lt;/p>
&lt;p>该条件也有鲜明的几何意义。$\nabla f(x)$ 导出了过点 $x$ 的对下水平集 $\{y\mid f(y)\le f(x)\}$ 的支撑超平面。（高维情况很难想象，不妨考虑一维情况，这时候支撑超平面就退化为一个点，下水平集是一个区间）&lt;/p>
&lt;p>&lt;img src="../../figures/Convex-function/quasiconvex.png" alt="">&lt;/p>
&lt;blockquote>
&lt;p>注意到 $\nabla f(x_0)^T(y-x_0)=0$ 对于给定的 $x_0$，表示的是一个平面。&lt;/p>
&lt;/blockquote>
&lt;p>虽然拟凸函数和凸函数在一阶条件上具有相似性，但是拟凸函数并不能用一阶条件来判断全局的最小值。使拟凸函数的梯度为0的点不一定是 global minimizer.&lt;/p>
&lt;h4 id="二阶条件">二阶条件&lt;/h4>
&lt;p>假如定义在凸集上的函数 $f$ 是二阶可微的，那么 $f$ 拟凸，当且仅当 $\forall x \in \operatorname{dom} f, y \in \mathrm{R}^n$ 有：&lt;/p>
&lt;p>$$
y^{T} \nabla f(x)=0 \Longrightarrow y^{T} \nabla^{2} f(x) y \geq 0\\
$$&lt;/p>
&lt;p>在一维的情况下，这个条件简化为：&lt;/p>
&lt;p>$$
f^{\prime}(x)=0 \Longrightarrow f^{\prime \prime}(x) \geq 0\\
$$&lt;/p>
&lt;p>这个条件，意味着 $\nabla^2 f$ 在 $\nabla f^{\perp}$ 是半正定的，暗示 $\nabla^2f$ 至多有一个负特征值。&lt;/p>
&lt;blockquote>
&lt;p>$\mathrm{span}\{\nabla f\}$ 是一维的，从而 $\nabla f^{\perp}$是 $n-1$ 维的。如果 $\nabla^2f$ 在 $\nabla f^{\perp}$ 是正定的，那么才能说明 $f$ 是拟凸的。&lt;/p>
&lt;/blockquote>
&lt;h3 id="保持拟凸性的操作">保持拟凸性的操作&lt;/h3>
&lt;h4 id="拟凸函数非负加权和的最大值">拟凸函数非负加权和的最大值&lt;/h4>
&lt;p>这里就用之前提到过的第二种定义方式进行证明即可。同样可以推广到逐点上确界的情况。&lt;/p>
&lt;p>例：$\lambda_{\max }(X, Y)=\sup _{u \neq 0} \displaystyle\frac{u^{T} X u}{u^{T} Y u}=\sup \{\lambda | \operatorname{det}(\lambda Y-X)=0\}$，其中$X\in S^n, Y\in S^n_{++}$，$\lambda_{\max}$是拟凸的，叫做 $(X,Y)$ 的广义特征值。&lt;/p>
&lt;h4 id="函数复合-1">函数复合&lt;/h4>
&lt;p>如果 $g: \mathrm{R}^n \to \mathrm{R}$ 是拟凸的，并且 $h: \mathrm{R} \to \mathrm{R}$ 是单调非减的，那么 $f = h \circ g$ 是拟凸的。&lt;/p>
&lt;h4 id="最小化">最小化&lt;/h4>
&lt;p>如果 $f(x, y)$ 对 $(x, y)$ 是拟凸的，并且 $C$ 是凸集，那么 $g(x) = \displaystyle\inf_{y \in C} f(x, y)$ 是拟凸的。&lt;/p>
&lt;h2 id="对数凹对数凸函数">对数凹/对数凸函数&lt;/h2>
&lt;p>简单讲，$f&amp;gt;0$，并且 $\log f$ 是凹函数，那么 $f$ 就称为对数凹的。&lt;/p>
&lt;p>对数凹还可以用 $f(\theta x+(1-\theta) y) \geq f(x)^{\theta} f(y)^{1-\theta}, \forall \theta \in [0, 1]$ 来定义。从这里看，凸函数可以视作一种“算术平均”，对数凸则是“几何平均”。&lt;/p>
&lt;p>如果 $h$ 凸，那么 $e^h$ 也是凸函数，从而我们知道，对数凸函数也是凸函数。&lt;/p>
&lt;p>对数凹函数常见于统计学。统计学中的似然函数，是一个经常要取对数的函数，&lt;strong>欲求参数的极大似然估计值，其实就是一个关于似然函数的优化问题&lt;/strong>，如果似然函数是对数凹的，那么求对数似然函数的最大值，就是一个凸优化问题！这是研究对数凹函数的目的所在。&lt;/p>
&lt;ul>
&lt;li>标准正态分布的累计分布函数 $\Phi(x)=\displaystyle\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{x} e^{-u^{2} / 2} d u$ 是对数凹的。&lt;/li>
&lt;li>多元正态概率密度函数是 $f(x)=\displaystyle\frac{1}{\sqrt{(2 \pi)^{n} \operatorname{det} \Sigma}} e^{-\frac{1}{2}(x-\bar{x})^{T} \Sigma^{-1}(x-\bar{x})}$ 是对数凹的。&lt;/li>
&lt;li>指数分布的密度函数 $f(x) = \lambda e^{-\lambda x}$ 是对数凹的。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>事实上，很多常见的概率分布函数，都是对数凹的。&lt;/strong>&lt;/p>
&lt;p>如果 $f$ 具有良好的光滑性，通过 $\log f$ 的凹凸性，我们可以得到一些关于 $f$ 的性质：
因为：&lt;/p>
&lt;p>$$
\nabla^{2} \log f(x)=\frac{1}{f(x)} \nabla^{2} f(x)-\frac{1}{f(x)^{2}} \nabla f(x) \nabla f(x)^{T}\\
$$&lt;/p>
&lt;p>于是可以得到 $f$ 对数凹的一个充要条件：&lt;/p>
&lt;p>$$
f(x) \nabla^{2} f(x) \preceq \nabla f(x) \nabla f(x)^{T}\\
$$&lt;/p>
&lt;p>在一元函数的情况，就是：$f\cdot f^{''}\leq(f^{'})^2$&lt;/p>
&lt;p>此外，对数凸/凹性是对乘法保持封闭的。因为：&lt;/p>
&lt;p>$$
h(x)=f(x)g(x)\Rightarrow \log h(x) = \log f(x) + \log g(x)\\
$$&lt;/p>
&lt;p>容易看出，&lt;strong>如果概率密度函数是对数凹的，那么多个密度函数相乘的结果也是对数凹的&lt;/strong>。&lt;/p>
&lt;p>对于积分，也有结果：&lt;/p>
&lt;p>$$
f \;\text{ log-concave} \Rightarrow g(x) = \int f(x, y) \mathrm{d} y \;\text{ log-concave}\\
$$&lt;/p>
&lt;p>应用到密度函数的卷积上：&lt;/p>
&lt;p>$$
(f \ast g)(x)=\int f(x-y) g(y) d y\\
$$&lt;/p>
&lt;p>&lt;strong>这说明 $f, g$ 如果 log-concave，那么它们的卷积也是对数凹的！&lt;/strong>&lt;/p>
&lt;h2 id="广义不等式下的凸性">广义不等式下的凸性&lt;/h2>
&lt;p>通过上一章提到的广义不等式，可以借助 proper cone 定义多元函数的“单调递增”和“严格单调递增”。&lt;/p>
&lt;p>$f: \mathrm{R}^n \to \mathrm{R}$ 叫做 $K$&lt;em>-nondecreasing&lt;/em> 如果：&lt;/p>
&lt;p>$$
x \preceq_{K} y \Longrightarrow f(x) \leq f(y),\\
$$&lt;/p>
&lt;p>K*-increasing* 如果：&lt;/p>
&lt;p>$$
x \preceq_{K} y, x \neq y \Longrightarrow f(x)&amp;lt;f(y) .\\
$$&lt;/p>
&lt;p>类似可以定义 $K$&lt;em>-nonincreasing&lt;/em> 和 $K$&lt;em>-decreasing&lt;/em>.&lt;/p>
&lt;p>例子：&lt;/p>
&lt;ul>
&lt;li>$f$ 是 $K=\mathrm{R}^n_+$ 上的 $K$&lt;em>-nondecreasing&lt;/em> 函数如果：$x_{1} \leq y_{1}, \ldots, x_{n} \leq y_{n} \Longrightarrow f(x) \leq f(y)$&lt;/li>
&lt;li>$f(X)=\mathrm{det} X$ 是 $\mathrm{S}^n_{+}$ 上的 &lt;em>increasing&lt;/em> 函数。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>如果$X$是半正定矩阵，那么$|X+I|&amp;gt;|X|$。（借助特征值证明）&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>$f(X)=\mathrm{tr} X^{-1}$是 $\mathrm{S}^n_{++}$ 上的 &lt;em>decreasing&lt;/em> 函数。&lt;/li>
&lt;/ul>
&lt;h3 id="单调性的梯度条件">单调性的梯度条件&lt;/h3>
&lt;p>对于这种新的单调性，我们可以用广义不等式下的梯度条件去判断。&lt;/p>
&lt;ul>
&lt;li>$f$ is &lt;em>K-nondecreasing&lt;/em> $\Leftrightarrow$ $\nabla f(x) \succeq_{K^{\ast}} 0 \quad\forall x \in \mathrm{dom} f$&lt;/li>
&lt;li>$f$ is &lt;em>K-increasing&lt;/em> $\Leftrightarrow$ $\nabla f(x) \succ_{K^{\ast}} 0\quad\forall x \in \mathrm{dom} f$&lt;/li>
&lt;/ul>
&lt;p>函数的梯度在对偶不等式的情况下是非负的。其实 $\nabla f(x) \succeq_{K^{\ast}} 0$ 这里暗指的，就是 $f$ 在 $K$ 中的每一个方向都是单调递增的。&lt;/p>
&lt;h3 id="广义不等式下的凸性-1">广义不等式下的凸性&lt;/h3>
&lt;p>进一步，通过 proper cone 还能把函数的凸性推广到向量值函数上：&lt;/p>
&lt;p>令 $K$ 是一个 proper cone，如果 $\forall x, y$，都有：&lt;/p>
&lt;p>$$
f(\theta x+(1-\theta) y) \preceq_{K} \theta f(x)+(1-\theta) f(y), \quad \;\forall \theta \in [0, 1]\\
$$&lt;/p>
&lt;p>就说 $f$ 是 $K$&lt;em>-convex&lt;/em> 的。&lt;/p>
&lt;p>特别地，如果：&lt;/p>
&lt;p>$$
f(\theta x+(1-\theta) y) \prec_{K} \theta f(x)+(1-\theta) f(y), \quad \;\forall \theta \in (0,1)\\
$$&lt;/p>
&lt;p>就说 $f$ 是 &lt;em>strictly&lt;/em> $K$&lt;em>-convex&lt;/em> 的。&lt;/p>
&lt;p>$f$ 的定义域不必拘泥于欧式空间！&lt;strong>proper cone 定义好了序的关系，使我们可以研究任意向量空间到向量空间的映射的凸性！&lt;/strong> 并且这种定义包容了我们通常理解的凸性。&lt;/p>
&lt;p>例：&lt;/p>
&lt;p>矩阵函数 $g: \mathrm{R}^{m \times n} \rightarrow \mathrm{S}^{n}$：&lt;/p>
&lt;p>$$
g(X)=X^{T} A X+B^{T} X+X^{T} B+C\\
$$&lt;/p>
&lt;p>其中 $A \in \mathbf{S}^{m}, B \in \mathbf{R}^{m \times n}$, $C \in \mathbf{S}^{n}$, 是凸函数当且仅当 $A \succeq 0$.&lt;/p>
&lt;hr>
&lt;h2 id="第二部分">第二部分&lt;/h2>
&lt;p>这是一条分割线，如果有时间，我会在下面继续补充一些重要的知识。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/" term="凸优化" label="凸优化"/><category scheme="https://allenz-me.github.io/tags/%E5%87%B8%E5%87%BD%E6%95%B0/" term="凸函数" label="凸函数"/><category scheme="https://allenz-me.github.io/tags/%E6%8B%9F%E5%87%B8%E5%87%BD%E6%95%B0/" term="拟凸函数" label="拟凸函数"/></entry><entry><title type="text">Convex Function — 续</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cvxopt/convex-function2/"/><id>https://allenz-me.github.io/posts/cvxopt/convex-function2/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-02T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">这是 convex function 的第二部分。 凸函数 对凸性的刻画，从弱到强，分别是： 拟凸 (quasiconvex) $f$ 的所有 sublevel sets $L_\alpha = \{x \mid……</summary><content type="html">&lt;p>这是 convex function 的第二部分。&lt;/p>
&lt;hr>
&lt;h2 id="凸函数">凸函数&lt;/h2>
&lt;p>对凸性的刻画，从弱到强，分别是：&lt;/p>
&lt;h3 id="拟凸-quasiconvex">拟凸 (quasiconvex)&lt;/h3>
&lt;p>$f$ 的所有 sublevel sets $L_\alpha = \{x \mid f(x) \leq \alpha\}$ 是凸集。&lt;/p>
&lt;h3 id="凸-convex">凸 (convex)&lt;/h3>
&lt;p>定义在凸集上的函数 $f$ 满足 $\forall x, y, \lambda \in [0, 1]$ 成立：
$$
f(\lambda x + (1-\lambda )y) \leq \lambda f(x) + (1-\lambda) f(y)
$$
则称 $f$ 是凸函数。&lt;/p>
&lt;h4 id="epigraph-characterization-of-convexity">Epigraph characterization of convexity&lt;/h4>
&lt;p>一个函数的 epigraph 指的是：
$$
\text { epi } f=\left\{(x, r) \in \mathbb{R}^{n} \times \mathbb{R} \mid f(x) \leq r\right\}
$$
如果把上面的小于等于号换成小于号，就是 $f$ 的 strict epigraph $\operatorname{epi}_s(f)$ .&lt;/p>
&lt;p>$f$ 是凸函数等价于：&lt;/p>
&lt;ul>
&lt;li>$\operatorname{epi}(f)$ 是凸集&lt;/li>
&lt;li>$\operatorname{epi}_s(f)$ 是凸集&lt;/li>
&lt;/ul>
&lt;h4 id="first-order-characterization-of-convexity">First-order characterization of convexity&lt;/h4>
&lt;p>如果函数 $f$ 可微，那么 $f$ 的凸性可以由一阶条件来刻画：
$$
f(x) \geq f\left(x_{0}\right)+\left\langle\nabla f\left(x_{0}\right), x-x_{0}\right\rangle \quad \forall x, x_0 \in \operatorname{dom} f
$$&lt;/p>
&lt;p>&lt;strong>Monotonicity of gradient&lt;/strong>&lt;/p>
&lt;p>可微的函数 $f$ 是凸函数当且仅当：
$$
\langle\nabla f(x)-\nabla f(y), x-y\rangle \geq 0 \quad \forall x, y \in \operatorname{dom} f
$$&lt;/p>
&lt;h4 id="second-order-characterization-of-convexity">Second-order characterization of convexity&lt;/h4>
&lt;p>二次可微的函数 $f$ 是凸函数的充要条件是： $\nabla^2 f (x) \succeq 0, \;\forall x \in \operatorname{dom} f$ .&lt;/p>
&lt;h3 id="严格凸-strictly-convex">严格凸 (strictly convex)&lt;/h3>
&lt;p>定义在凸集上的函数 $f$ 满足 $\forall x \neq y, \lambda \in (0, 1)$ 成立：
$$
f(\lambda x + (1-\lambda )y) &amp;lt; \lambda f(x) + (1-\lambda) f(y)
$$
则称 $f$ 是严格凸函数。&lt;/p>
&lt;h4 id="first-order-characterization-of-strict-convexity">First-order characterization of strict convexity&lt;/h4>
&lt;p>如果函数 $f$ 可微，那么 $f$ 的严格凸性可以由一阶条件来刻画：
$$
f(x) &amp;gt; f\left(x_{0}\right)+\left\langle\nabla f\left(x_{0}\right), x-x_{0}\right\rangle \quad \forall x \neq x_0 \in \operatorname{dom} f
$$&lt;/p>
&lt;p>&lt;strong>Monotonicity of gradient&lt;/strong>&lt;/p>
&lt;p>可微的函数 $f$ 是严格凸函数当且仅当：
$$
\langle\nabla f(x)-\nabla f(y), x-y\rangle &amp;gt; 0 \quad \forall x \neq y \in \operatorname{dom} f
$$&lt;/p>
&lt;h4 id="second-order-characterization-of-strict-convexity">Second-order characterization of strict convexity&lt;/h4>
&lt;p>二次可微的函数 $f$ 是严格凸函数的充分条件是： $\nabla^2 f \succ 0, \; \forall x \in \operatorname{dom} f$ .&lt;/p>
&lt;h3 id="强凸-strongly-convex">强凸 (strongly convex)&lt;/h3>
&lt;p>存在 $\sigma &amp;gt; 0$ 使得定义在凸集上的函数 $f$ 满足 $\forall x, y, \lambda \in [0, 1]$ 成立：
$$
f(\lambda x+(1-\lambda) y) \leq \lambda f(x)+(1-\lambda) f(y)-\frac{\sigma}{2} \lambda(1-\lambda)\|x-y\|^{2}
$$
则称 $f$ 是强凸函数，且 $\sigma$ 称为强凸系数 (modulus of strong convexity)。上式的等价定义是：$f(x) - \displaystyle\frac{\sigma}{2} \|x \|^2$ 是凸函数。&lt;/p>
&lt;h4 id="first-order-characterization-of-strong-convexity">First-order characterization of strong convexity&lt;/h4>
&lt;p>如果函数 $f$ 可微，那么 $f$ 以系数 $\sigma$ 强凸可以由一阶条件来刻画：
$$
f(x) \geq f\left(x_{0}\right)+\left\langle\nabla f\left(x_{0}\right), x-x_{0}\right\rangle+\frac{1}{2} \sigma \left\|x-x_{0}\right\|^{2} .
$$&lt;/p>
&lt;h4 id="monotonicity-of-gradient">Monotonicity of gradient&lt;/h4>
&lt;p>可微的函数 $f$ 以系数 $\sigma$ 强凸当且仅当：
$$
\langle\nabla f(x)-\nabla f(y), x-y\rangle \geq \sigma\|x-y\|^{2} \quad \forall x, y \in \operatorname{dom} f
$$&lt;/p>
&lt;h4 id="second-order-characterization-of-strong-convexity">Second-order characterization of strong convexity&lt;/h4>
&lt;p>二次可微的函数 $f$ 以系数 $\sigma$ 强凸当且仅当 $\nabla^2 f(x) \succ \sigma I$ .&lt;/p>
&lt;h2 id="infimal-convolution">Infimal Convolution&lt;/h2>
&lt;p>函数 $f$ 和 $g$ 的 &lt;strong>infimal convolution&lt;/strong> (or epi-sum) 指的是：&lt;/p>
&lt;p>$$
\begin{aligned}
(f\, \square\, g)(x) &amp;amp;= \inf_{y \in \mathrm{R}^{n}} \left\{f(x-y)+g(y)\right\} \\
&amp;amp;= \inf \{ f(x_1) + g(x_2) \mid x_1 + x_2 = x\}
\end{aligned}
$$&lt;/p>
&lt;p>其几何含义是：$\operatorname{epi}(f \,\square\, g) = \operatorname{epi}(f) + \operatorname{epi}(g)$ 或是 $\operatorname{epi}_s(f \,\square\, g) = \operatorname{epi}_s(f) + \operatorname{epi}_s(g)$，由此可见，凸函数的 infimal convolution 是凸的。同时我们还得到：$\operatorname{dom} (f \,\square\, g) = \operatorname{dom} f + \operatorname{dom} g$ .&lt;/p>
&lt;p>还可以定义一族函数的 infimal convolution:
$$
\begin{aligned}
f(x) &amp;amp;=\left(f_{1} \square f_{2} \square \ldots f_{m}\right)(x) \\
&amp;amp;=\inf \left\{\sum_{k=1}^{m} f\left(x_{k}\right) \mid x_{k} \in \mathrm{R}^{n}, \sum_{k=1}^{m} x_{k}=x\right\}
\end{aligned}
$$&lt;/p>
&lt;p>例：&lt;/p>
&lt;ol>
&lt;li>$I_C \square \| \cdot \| = d_C(\cdot)$&lt;/li>
&lt;li>$I_{C_1} \square I_{C_2} = I_{C_1 + C_2}$&lt;/li>
&lt;li>如果 $f_1(x)=\displaystyle\frac{1}{2} x^T A_1 x,\, f_2(x)=\displaystyle\frac{1}{2} x^T A_2 x$，则 $(f_1 \square f_2) (x) = \displaystyle\frac{1}{2} x^T(A_1^{-1} + A_2^{-1})^{-1} x$&lt;/li>
&lt;/ol>
&lt;p>infimal convolution 有以下性质：&lt;/p>
&lt;ul>
&lt;li>commutativity: $f \square g = g \square f$&lt;/li>
&lt;li>associativity: $(f \square g) \square h = f \square (g \square h)$&lt;/li>
&lt;li>preserves the order: $f_1 \leq f_2 \Rightarrow f_1 \square g \leq f_2 \square g$&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Infimal convolution of support functions&lt;/strong>&lt;/p>
&lt;p>Let $\left\{C_{k}\right\}_{k=1, \ldots, m}$ be non-empty convex sets, $C_{k} \in \mathbb{R}^{n}$. Then
$$
\begin{gathered}
\delta^{\ast}\left(\cdot \mid C_{1}+\ldots+C_{m}\right)=\delta^{\ast}\left(\cdot \mid C_{1}\right)+\ldots+\delta^{\ast}\left(\cdot \mid C_{m}\right) \\
\delta^{\ast}\left(\cdot \mid \operatorname{cl} C_{1} \cap \ldots \cap \operatorname{cl} C_{m}\right)=\operatorname{cl}\left\{\delta^{\ast}\left(\cdot \mid C_{1}\right) \square \ldots \delta^{\ast}\left(\cdot \mid C_{m}\right)\right\}
\end{gathered}
$$&lt;/p>
&lt;h2 id="凸函数的连续性">凸函数的连续性&lt;/h2>
&lt;p>$\mathrm{R}^n \to \mathrm{R}$的凸函数是连续函数。&lt;/p>
&lt;p>&lt;strong>proper 的凸函数在相对内点集上是连续的。&lt;/strong>&lt;/p>
&lt;p>由此可见，$\mathrm{R}^n$ 上的凸函数一定是闭的（下半连续）。&lt;/p>
&lt;h2 id="log-determinant">Log-determinant&lt;/h2>
&lt;p>$\mathrm{S}_{++}^n \to \mathrm{R}$ 的函数 $f(X) = \log \det X$ 称为 log-det function，其梯度：
$$
\nabla f(X) = X^{-1}
$$
先证明 $\nabla f(I) = I$，设 $\Delta \in \mathrm{S}^n$ 其特征值为 $\lambda_1, \lambda_2, \dots, \lambda _n$
$$
\begin{aligned}
f(I+\Delta)-f(I)-\langle I, \Delta\rangle &amp;amp;=\log \left(\prod_{i=1}^{n}\left(\lambda_{i}+1\right)\right)-\operatorname{tr}(\Delta)=\sum_{i=1}^{n}\left[\log \left(\lambda_{1}+1\right)-\lambda_{i}\right] \\
&amp;amp;=o(\operatorname{tr}(\Delta))=o\left(\sqrt{\operatorname{tr}\left(\Delta^{T} \Delta\right)}\right)=o(\|\Delta\|)
\end{aligned}
$$
对于一般情形：
$$
\begin{aligned}
&amp;amp; f(X+\Delta)-f(X)-\left\langle X^{-1}, \Delta\right\rangle \\
=&amp;amp; \log \left(\operatorname{det}\left(X^{\frac{1}{2}}\left(I+X^{-\frac{1}{2}} \Delta X^{-\frac{1}{2}}\right) X^{\frac{1}{2}}\right)\right)-\log (\operatorname{det}(X))-\operatorname{tr}\left(X^{-1} \Delta\right) \\
=&amp;amp; \log \left(\operatorname{det}\left(I+X^{-\frac{1}{2}} \Delta X^{-\frac{1}{2}}\right)\right)-\operatorname{tr}\left(X^{-\frac{1}{2}} \Delta X^{-\frac{1}{2}}\right) \\
=&amp;amp; o\left(\operatorname{tr}\left(X^{-\frac{1}{2}} \Delta X^{-\frac{1}{2}}\right)\right) = o(\|\Delta\|)
\end{aligned}
$$
以上的内积都是矩阵的 Frobenius 内积。&lt;/p>
&lt;p>或者使用复合函数的求导公式：
$$
\frac{\partial}{\partial X_{i j}} \log \operatorname{det} X=\frac{1}{\operatorname{det} X} \frac{\partial \operatorname{det} X}{\partial X_{i j}}=\frac{1}{\operatorname{det} X} \operatorname{adj}(X)_{j i}=\left(X^{-1}\right)_{j i}
$$
其中 $\operatorname{adj}(X)$ 表示 $X$ 的伴随矩阵，满足 $\operatorname{adj}(X) = \det(X) \cdot X^{-1}$ .&lt;/p>
&lt;p>&lt;strong>Jacobi's formula&lt;/strong>&lt;/p>
&lt;p>$$
\frac{d}{d t} \operatorname{det} A(t)=\operatorname{tr}\left(\operatorname{adj}(A(t)) \frac{d A(t)}{d t}\right)=(\operatorname{det} A(t)) \cdot \operatorname{tr}\left(A(t)^{-1} \cdot \frac{d A(t)}{d t}\right)
$$&lt;/p>
&lt;p>由此得到&lt;/p>
&lt;p>$$
\frac{\partial \det A}{\partial A_{ij}} = \operatorname{adj}(A)_{ji}
$$&lt;/p>
&lt;p>根据 $f(X)$ 的梯度，我们能得到它的一阶近似：
$$
f(Z) \approx f(X) + \langle X^{-1}, Z - X \rangle = f(X)+\operatorname{tr}\left(X^{-1}(Z-X)\right)
$$&lt;/p>
&lt;h3 id="convexity-of-log-determinant">Convexity of log-determinant&lt;/h3>
&lt;p>【待】&lt;/p>
&lt;h2 id="半连续-semi-continuity">半连续 Semi-Continuity&lt;/h2>
&lt;p>半连续是对连续性的一种弱化，跟连续性类似，它有分析学上的定义，也有拓扑学意义上的定义。&lt;/p>
&lt;p>&lt;strong>分析学意义&lt;/strong>&lt;/p>
&lt;p>称 $f$ 在 $\bar{x}$ 下半连续, 如果 $\displaystyle\liminf _{x \rightarrow \bar{x}} f(x)\geq f(\bar{x})$&lt;/p>
&lt;p>称 $f$ 在 $\bar{x}$ 上半连续, 如果 $\displaystyle\limsup _{x \rightarrow \bar{x}} f(x) \leq f(\bar{x})$&lt;/p>
&lt;p>上（下）半连续函数是在各个点都上（下）半连续的函数。&lt;/p>
&lt;p>&lt;strong>拓扑学意义&lt;/strong>&lt;/p>
&lt;p>Let $f$ be a real (or extended-real) function on a topological space. If
$$
\{x: f(x)&amp;gt;\alpha\}
$$
is open for every real $\alpha, f$ is said to be &lt;em>lower semi-continuous&lt;/em>. If
$$
\{x: f(x)&amp;lt;\alpha\}
$$
is open for every real $\alpha, f$ is said to be &lt;em>upper semi-continuous&lt;/em>.&lt;/p>
&lt;p>最简单的例子是，开集的 indicator function $\mathbf{1}_A(x) = \begin{cases} 1 &amp;amp; x\in A\\ 0 &amp;amp; x \notin A \end{cases}$ 是下半连续的，闭集的 indicator function 是上半连续的。&lt;/p>
&lt;p>$\mathrm{R}$ 上的半连续函数：&lt;/p>
&lt;img src="../../figures/Convex-function-2/Lower-left-and-upper-right-semi-continuous-functions.png" alt="Lower (left) and upper (right) semi-continuous functions" style="zoom:67%;" />
&lt;p>&lt;strong>等价定义&lt;/strong>&lt;/p>
&lt;p>$f: X \to \mathrm{\bar{R}}$ 是上半连续的等价于：&lt;/p>
&lt;ul>
&lt;li>All superlevel sets $\{x \in X: f(x) \geq y\}$ with $y \in \mathrm{R}$ are closed in $X$.&lt;/li>
&lt;li>The hypograph $\{(x, t) \in X \times \mathrm{R}: t \leq f(x)\}$ is closed in $X \times \mathrm{R}$.&lt;/li>
&lt;/ul>
&lt;p>$f: X \to \mathrm{\bar{R}}$ 是下半连续的等价于：&lt;/p>
&lt;ul>
&lt;li>All sublevel sets $\{x \in X: f(x) \leq y\}$ with $y \in \mathrm{R}$ are closed in $X$.&lt;/li>
&lt;li>The epigraph $\{(x, t) \in X \times \mathrm{R}: t \geq f(x)\}$ is closed in $X \times \mathrm{R}$.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>在凸优化中，有时把闭函数定义为 epigraph 为闭集的函数，它与下半连续函数是等价的。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>性质&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>$f$ 连续当且仅当它是上半连续和下半连续的。&lt;/li>
&lt;li>下半连续函数的和是下半连续的；上半连续函数的和是下半连续的。&lt;/li>
&lt;li>$f$ 下半连续当且仅当 $-f$ 是上半连续的。&lt;/li>
&lt;li>紧集上的下半连续函数存在最小值；紧集上的上半连续函数存在最大值。两个联系起来就是紧集上的连续函数存在最值。(Weierstrass extreme value theorem)&lt;/li>
&lt;/ul></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/" term="凸优化" label="凸优化"/></entry><entry><title type="text">Convex Set</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cvxopt/convex-set/"/><id>https://allenz-me.github.io/posts/cvxopt/convex-set/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">凸的背后，往往蕴含着非常好的性质，这也就是我们要去研究凸集的原因。凸，在优化理论扮演……</summary><content type="html">&lt;!-- #! https://zhuanlan.zhihu.com/p/441144463 -->
&lt;p>凸的背后，往往蕴含着非常好的性质，这也就是我们要去研究凸集的原因。凸，在优化理论扮演着非常重要的角色。本文是我的一个注记，不会包含太多证明，主要取自Boyd的 《convex optimization》、GTM 264 《Functional Analysis, Calculus of Variations and Optimal Control》。&lt;/p>
&lt;p>未经特别说明，讨论的空间都是 $\mathrm{R}^n$ 。&lt;/p>
&lt;h2 id="仿射和凸集">仿射和凸集&lt;/h2>
&lt;h3 id="仿射集">仿射集&lt;/h3>
&lt;p>一个集合 $C$ 是仿射集（affine set），如果对任意 $x_1, x_2 \in C$ 和 $\theta \in \mathrm{R}$ 都有：&lt;/p>
&lt;p>$$
\theta x_1 + (1-\theta)x_2 \in C\\
$$&lt;/p>
&lt;p>需要注意的是这里的 $\theta$ 取值是全体实数（而不是介于0到1之间）。这意味着仿射集中任意两点连成的直线位于集合内。&lt;/p>
&lt;p>仿射集可以写成一个线性子空间的仿射，即存在子空间 $V$ 使得：&lt;/p>
&lt;p>$$
C=V+x_{0}=\left\{v+x_{0} \mid v \in V\right\}\\
$$&lt;/p>
&lt;p>类比非齐次线性方程组的解是一个仿射子集而对应的齐次线性方程组的解是一个线性子空间。&lt;/p>
&lt;p>$\{0\}$ 是一个子空间，单点集是一个仿射集。&lt;/p>
&lt;p>定义 $x_1, x_2, \dots, x_n$ 的 &lt;strong>affine combination&lt;/strong> 为：
$$
\theta_1 x_1 + \cdots + \theta_n x_n \qquad \text{where } \; \sum_{i=1}^n \theta_i = 1
$$
对任意一个集合$C$，我们可以定义它的仿射包 （affine hull）为其所有点的 affine combination：&lt;/p>
&lt;p>$$
\text { aff } C=\left\{\theta_{1} x_{1}+\cdots+\theta_{k} x_{k} \mid x_{1}, \ldots, x_{k} \in C, \theta_{1}+\cdots+\theta_{k}=1\right\}\\
$$&lt;/p>
&lt;p>仿射包是包含这个集合的最小仿射集。&lt;/p>
&lt;blockquote>
&lt;p>affine hull、convex hull、conic hull 可以由内向外来定义（各个点的组合），也可以由外向内定义（所有包含该集合的xx的交）。&lt;/p>
&lt;/blockquote>
&lt;p>任何一个非空仿射集 $C$ 都可以写成一个 $x + U$，其中 $x \in C, \; U$ 是一个子空间。仿射集的维数定义为与它平行的子空间的维数。&lt;/p>
&lt;p>通过仿射包可以定义相对内点（relative interior）这一概念。仿射集可以认为是一个平移后的子空间，所以仿射包能揭示一个集合真正的维度。&lt;/p>
&lt;p>相对内点与内点是有所不同的，比如考虑一个三维空间中的正方形。这个正方形内部的点，不是内点，但是是相对内点，只要我们把空间缩小为一个特定的二维空间，其相对内点就会变成真正的内点。再考虑三维空间中的一个球面，它的相对内点集是空集。&lt;/p>
&lt;p>相对内点准确的定义是：&lt;/p>
&lt;p>$$
\text { relint } C=\{x \in C \mid B(x, r) \cap \text { aff } C \subseteq C \; \text { for some } r&amp;gt;0\}\\
$$&lt;/p>
&lt;p>其中 $B(x, r)=\{y \mid\|y-x\| \leq r\}$。通过相对内点还可以引出相对边界这一概念。还是上面那两个例子，正方形的相对边界就是正方形的四条边，球面的相对边界就是自己。&lt;/p>
&lt;h3 id="凸集">凸集&lt;/h3>
&lt;p>一个集合 $C$ 是凸集（convex set），如果对任意 $x_1, x_2 \in C$ 和 $\theta \in [0, 1]$ 都有：&lt;/p>
&lt;p>$$
\theta x_1 + (1-\theta)x_2 \in C\\
$$&lt;/p>
&lt;p>仿射集自然是凸集的一种，类比于仿射集的相关概念，我们可以得到&lt;strong>凸包&lt;/strong>（&lt;em>convex hull&lt;/em> ）的定义：&lt;/p>
&lt;p>$$
\operatorname{conv} C=\left\{\theta_{1} x_{1}+\cdots+\theta_{k} x_{k} \mid x_{i} \in C, \theta_{i} \geq 0, i=1, \ldots, k, \theta_{1}+\cdots+\theta_{k}=1\right\}\\
$$&lt;/p>
&lt;p>集合 $\mathrm{C}$ 的凸包是包含集合 $\mathrm{C}$ 的最小凸集。&lt;/p>
&lt;h3 id="锥">锥&lt;/h3>
&lt;p>在优化理论中，锥（Cones）是一个无限大的集合，与日常所接触的圆锥有所不同。&lt;/p>
&lt;p>一个集合$C$是锥，如果对任意 $x \in C$ 和 $\theta \geq 0$ 都有：&lt;/p>
&lt;p>$$
\theta x \in C\\
$$&lt;/p>
&lt;p>&lt;strong>锥不一定是凸的&lt;/strong>。如果一个锥是凸的，就称它是一个&lt;strong>凸锥&lt;/strong>（convex cone）。比如 $y=|x|$ 的图像就是一个锥，但不是凸锥。但是 $y\ge |x|$ 就是一个凸的锥了。注意到任何锥都包含原点 $\mathbf{0}$。&lt;/p>
&lt;p>&lt;strong>锥也不不一定是闭的&lt;/strong>。比如非负象限锥去掉 $x$ 轴正半轴，仍然是一个锥，但不是闭的。&lt;/p>
&lt;p>一个锥 $K$ 是凸锥的充分必要条件：$K + K \subset K$&lt;/p>
&lt;p>类似的可以得到锥包（conic hull ），&lt;strong>它是包含某个集合的最小的凸锥&lt;/strong>。&lt;/p>
&lt;p>$$
\operatorname{cone}(C) = \{\theta_1 x_1 + \cdots +\theta_k x_k \mid x_i \in C, \theta_i \geq 0, i=1, ..., k\}\\
$$&lt;/p>
&lt;p>称一个锥 &lt;em>pointed&lt;/em>（尖锐的），如果它不包含任何的直线。&lt;/p>
&lt;h3 id="示例">示例&lt;/h3>
&lt;p>下面列举一些凸集的例子：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>空集、一个点、一个线段、全空间&lt;/p>
&lt;/li>
&lt;li>
&lt;p>超平面和半平面&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>$\mathrm{R}^n$ 中超平面有形式：&lt;/p>
&lt;p>$$
\left\{x \mid a^{T} x=b\right\}, \quad a \neq 0\\
$$&lt;/p>
&lt;p>$a$ 是这个超平面的法向。也可以写成：&lt;/p>
&lt;p>$$
\left\{x \mid a^{T}\left(x-x_{0}\right)=0\right\} ,\quad a \neq 0\\
$$&lt;/p>
&lt;p>一个超平面将全空间分成了两个半平面，一个半平面有形式：&lt;/p>
&lt;p>$$
\left\{x \mid a^{T} x\leq b\right\}, \quad a \neq 0\\
$$&lt;/p>
&lt;ul>
&lt;li>球、椭球&lt;/li>
&lt;/ul>
&lt;p>欧式空间中的球有形式：&lt;/p>
&lt;p>$$
B\left(x_{c}, r\right)=\left\{x \mid\left\|x-x_{c}\right\|_{2} \leq r\right\}=\left\{x \mid\left(x-x_{c}\right)^{T}\left(x-x_{c}\right) \leq r^{2}\right\}\\
$$&lt;/p>
&lt;p>其中 $x_c$ 是球心，$r$ 是半径。另一种常用的表达方式是：&lt;/p>
&lt;p>$$
B\left(x_{c}, r\right)=\left\{x_{c}+r u \mid\|u\|_{2} \leq 1\right\}\\
$$&lt;/p>
&lt;p>椭球会借助一个正定矩阵来表示：&lt;/p>
&lt;p>$$
\mathcal{E}=\left\{x \mid\left(x-x_{c}\right)^{T} P^{-1}\left(x-x_{c}\right) \leq 1\right\}\\
$$&lt;/p>
&lt;p>$x_c$ 是椭球的球心，$P = P^T$ 是正定矩阵。椭球各个轴的方向是 $P$ 的特征向量的方向，每个半轴的长度是 $\sqrt{\lambda_i}$，即特征值开根号。另一种表达方式是：&lt;/p>
&lt;p>$$
\mathcal{E}=\left\{x_{c}+A u \mid\|u\|_{2} \leq 1\right\}\\
$$&lt;/p>
&lt;ul>
&lt;li>正规锥（norm cones）&lt;/li>
&lt;/ul>
&lt;p>$$
C=\{(x, t) |\|x\| \leq t\} \subseteq \mathbf{R}^{n+1}\\
$$&lt;/p>
&lt;p>也被叫做二阶锥（second order cones）、冰淇淋锥（ice-cream cones）。&lt;/p>
&lt;ul>
&lt;li>凸多面体（polyhedron）&lt;/li>
&lt;/ul>
&lt;p>$$
\mathcal{P}=\left\{x | a_{j}^{T} x \leq b_{j}, j=1, \ldots, m, c_{j}^{T} x=d_{j}, j=1, \ldots, p\right\}\\
$$&lt;/p>
&lt;p>线性规划的可行域是一个 polyhedron；当一个 polyhedron 有界时，也经常叫 polytope。非负象限是一个特殊的 polyhedron：&lt;/p>
&lt;p>$$
\mathbf{R}_{+}^{n}=\left\{x \in \mathbf{R}^{n} \mid x_{i} \geq 0, i=1, \ldots, n\right\}=\left\{x \in \mathbf{R}^{n} \mid x \succeq 0\right\}\\
$$&lt;/p>
&lt;p>非负象限既是一个锥，也是一个凸多面体，因此有一个特殊的名称，polyhedral cone，一般可表示为 $\{x \mid Ax \geq 0\}$。&lt;/p>
&lt;ul>
&lt;li>单纯形 （simplex）&lt;/li>
&lt;/ul>
&lt;p>单纯形是一类重要的凸多面体，它由 $k+1$ 个不在同一个平面的点 $v_0, \dots, v_k \in \mathbf{R}^n$ 的凸组合构成。&lt;/p>
&lt;p>$$
C=\operatorname{conv}\left\{v_{0}, \ldots, v_{k}\right\}=\left\{\theta_{0} v_{0}+\cdots+\theta_{k} v_{k} \mid \theta \succeq 0,1^{T} \theta=1\right\}\\
$$&lt;/p>
&lt;p>不同的两个点、不共线的三个点、不共面的四个点，它们生成的凸包，分别构成了一维、二维、三维的单纯形。&lt;/p>
&lt;p>特别地，unit simplex，或者说 probability simplex，由 $n$ 个直角坐标单位向量的凸组合构成：&lt;/p>
&lt;p>$$
x \succeq 0, \quad \mathbf{1}^{T} x \leq 1\\
$$&lt;/p>
&lt;p>离散型随机变量的概率分布就是一个典型的 unit simplex。&lt;/p>
&lt;ul>
&lt;li>半定矩阵锥（positive semideﬁnite cone）&lt;/li>
&lt;/ul>
&lt;p>我们用记号 $\mathrm{S}^n$ 来表示所有的 $n$ 阶实对称矩阵组成的集合， $\mathrm{S}^n_+$ 表示所有的半正定矩阵，$\mathrm{S}^n_{++}$ 表示所有的正定矩阵。其中，$\mathrm{S}_{+}^n$ 是一个闭的 convex cone。所有的 $n$ 阶方阵是一个线性空间，$\mathrm{S}^n_{+}$ 是这个线性空间的一个锥。
半定矩阵锥，让我们 &lt;strong>对凸集的认识，从欧式空间，飞跃到了抽象的线性空间！&lt;/strong> 这让我们得以在矩阵空间上考虑优化问题。这也是锥线性规划的起点！&lt;/p>
&lt;ul>
&lt;li>几类概率约束&lt;/li>
&lt;/ul>
&lt;p>给定一个离散型随机变量 $X$，$P(X=a_i)=p_i, i=1,2,...n$，已知概率分布 $\vec{p}$ 构成一个标准的单纯形，这是一个凸集，且以下概率约束对 $\vec{p}$ 也是凸的：&lt;/p>
&lt;ol>
&lt;li>$\alpha \leq \mathbb{E}f(X)\leq \beta$，其中 $f$ 是给定的函数&lt;/li>
&lt;li>$\mathbb{P}(X &amp;gt; \alpha)\leq \beta$&lt;/li>
&lt;li>$\mathbb{E}X^2\leq \alpha, \, \mathbb{E}X^2 \geq \alpha$&lt;/li>
&lt;li>$\mathrm{Var}(X)\geq \alpha$（ $\mathrm{Var}(X) \leq \alpha$ 不是凸的约束）&lt;/li>
&lt;/ol>
&lt;p>此外，KL散度 $D_{KL}(p \lVert q)=\displaystyle\sum_{i=1}^n p_i \log \displaystyle\frac{p_i}{q_i}$ 对 $(p, q)$ 是凸的。&lt;/p>
&lt;ul>
&lt;li>Spectrahedron&lt;/li>
&lt;/ul>
&lt;p>$$
\operatorname{conv}\left\{u u^{T} \in \mathbb{S}^{n} \mid u \in \mathbb{R}^{n},u^T u=1\right\} = \{M \in \mathbb{S}^n \mid \operatorname{tr} (M) = 1\}
$$&lt;/p>
&lt;h3 id="保持凸性的操作">保持凸性的操作&lt;/h3>
&lt;ul>
&lt;li>凸集的交、和、差、直积&lt;/li>
&lt;/ul>
&lt;p>如果 $C_1, C_2$ 是凸集，那么 $C_1 \bigcap C_2, \;C_1 + C_2, \; C_1 - C_2, \; C_1 \times C_2$ 是凸集。&lt;/p>
&lt;p>例：&lt;/p>
&lt;p>半正定矩阵锥是&lt;strong>无穷多个&lt;/strong>半平面的交：&lt;/p>
&lt;p>$$
\mathrm{S}^n_+=\bigcap_{z \neq 0}\left\{X \in \mathbf{S}^{n} \mid z^{T} X z \geq 0\right\}\\
$$&lt;/p>
&lt;p>所以它是凸的。&lt;strong>无穷多个凸集的交，仍然是凸集&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>由于 $f(X) = z^T Xz$ 是一个线性函数，所以 $\left\{X \in \mathbf{S}^{n} \mid z^{T} X z \geq 0\right\}$ 实质是 $\mathrm{S}^n_+$ 中的半平面。&lt;/p>
&lt;/blockquote>
&lt;p>要注意&lt;strong>两个凸集的并不是凸集&lt;/strong>。事实上，如果 $C_1, C_2$ 是凸集，那么成立：&lt;/p>
&lt;p>$$
\operatorname{conv}(C_1\cup C_2) = C_1 + C_2\\
$$&lt;/p>
&lt;ul>
&lt;li>仿射变换（affine transformation）&lt;/li>
&lt;/ul>
&lt;p>如果 $C$ 是一个凸集，$f$ 是一个有着形式 $f(x)=Ax+b$ 的仿射函数，那么 $f(C), f^{-1}(C)$ 都是凸集。&lt;/p>
&lt;p>比如椭球就可以看作是球的仿射。&lt;/p>
&lt;p>例：&lt;/p>
&lt;p>形如 $A(x)=x_{1} A_{1}+\cdots+x_{n} A_{n} \preceq B$ 的叫做线性矩阵不等式（linear matrix inequality)，其中 $A_i, B \in \mathrm{S}^m$，其解集 $\{x \mid A(x) \preceq B\}$ 是一个凸集，它是 $\mathrm{S}^n_{+}$ 在仿射函数 $f: \mathrm{R}^n \to \mathrm{S}^m_+,\;\; f(x)=B-A(x)$ 下的逆像。&lt;/p>
&lt;ul>
&lt;li>透视函数（perspective functions）&lt;/li>
&lt;/ul>
&lt;p>定义透视函数 $P: \mathrm{R}^{n+1} \to \mathrm{R}^n$，$\operatorname{dom} P=\mathrm{R}^n \times R_{++}$ 且 $P(z, t) = z/t$，那么，如果凸集 $C \subseteq \operatorname{dom} P$ ，那么 $P(C) = \{P(x) \mid x \in C\}$。&lt;/p>
&lt;p>一个凸集在透视函数的映射下仍然是凸的。透视函数的作用可以理解为是“针孔摄像头”！&lt;/p>
&lt;ul>
&lt;li>线性分数变换（Linear fractional transformation）&lt;/li>
&lt;/ul>
&lt;p>线性分数变换指的是：&lt;/p>
&lt;p>$$
f(x) = \displaystyle\frac{Ax +b}{c^T x + d}, \operatorname{dom} f = \{x \mid c^T x + d &amp;gt; 0\}\\
$$&lt;/p>
&lt;p>凸集，在线性分数变换下的像/原像，都是凸的。&lt;/p>
&lt;h2 id="广义不等式generalized-inequalities">广义不等式（generalized inequalities）&lt;/h2>
&lt;h3 id="正规锥proper-cones">正规锥（proper cones）&lt;/h3>
&lt;p>这是一个很重要的概念！如果一个锥是闭凸，并且内点集非空（solid），并且 pointed，就称它为 proper cone。&lt;/p>
&lt;blockquote>
&lt;p>pointed: $x \in K, -x \in K \Longrightarrow x = 0$; cone $K$ Contains no line&lt;/p>
&lt;p>Convex cone $K$ is pointed iff $K \cap (-K) = \{0\}$&lt;/p>
&lt;/blockquote>
&lt;p>乍一看这个概念很突兀。&lt;strong>proper cone 最重要的作用是能定义一个集合上的偏序关系（partial ordering）&lt;/strong>：&lt;/p>
&lt;p>$$
x \preceq_{K} y \Longleftrightarrow y-x \in K\;\; , \;x \prec_{K} y \Longleftrightarrow y-x \in \mathbf{int}K\\
$$&lt;/p>
&lt;p>如果取 $K=\mathrm{R}_+$，那么得到的序关系就是正常的实数比较。&lt;/p>
&lt;p>如果取 $K=\mathrm{R}^{n}_+$，那么向量 $\vec{x}\preceq\vec{y}$ 当且仅当每个分量 $x_i \leq y_i$。&lt;/p>
&lt;p>$\mathrm{S^{n}_+}$ 是 $\mathrm{S^n}$ 中的一个 $\mathrm{proper\; cone}$，$X\preceq_{K} Y$，当且仅当 $Y-X$ 是半正定的；$X\prec_{K} Y$，意味着 $Y-X$是正定的，所以很多时候我们直接简写 $X\succ 0$ 来声明$X$是正定矩阵；同时简写 $x \succ 0$ 声明向量 $x$ 的各个分量都大于0。&lt;/p>
&lt;p>这样的偏序关系有非常良好的性质：&lt;/p>
&lt;ul>
&lt;li>传递性：&lt;/li>
&lt;/ul>
&lt;p>$$
x \preceq_{K} y \;\text { and }\; y \preceq_{K} z \Longrightarrow x \preceq_{K} z\\
$$&lt;/p>
&lt;ul>
&lt;li>加性：&lt;/li>
&lt;/ul>
&lt;p>$$
x \preceq_{K} y \;\text { and }\; u \preceq_{K} v \Longrightarrow x+u \preceq_{K} y+v\\
$$&lt;/p>
&lt;p>以上的 $\preceq_K$ 均可换成 $\prec_K$ 。虽然《convex optimization》书上列了很多性质，但是大都不太重要（我认为）。&lt;/p>
&lt;p>更多 proper cone 的例子：&lt;/p>
&lt;ul>
&lt;li>单调非负锥（monotone nonnegative cone）：&lt;/li>
&lt;/ul>
&lt;p>$$
K_{m+} = \{x \in \mathrm{R}^n \mid x_1 \geq x_2 \geq \cdots \geq x_n \geq 0\}\\
$$&lt;/p>
&lt;ul>
&lt;li>多项式锥（cone of polynomials）：&lt;/li>
&lt;/ul>
&lt;p>$$
K=\left\{c \in \mathrm{R}^{n} \mid c_{1}+c_{2} t+\cdots+c_{n} t^{n-1} \geq 0 \;\; \text { for } t \in[0,1]\right\}\\
$$&lt;/p>
&lt;h3 id="最大和最小元素">最大和最小元素&lt;/h3>
&lt;p>有了序关系，自然就会考虑这样一个偏序集的最大/最小的元素（maximum/minimum）。&lt;/p>
&lt;p>显然，如果这样的元素存在，那么必然是唯一的。但实际上，并不是所有的元素都是可比较的（comparable），我们可以藉此定义“极小元”、“极大元”（minimal, maximal），意为，没有元素比它们来的更大/更小，容易知道它们不是唯一的。&lt;/p>
&lt;p>比如0是非负象限锥的最小元，但是不是任意两点都是能比较的。&lt;/p>
&lt;p>用 $x-K$ 表示，所有的可以与 $x$ 比较的、并且小于等于 $x$ 的元素全体，此时 $x$ 是 $S$ 中的极小元，当且仅当：$(x-K) \cap S=\{x\}$。&lt;/p>
&lt;p>类似地，用 $x+K$ 表示所有与 $x$ 可以比较的、并且大于等于 $x$ 的元素全体。&lt;/p>
&lt;p>$K=R^2_+$ 的情况如下图，左图的 $x_1$ 是最小点，而右图的 $x_2$ 是极小点&lt;/p>
&lt;p>&lt;img src="../../figures/Convex-Set/minimal.png" alt="">&lt;/p>
&lt;h2 id="分割和支撑超平面定理">分割和支撑超平面定理&lt;/h2>
&lt;h3 id="分割超平面separating-hyperplane">分割超平面（separating hyperplane）&lt;/h3>
&lt;p>考虑两个&lt;strong>不相交&lt;/strong>的凸集，是不是能找到一个超平面把它们分割开？如果其中一个变成凹集，是不是就做不到了？&lt;/p>
&lt;p>对于 $n$ 维空间中的一个超平面，我们习惯用$\left\{x | a^{T} x=b\right\}$来表示。一个超平面将全空间分割成两个半空间。&lt;/p>
&lt;p>超平面分割定理，说的就是两个不相交的凸集，一定存在 $a \neq 0$ 的超平面使得在其中一个凸集满足 $a^T x\ge b$，而另一个凸集中成立 $a^T x \leq b$。这个超平面 $\left\{x | a^{T} x=b\right\}$ 就叫做这两个凸集的分割超平面。&lt;/p>
&lt;p>进一步，如果不等关系&lt;strong>严格成立&lt;/strong>，就说这两个凸集被&lt;strong>严格分割&lt;/strong>（strict separation）。&lt;/p>
&lt;blockquote>
&lt;p>定义两个凸集的欧几里得距离是这两个集合中两个点距离的下确界：
$$
\operatorname{dist}(C, D)=\inf \left\{\|u-v\|_{2} | \;u \in C, v \in D\right\}\\
$$&lt;/p>
&lt;/blockquote>
&lt;p>乍一看，好像很多的凸集对都是可以被严格分割的。&lt;strong>但注意凸集不一定要是闭的&lt;/strong>。&lt;/p>
&lt;p>两个开的不相交的凸集很容易找到例子说明它们可能不能被严格分割。当两个不相交的凸集都是闭的，比如 $A=\{(x, y) \mid x y \geq 1, x, y&amp;gt;0\}$和 $B=\{(x, y) \mid x \leq 0\}$，也可能不能被严格分割。&lt;/p>
&lt;p>&lt;strong>严格分割只在某些特定的情况下&lt;/strong>，比如&lt;strong>一个闭凸集和这个凸集外的一点&lt;/strong>，这两个凸集是可以被严格分割的。如果这个凸集是开的，并且这个点选为凸集的边界点，那么就不能被严格分割。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>点与凸集的分离定理&lt;/strong>，说的正是闭凸集和集合外一点可以被严格分割；利用它，我们进一步才得到下面的&lt;strong>支持超平面定理&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>通过上一段的那个结论，我们可以得到，任意一个闭凸集是所有包含这个凸集的半空间的交。如果这些凸集可以是有限个，就说这个凸集是可以被有限生成的。（finitely generated）&lt;/p>
&lt;h3 id="支撑超平面supporting-hyperplane">支撑超平面（supporting hyperplane）&lt;/h3>
&lt;p>对凸集 $C$，如果有 $x_{0} \in \mathbf{b d} C=\mathbf{cl} C \backslash \mathbf{ int } C$，并且&lt;/p>
&lt;p>$$
a^{T} x \leq a^{T} x_{0}\;\;\; \text { for all } x \in C\\
$$&lt;/p>
&lt;p>那么超平面 $\left\{x | a^{T} x=a^{T} x_{0}\right\}$ 就叫做 $C$ 在 $x_0$ 这点的支撑超平面。凸集的任何个边界点都存在支撑超平面，这就是支撑超平面定理。&lt;/p>
&lt;p>&lt;img src="../../figures/Convex-Set/supporting.png" alt="">&lt;/p>
&lt;p>值得一提的是，支撑超平面定理的逆定理也成立。即如果一个内点集非空的闭集，在每一个边界点上都有一个支撑超平面，那么它是凸的。（虽然这个命题非常直觉，但是证起来可不容易）&lt;/p>
&lt;p>分离和支持超平面定理是优化理论非常基本的定理。&lt;/p>
&lt;h2 id="对偶锥dual-cones">对偶锥（dual cones）&lt;/h2>
&lt;p>设 $K$ 是个锥，定义 $K$ 的对偶锥为：&lt;/p>
&lt;p>$$
K^{\ast}=\left\{y \mid x^{T} y \geq 0 \;\; \text { for all } x \in K\right\}\\
$$&lt;/p>
&lt;p>顾名思义，$K^{\ast}$ 是一个锥，更有意思的是 $K^\ast$ 总是凸的，不论 $K$ 是不是凸的。&lt;/p>
&lt;p>例：&lt;/p>
&lt;ul>
&lt;li>线性空间的子空间 $V$ 的对偶锥是其正交补 $V^\perp$。&lt;/li>
&lt;li>二阶锥、非负象限锥和半正定锥的对偶锥是其本身，称其为&lt;strong>自对偶锥（self-dual）&lt;/strong>。&lt;/li>
&lt;li>polyhedron cone $\{x \mid Ax \succeq 0\}$ 的对偶锥是 $\{A^Ty \mid y \succeq 0\}$。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>在 $\mathrm{S}^n$ 上定义 Frobenius 范数，$\mathrm{S}^n_+$ 的自对偶性源自
$$
\operatorname{tr}(XY) \geq 0 \;\; \text{ for all} \; X \succeq 0 \Longleftrightarrow Y \succeq 0 \\
$$&lt;/p>
&lt;/blockquote>
&lt;p>$p$ 范数锥 $K = \{(x, t) \in \mathrm{R}^{n+1} \mid \| x\|_ p\leq t\}$ 的对偶是 $q$ 范数锥 $K^\ast = \{(u, v) \in \mathrm{R}^{n+1} \mid \| u \|_{\ast} \leq v\}$，且 $1/p + 1/q = 2$。&lt;/p>
&lt;p>从几何上看，$K^\ast$ 中任意一点与 $K$ 中所有点的夹角不超过90°。&lt;/p>
&lt;p>&lt;img src="../../figures/Convex-Set/dual-cone.png" alt="">&lt;/p>
&lt;p>对偶锥有以下性质：&lt;/p>
&lt;ul>
&lt;li>$K^\ast$ 闭且凸（closed and convex）&lt;/li>
&lt;li>$K_1 \subseteq K_2 \Longrightarrow K_2^\ast\subseteq K_1^\ast$&lt;/li>
&lt;li>如果 $K$ 有非空内点，那么 $K^\ast$ 不包含任何直线（pointed）&lt;/li>
&lt;li>$K^{\ast\ast}$ 是 $\operatorname{conv} K$ 的闭包，从而，如果 $K$ 是闭且凸的，那么 $K=K^{\ast\ast}$。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>pointed: if $x\in K, -x \in K$，then $x=0$&lt;/p>
&lt;/blockquote>
&lt;p>这些性质说明如果 $K$ 是一个 proper cone，那么 $K^\ast$ 也是一个 proper cone，并且 $K=K^{\ast\ast}$。&lt;/p>
&lt;h3 id="对偶广义不等式">对偶广义不等式&lt;/h3>
&lt;p>之前我们已经知道 proper cone $K$ 能诱导出一个偏序关系。现在$K^\ast$也是一个 proper cone，通过 $K^\ast$ 也能定义一个偏序关系了。&lt;/p>
&lt;p>此外，通过对偶性可以给出广义不等式的等价命题：&lt;/p>
&lt;ul>
&lt;li>$x \preceq_{K} y \Leftrightarrow \lambda^{T} x \leq \lambda^{T} y ,\; \; \forall \lambda \succeq_{K^{\ast}} 0$&lt;/li>
&lt;li>$x \prec_{K} y \Leftrightarrow \lambda^{T} x&amp;lt;\lambda^{T} y,\;\; \forall \lambda \succeq_{K^{\ast}} 0, \lambda \neq 0 .$&lt;/li>
&lt;/ul>
&lt;p>通过定义很容易验证上图的结论。&lt;/p>
&lt;p>上式的含义是，&lt;strong>对偶锥中的每个元素（向量）可以看成原来锥的一个合法的投影方向，原来锥上两个点在这些合法的投影方向上序不变！&lt;/strong> 在锥形式的对偶中就要用到这条性质。&lt;/p>
&lt;blockquote>
&lt;p>Since $K=K^{\ast\ast}$, we have $\lambda \preceq_{K^\ast} \mu$ if and only if $\lambda^{T} x \leq \mu^{T} x$ for all $x \succeq_K 0$&lt;/p>
&lt;/blockquote>
&lt;h3 id="对偶不等式与最小元极小元">对偶不等式与最小元/极小元&lt;/h3>
&lt;p>借助对偶性可以建立起最小元和极小元的相关理论。&lt;/p>
&lt;p>对于最小元，$x$ 是 $S$ 的最小元当且仅当，$\forall \lambda \succ_{K^\ast} 0$， $x$ 是问题 $\min_{z\in S} \;\lambda^T z$ 的最优解。&lt;/p>
&lt;p>对于极小元，没有充分必要的条件。其充分条件是：如果 $\exists \lambda \succ_{K^\ast} 0$ 并且 $x$ 是问题 $\min_{z\in S} \;\lambda^T z$ 的最优解，那么 $x$ 就是 $S$ 的极小元。（找到一个方向，在这个方向上的投影是最小的）。如果 $S$ 是凸集，这就是充分必要的。&lt;/p>
&lt;p>（完结撒花）&lt;/p>
&lt;hr></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/" term="凸优化" label="凸优化"/><category scheme="https://allenz-me.github.io/tags/%E5%87%B8%E9%9B%86/" term="凸集" label="凸集"/><category scheme="https://allenz-me.github.io/tags/%E9%94%A5/" term="锥" label="锥"/></entry><entry><title type="text">Convex Set — 续</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/cvxopt/convex-set-2/"/><id>https://allenz-me.github.io/posts/cvxopt/convex-set-2/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">这是 convex set 的第二部分 仿射无关 (affinely independent) 定义 $x_{1}, x_{2}, \ldots, x_{k}$ 仿射无关，当且仅当 $x_{2}-x_{1}, \ldots, x_{k}-x_{1}$ 线性无关。否则就称……</summary><content type="html">&lt;p>这是 convex set 的第二部分&lt;/p>
&lt;hr>
&lt;h2 id="仿射无关-affinely-independent">仿射无关 (affinely independent)&lt;/h2>
&lt;p>定义 $x_{1}, x_{2}, \ldots, x_{k}$ 仿射无关，当且仅当 $x_{2}-x_{1}, \ldots, x_{k}-x_{1}$ 线性无关。否则就称是仿射相关的。&lt;/p>
&lt;p>关于仿射更多的性质：&lt;/p>
&lt;ul>
&lt;li>$x_{0}+\operatorname{span}\left\{x_{1}-x_{0}, \ldots, x_{k}-x_{0}\right\}=\operatorname{aff}\left\{x_{0}, \ldots, x_{k}\right\}$&lt;/li>
&lt;li>$\left\{x_{0}, \ldots, x_{k}\right\}$ 包含 $0 \Rightarrow \operatorname{aff}\left\{x_{0}, \ldots, x_{k}\right\}=\operatorname{span}\left\{x_{0}, \ldots, x_{k}\right\}$&lt;/li>
&lt;li>线性无关的向量组也是仿射无关的&lt;/li>
&lt;li>如果 $x_1, x_2, \dots, x_k$ 仿射无关，那么 $0=\sum_{i=0}^{k} \alpha_{i} x_{i}, 0=\sum_{i=0}^{k} \alpha_{i}$ 有唯一解 $\alpha_{0}=\alpha_{1}=\cdots=\alpha_{k}=0$&lt;/li>
&lt;/ul>
&lt;h2 id="仿射变换-affine-transformation">仿射变换 (affine transformation)&lt;/h2>
&lt;p>仿射变换是指满足以下条件的函数：&lt;/p>
&lt;p>$$
F(\lambda x+(1-\lambda) y)=\lambda F(x)+(1-\lambda) F(y), \quad \lambda \in \mathrm{R}
$$&lt;/p>
&lt;p>一些性质：&lt;/p>
&lt;ul>
&lt;li>对于仿射变换 $F$， $T(x) = F(x) - F(0)$ 是线性变换。&lt;/li>
&lt;li>如果 $S$ 是仿射集，那么 $F(S)$ 也是仿射集，即仿射集在仿射变换的作用下也是仿射的。&lt;/li>
&lt;li>$F(\operatorname{aff} M)=\operatorname{aff}(F(M))$&lt;/li>
&lt;/ul>
&lt;h2 id="包算子-closure-operator">包算子 (Closure operator)&lt;/h2>
&lt;p>In mathematics, a closure operator on a set $S$ is a function $\mathrm{cl}: \mathcal{P}(S) \rightarrow \mathcal{P}(S)$ from the power set of $S$ to itself that satisfies the following conditions for all sets $X, Y \subseteq S$&lt;/p>
&lt;p>$$
\begin{array}{ll}
&amp;amp; X \subseteq \operatorname{cl}(X) \quad &amp;amp; \text{(cl is extensive)} \\
&amp;amp; X \subseteq Y \Rightarrow \operatorname{cl}(X) \subseteq \operatorname{cl}(Y) \quad &amp;amp; \text{(cl is monotone)} \\
&amp;amp; \operatorname{cl}(\operatorname{cl}(X))=\operatorname{cl}(X) \quad &amp;amp; \text {(cl is idempotent) }
\end{array}
$$&lt;/p>
&lt;p>Closure operators are also called &amp;quot;&lt;strong>hull operators&lt;/strong>&amp;quot;.&lt;/p>
&lt;p>闭包算子是集合到集合的映射，满足扩展性、单调性、幂等性。&lt;/p>
&lt;p>这是一个拓扑学的概念，最简单的例子，就是欧式空间中的闭包。&lt;/p>
&lt;p>一个拓扑空间可以很容易地引出一个闭包算子。&lt;/p>
&lt;p>容易验证，affine hull、convex hull、conic hull 是包算子。&lt;/p>
&lt;h2 id="topological-properties-of-convex-sets">Topological properties of convex sets&lt;/h2>
&lt;p>凸集有很好的拓扑性质。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>$C$ 是凸集 $\Longrightarrow \operatorname{int} C$ 和 $\operatorname{cl} C$ 都是凸的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$C$ 是紧的 $\Longrightarrow \operatorname{conv} C$ 也是紧的 （$\operatorname{conv} C$ 的任意点列都有收敛子列）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$C$ 有界 $\Longrightarrow \operatorname{cl} C$ 是紧集 $\Longrightarrow\operatorname{conv} (\operatorname{cl}C)$ 是紧集 $\Longrightarrow\operatorname{conv} C$ 有界&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>包算子 $\operatorname{conv}$ 保持紧性，但是不保持闭性（$x^2y=1$ 的函数图像的凸包是一个开集），除非集合有界。&lt;/p>
&lt;p>由此可定义闭凸包算子
$$
\overline{\operatorname{conv}} S := \bigcap_{\begin{array}{c} &amp;amp;S \subset U \\ &amp;amp;S \text{ convex,\,closed} \end{array} } U = \operatorname{cl} (\operatorname{conv} S)
$$&lt;/p>
&lt;p>对于非空集合 $S$&lt;/p>
&lt;ul>
&lt;li>$\overline{\operatorname{conv}} S=\operatorname{cl}(\operatorname{conv} S)$&lt;/li>
&lt;li>如果 $S$ 有界，那么 $\overline{\operatorname{conv}} S=\operatorname{conv}(\operatorname{cl} S)=\operatorname{cl}(\operatorname{conv} S)$&lt;/li>
&lt;/ul>
&lt;p>包算子 $\operatorname{cone}$ 不保持闭性也不保持紧性的，如 $\{(x, y) \in \mathrm{R}^2 \mid y \geq 1\}$，其锥包不是闭的；如 $\{(x, y) \in \mathrm{R}^2 \mid (x-1)^2 + y^2 \leq 1\}$，其锥包不是紧的。&lt;/p>
&lt;p>由此可以定义闭锥包算子：
$$
\overline{\operatorname{cone}} S:= \operatorname{cl} (\operatorname{cone} S)
$$
若 $S$ 是紧集且 $0 \notin \operatorname{conv} S$， 则锥包保持闭性，有 $\overline{\operatorname{cone}} S=\operatorname{cone} S$ ，即 $\operatorname{cone} S$ 是闭的。&lt;/p>
&lt;h2 id="relative-topology">Relative topology&lt;/h2>
&lt;p>如果一个凸集 $C$ 有内点，那么 $C$ 的仿射包一定是全空间。但是，比如说三维空间中的一张纸，它在 $\mathrm{R}^3$ 不存在内点，但是在二维空间中，却是有内点的。&lt;/p>
&lt;p>相对内点(相对拓扑)的概念就是为了处理这种问题的。相对内点的定义是：&lt;/p>
&lt;p>$$
\operatorname{ri} C=\{x \in C \mid B(x, r) \cap \text { aff } C \subseteq C \;\text { for some } r&amp;gt;0\}\\
$$&lt;/p>
&lt;p>&lt;strong>“相对”二字指的是相对凸集的仿射包&lt;/strong>。&lt;/p>
&lt;p>常见集合的相对内点：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">$C$&lt;/th>
&lt;th style="text-align:center">$\operatorname{aff} C$&lt;/th>
&lt;th style="text-align:center">$\operatorname{dim} C$&lt;/th>
&lt;th style="text-align:center">$\operatorname{ri} C$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">$\{x\}$&lt;/td>
&lt;td style="text-align:center">$\{x\}$&lt;/td>
&lt;td style="text-align:center">$0$&lt;/td>
&lt;td style="text-align:center">$\{x\}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">$[a, b]$&lt;/td>
&lt;td style="text-align:center">line generated by $a$ and $b$&lt;/td>
&lt;td style="text-align:center">$1$&lt;/td>
&lt;td style="text-align:center">$(a, b)$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">$B(x, r)$&lt;/td>
&lt;td style="text-align:center">$\mathrm{R}^n$&lt;/td>
&lt;td style="text-align:center">$n$&lt;/td>
&lt;td style="text-align:center">$\operatorname{int} B(x, r)$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>相对内点并不是一个单调的操作， $A \subseteq B$ 并不意味着 $\operatorname{ri} A \subseteq \operatorname{ri} B$ . 如 $\{0\}$ 和 $[0, 1]$ .&lt;/p>
&lt;p>由相对内点可以引出相对闭包：
$$
\operatorname{rbd} C = \operatorname{cl} C \backslash \operatorname{ri} C
$$
相对内点对凸集十分重要，因为&lt;strong>非空的凸集必然存在相对内点&lt;/strong>。闭凸集可以分解为相对内点集和相对边界集。&lt;/p>
&lt;p>对凸集而言&lt;/p>
&lt;ul>
&lt;li>$\operatorname{aff}(\mathrm{cl} C)=\operatorname{aff} C=\operatorname{aff}(\operatorname{ri} C)$&lt;/li>
&lt;li>$\mathrm{ri}(\mathrm{ri} C)=\operatorname{ri} C=\operatorname{ri}(\operatorname{cl} C)$&lt;/li>
&lt;li>$\mathrm{cl} C=\mathrm{cl}(\mathrm{ri} C)$&lt;/li>
&lt;li>$\operatorname{rbd} C=\operatorname{rbd}(\operatorname{ri} C)=\operatorname{rbd}(\operatorname{cl} C)$&lt;/li>
&lt;li>$C$ and $\bar{C}$ share the same closure / relative interior if and only if $\operatorname{ri}(C) \subseteq \bar{C} \subseteq \operatorname{cl}(C)$&lt;/li>
&lt;li>$F(\operatorname{ri} C) = \operatorname{ri} F(C)$ for any affine map $F$&lt;/li>
&lt;/ul>
&lt;p>引入相对内点的作用就是，可以把一个不满维的集合，通过改变拓扑，将它变成满维的。&lt;/p>
&lt;p>如果两个凸集 $C_1, C_2$ 满足 $\operatorname{ri} C_1 \cap \operatorname{ri} C_2 \neq \emptyset$，那么：&lt;/p>
&lt;p>$$
\begin{aligned}
\operatorname{ri}\left(C_{1} \cap C_{2}\right) &amp;amp;=\operatorname{ri} C_{1} \cap \operatorname{ri} C_{2} \\
\operatorname{cl}\left(C_{1} \cap C_{2}\right) &amp;amp;=\operatorname{cl} C_{1} \cap \operatorname{cl} C_{2} \\
\operatorname{ri}\left(C_{1}+C_{2}\right)&amp;amp;=\operatorname{ri} C_{1}+\operatorname{ri} C_{2} \\
\end{aligned}
$$&lt;/p>
&lt;p>下面是三个相对内点最重要的性质&lt;/p>
&lt;p>&lt;strong>Line segment principle&lt;/strong>&lt;/p>
&lt;p>If $C$ is a convex set, $x \in \operatorname{ri}(C)$ and $\bar{x} \in \operatorname{cl}(C)$, then all points on the line segment connecting $x$ and $\bar{x}$, except possibly $\bar{x}$, belong to $\operatorname{ri}(C)$ .&lt;/p>
&lt;p>&lt;strong>Nonemptiness of Relative Interior&lt;/strong>&lt;/p>
&lt;p>If $C$ is a nonempty convex set, then $\operatorname{ri} (C)$ is a nonempty convex set, and has the same affine hull as $C$ .&lt;/p>
&lt;p>&lt;strong>Prolongation Lemma / Stretching principle&lt;/strong>&lt;/p>
&lt;p>$x \in \operatorname{ri}(C)$ if and only if every line segment in $C$ having $x$ as one endpoint can be prolonged beyond $x$ without leaving $C$.&lt;/p>
&lt;p>i.e, if $C$ convex
$$
x \in \operatorname{ri} C \Leftrightarrow \forall z \in C, \exists \mu&amp;gt;1, \mu x+(1-\mu) z \in C .
$$&lt;/p>
&lt;h2 id="凸集的表示">凸集的表示&lt;/h2>
&lt;p>&lt;strong>Caratheodory’s theorem&lt;/strong>：&lt;/p>
&lt;p>$S$ 是 $\mathrm{R}^n$ 的子集，那么， $x \in \operatorname{conv} S$，当且仅当存在一个至多包含 $S$ 中 $n+1$ 个点的 $A \subset S$，使得 $x$ 是 $A$ 中点的凸组合。&lt;/p>
&lt;p>这个定理反过来是显然的，如果 $x$ 是 $A$ 中点的凸组合，那么必然 $x \in \operatorname{conv} S$。&lt;/p>
&lt;p>现在设 $x \in \operatorname{conv} S$，那么根据凸包的定义，不妨设存在 $k &amp;gt; n, \lambda _i &amp;gt; 0$，使得 $x = \sum_{i=0}^k \lambda_i x_i$， 现在 $x_i - x_0 \: (1 \leq i \leq k)$ 这超过 $n$ 个的向量族必然是线性相关的，这样就可以采用线性相关的定义来证明 $x$ 只需要用 $k-1$ 个点的凸组合来表示就行了。&lt;/p>
&lt;p>例：设 $S = \{(0, 0), (0, 1), (1, 1), (1,0)\}$，$S$ 表示的是单位正方形的四个点，$\operatorname{conv} S$ 就是一个单位正方形。在这个单位正方形里随便取一个点，总能找到由 $S$ 中三个点组成的三角形覆盖住这个点。&lt;/p>
&lt;p>关于 $\operatorname{coni} S$ 成立一个类似的定理，只不过点数从 $n+1$ 减少到了 $n$ 个。&lt;/p>
&lt;p>&lt;strong>W. Fenchel and L. Bunt&lt;/strong>：&lt;/p>
&lt;p>如果 $S \subset \mathrm{R}^n$ 是一个&lt;strong>连通&lt;/strong>的凸集，则 $x \in \operatorname{conv} S$ 可以被表示成 $S$ 中 $n$ 个点的凸组合；这个结论可以被放宽至 $S$ 有不超过 $n$ 个连通分量。&lt;/p>
&lt;p>&lt;strong>Minkowski and Weyl&lt;/strong>：polyhedron representation&lt;/p>
&lt;p>有界多面体可由其极点唯一决定。对于无界的多面体，可以由极点和方向来决定（extreme points and extreme rays）。&lt;/p>
&lt;p>这写下来就是：对任何 $P = \{x \mid Ax \leq b\}$，总能找到两个点集 $V, R$ ，使得：&lt;/p>
&lt;p>$$
P = \operatorname{conv}(V) + \operatorname{coni}(R)\\
$$&lt;/p>
&lt;p>也就是说，多面体可以表示成一个凸集和一个锥的 Minkowski sum。这个定理在一些教材中也叫做 resolution theorem.&lt;/p>
&lt;h2 id="投影算子">投影算子&lt;/h2>
&lt;p>对于集合 $C$ 和集合外一点 $x$，$x$ 到 $C$ 的投影被定义为 $\arg \min _{y\in C} \|y - x \|$&lt;/p>
&lt;p>对于闭子空间，我们有一个经典的投影分解：
$$
x = \mathrm{p}_V(x) + \mathrm{p}_{V^\perp}(x)
$$
一个点到闭凸集 $C$ 的投影是唯一的，由此可以定义投影算子：
$$
x \mapsto \mathrm{p}_{C}(x)
$$&lt;/p>
&lt;p>成立：
$$
\langle x - \mathrm{p}_C(x), y - \mathrm{p}_C(x)\rangle \leq 0, \quad \forall y \in C
$$&lt;/p>
&lt;p>$x_0$ 是 $x$ 到 $C$ 的投影点当且仅当 $\langle x - x_0, y - x_0\rangle \leq 0, \; \forall y \in C$&lt;/p>
&lt;p>投影算子是非扩张的 (nonexpansive)：
$$
\left\|\mathrm{p}_{C}\left(x_{1}\right)-\mathrm{p}_{C}\left(x_{2}\right)\right\| \leq \left\|x_{1}-x_{2}\right\|
$$&lt;/p>
&lt;p>但它并不是一个压缩映射 (contraction)，上面的不等号是可以取等的。&lt;/p>
&lt;h2 id="几类重要的锥">几类重要的锥&lt;/h2>
&lt;h3 id="polar-cone">Polar cone&lt;/h3>
&lt;p>&lt;strong>对于闭凸锥来说，非常有意义的一点是，它可以认为是介于 subspace 与 general convex set 之间的产物。&lt;/strong>&lt;/p>
&lt;p>锥 $K$ 的极锥定义为：
$$
K^{\circ}=\left\{s \in \mathbb{R}^{n}:\langle s, x\rangle \leqslant 0 \text { for all } x \in K\right\}
$$
如果 $K$ 是一个超平面，那么 $K^\circ = K^\perp$ . 实际上，&lt;strong>极锥就可以看成是正交补的推广&lt;/strong>，此时内积小于等于0可以看成是正交的推广。&lt;/p>
&lt;img src="../../figures/Convex-set-2/image-20220321163212889.png" alt="image-20220321163212889" style="zoom:50%;" />
&lt;p>如上图，几何上看，$K$ 与 $K^\circ$ 中的元素夹角大于等于$\displaystyle\frac{\pi}{2}$ .&lt;/p>
&lt;p>注意到极锥一定是闭凸锥；它的定义刚好是对偶锥反过来， $K^\circ = - K ^\ast$ .&lt;/p>
&lt;p>极运算是反序的，如果锥 $K_1 \subseteq K_2$，那么 $K_2^\circ \subseteq K_1^\circ$ .&lt;/p>
&lt;h4 id="polar-cone-theorem">Polar Cone Theorem&lt;/h4>
&lt;p>对任何非空的锥 $K$，有：
$$
K^{\circ \circ} = \overline{\operatorname{conv}} K
$$
特别的，如果 $K$ 是一个闭凸锥，那么：$K^{\circ \circ} = K$ . (Farkas Lemma)&lt;/p>
&lt;p>&lt;strong>这个定理一定程度上说明了“子空间、正交补”和“闭凸锥、极运算”是类似的。&lt;/strong>&lt;/p>
&lt;h4 id="闭凸锥的投影">闭凸锥的投影&lt;/h4>
&lt;p>根据投影定理很容易得到：
$$
\langle x - \mathrm{p}_K(x), \,\mathrm{p}_K(x) \rangle = 0 \,\text{ and }\, \langle x - \mathrm{p}_K(x), \, y\rangle \leq 0 \;\; \forall y \in K \Rightarrow x - \mathrm{p}_K(x) \in K^{\circ}
$$
这点跟投影到子空间是非常类似的。对闭凸锥还成立：
$$
\begin{array}{c}
\mathrm{p}_{K}(x)=0 \;\Longleftrightarrow \; x \in K^{\circ} \\
x \in K \; \Longleftrightarrow \; \langle s, x\rangle \leqslant 0 \text { for all } s \in K^{\circ} \\
\mathrm{p}_{K}(x)+\mathrm{p}_{K^{\circ}}(x)=x \\
\end{array}
$$
&lt;strong>(Moreau定理)&lt;/strong> 设 $K$ 是一个闭凸锥，则下面两个说法等价&lt;/p>
&lt;ol>
&lt;li>$x=x_{1}+x_{2}$ with $x_{1} \in K, x_{2} \in K^{\circ}$ and $\left\langle x_{1}, x_{2}\right\rangle=0$;&lt;/li>
&lt;li>$x_{1}=\mathrm{p}_{K}(x)$ and $x_{2}=\mathrm{p}_{K^{\circ}}(x)$.&lt;/li>
&lt;/ol>
&lt;h3 id="tangent-cone">Tangent cone&lt;/h3>
&lt;p>对于光滑的集合，在某一点我们可以用超平面来近似。但是，凸跟光滑没有必然的联系，在集合 $S$ 的一点 $\bar{x}$，我们用切锥来表示这种近似 (first-order approximation of sets)：
$$
\mathrm{T}_{S}(\bar{x})=\left\{d \mid \exists S \supseteq\left\{x_{k}\right\} \rightarrow \bar{x},\left\{t_{k}&amp;gt;0\right\} \rightarrow 0, \; \lim _{k \rightarrow \infty} \frac{x_{k}-\bar{x}}{t_{k}}=d\right\} = \overline{\operatorname{cone}}(S-x)
$$
&lt;strong>切锥一定是闭的，但不一定是凸的&lt;/strong>，如 $K=\left\{(x, y) \in \mathbb{R}^{2} \mid x, y \geq 0, x y=0\right\}, \mathrm{T}_{K}(0)=K$.&lt;/p>
&lt;p>切锥作为一阶近似，与光滑集合边界上的超平面近似的相容的。不难发现，单位圆上一点的切锥就是它这点的切线。切锥可以看成是若干个半平面的交。&lt;/p>
&lt;p>令 $S$ 是一个适当的光滑曲面
$$
S:=\left\{x \in \mathbb{R}^{n} \mid c_{i}(x)=0 \text { for } i=1, \ldots, m\right\}
$$
则 $\mathrm{T}_S(x) = \left\{d \in \mathbb{R}^{n}\mid\left\langle\nabla c_{i}(x), d\right\rangle=0 \text { for } i=1, \ldots, m\right\}$&lt;/p>
&lt;p>另一个例子是：
$$
S:= \{x \in \mathbb{R}^n \mid c(x) \leq 0\}
$$
如果 $c(\bar{x}) = 0$ 并且 $\nabla c(\bar{x}) \neq 0$，则 $\mathrm{T}_S(\bar{x}) = \{d \in \mathbb{R}^n \mid \langle c(\bar{x}), d\rangle \leq 0\}$&lt;/p>
&lt;p>切锥是一个一般性的概念，闭凸集的切锥有更简洁的表示。一般地，如果 $C$ 是一个闭凸集，那么 $C \subseteq \{x\} + \mathrm{T}_C(x)$ 且成立 $\mathrm{T}_C(x) = \overline{\operatorname{cone}} (C-x)$，自然 $\mathrm{T}_C(x)$ 是一个闭凸锥。如果 $x \in \operatorname{ri} C$，那么 $\mathrm{T}_C(x)$ 是子空间。&lt;/p>
&lt;h3 id="normal-cone">Normal cone&lt;/h3>
&lt;p>集合 $C$ 在 $x$ 处的法锥，定义为切锥的极锥：&lt;/p>
&lt;p>$$
\mathrm{N}_C(x) = (\mathrm{T}_C(x))^\circ
$$&lt;/p>
&lt;p>如果 $C$ 是一个凸集，那么有更简洁的表示：&lt;/p>
&lt;p>$$
\mathrm{N}_{C}(x)=\{v \mid \langle v, y - x\rangle \leq 0, \, \forall y \in C\}
$$&lt;/p>
&lt;p>如果 $K$ 是一个闭凸锥，那么
$$
\mathrm{N}_{K}(\bar{x})=\{\bar{x}\}^{\perp} \cap K^{\circ}=\left\{v \in K^{\circ} \mid\langle v, \bar{x}\rangle=0\right\}
$$
特别地 $\mathrm{N}_K(0) = K^\circ$ .&lt;/p>
&lt;p>令 $C_1, C_2$ 是非空的闭凸集，则：
$$
\begin{gathered}
\mathrm{T}_{C_{1}+C_{2}}\left(x_{1}+x_{2}\right)=\mathrm{cl}\left[\mathrm{T}_{C_{1}}\left(x_{1}\right)+\mathrm{T}_{C_{2}}\left(x_{2}\right)\right] \text{ and }
\mathrm{N}_{C_{1}+C_{2}}\left(x_{1}+x_{2}\right)=\mathrm{N}_{C_{1}}\left(x_{1}\right) \cap \mathrm{N}_{C_{2}}\left(x_{2}\right) \\
\mathrm{T}_{C_{1} \cap C_{2}}(x) \subset \mathrm{T}_{C_{1}}(x) \cap \mathrm{T}_{C_{2}}(x) \text { and } \mathrm{N}_{C_{1} \cap C_{2}}(x) \supset \mathrm{N}_{C_{1}}(x)+\mathrm{N}_{C_{2}}(x)
\end{gathered}
$$
取 $x \in C, s \in \mathbb{R}^n$，以下结论是等价的：&lt;/p>
&lt;ul>
&lt;li>$s \in \mathrm{N}_C(x)$&lt;/li>
&lt;li>$x = \mathrm{p}_C(x+s)$&lt;/li>
&lt;/ul>
&lt;p>即 $x + \mathrm{N}_C(x) = \mathrm{p}_C^{-1}(x)$ .&lt;/p>
&lt;h4 id="fermats-rule-in-a-context-of-constrained-minimization">Fermat’s rule in a context of constrained minimization&lt;/h4>
&lt;p>如果定义在 $C$ 上的可微的函数 $f$ 在 $x$ 处取得局部极小，那么：&lt;/p>
&lt;ul>
&lt;li>$\langle \nabla f(x), d \rangle \geq 0 \;\; \forall d \in \mathrm{T}_C(x)$&lt;/li>
&lt;li>$-\nabla f(x) \in \mathrm{N}_C(x)$&lt;/li>
&lt;li>若 $C$ 是凸集，那么 $\langle \nabla f(y), y -x \rangle \geq 0 \;\; \forall y \in C $&lt;/li>
&lt;/ul>
&lt;p>切锥一定程度反映了可行方向。如果 $x$ 是内点，那么 $\mathrm{N}_C(x) = \{0\}$，这就是经典的 Fermat 引理了。&lt;/p>
&lt;h3 id="horizon-cone">Horizon cone&lt;/h3>
&lt;p>$S$ 的地平锥是：&lt;/p>
&lt;p>$$
S^{\infty}=\left\{v \in \mathbb{E} \mid \exists\left\{x_{k}\right\} \subseteq S,\left\{t_{k}&amp;gt;0\right\} \rightarrow 0, t_{k} x_{k} \rightarrow v\right\}
$$&lt;/p>
&lt;p>地平锥和回收锥类似，但不同的一点是，任意一个集合的地平锥一定是闭的，但是回收锥不一定。&lt;/p>
&lt;p>$S$ 有界当且仅当 $S^\infty = \{0\}$ .&lt;/p>
&lt;p>如果 $C$ 是凸集，那么 $\operatorname{rec}( \operatorname{cl} C) = C^\infty$ . 这说明对闭凸集而言，地平锥和回收锥是一个概念。&lt;/p>
&lt;h4 id="closedness-under-linear-transformations">Closedness Under Linear Transformations&lt;/h4>
&lt;p>线性变换不一定把闭集映射成闭集，如线性算子 $T: (x, y) \to x$ 把闭集 $\{(x, y) \mid xy \geq 1\}$ 映射成 $\mathrm{R}\backslash \{0\}$ 。利用地平锥，我们能得到一个充分条件。&lt;/p>
&lt;p>给定线性算子 $T$ 和闭集 $C$，若 $\ker T \cap C^{\infty} = \{0\}$，则 $T(C)$ 是闭集。特别地，如果 $T$ 是单射或者 $C$ 有界，结论也成立。&lt;/p>
&lt;h3 id="recession-cone">Recession cone&lt;/h3>
&lt;p>对一个任意的集合 $C$，它的 &lt;strong>recession cone&lt;/strong> 被定义为：&lt;/p>
&lt;p>$$
\operatorname{rec}(C)=\{y \mid x + ty \in C, \; \forall x \in C, \; \forall t \geq 0\}
$$&lt;/p>
&lt;p>$C$ 的回收锥可以理解为是 $C$ 内所有可以无限延伸的方向生成的锥。凸集 $C$ 有界当且仅当 $\operatorname{rec}(C) = \{0\}$ .&lt;/p>
&lt;p>&lt;strong>凸集的回收锥是一个凸锥&lt;/strong>。闭凸集的回收锥是闭凸锥。特别地，多面体 $\{x \mid Ax = b\}$ 的 recession cone 是 $\{y \mid Ay=0\}$。&lt;/p>
&lt;p>&lt;strong>Lineality Space&lt;/strong>&lt;/p>
&lt;p>集合 $C$ 的 Lineality Space 定义为：
$$
\operatorname{lin}(C) = \operatorname{rec}(C) \cap (-\operatorname{rec}(C))
$$
$d \in \operatorname{lin}(C)$ 当且仅当 $\forall x \in C$，整条直线 $\{d \mid x + \alpha d, \alpha \geq 0\}$ 都包含在 $C$ 中。&lt;/p>
&lt;p>&lt;strong>Decomposition of a Convex Set&lt;/strong>&lt;/p>
&lt;p>设 $C$ 是一个非空凸集，则对于任意包含在 $\operatorname{lin}(C)$ 中的子空间 $S$，成立分解式：
$$
C = S + (C \cap S^{\perp})
$$&lt;/p>
&lt;h3 id="barrier-cone">Barrier cone&lt;/h3>
&lt;p>对一个任意的集合 $C$，它的 &lt;strong>barrier cone&lt;/strong> 被定义为集合 $\{y \mid |y^T x| &amp;lt; +\infty, \; \forall x \in C\}$。即非零向量 $y$ 是包含 $C$ 的半平面 $\{x \mid y^T x \leq \alpha\}$ 的法向。不论集合 $C$ 是什么，它的 barrier cone 也是一个凸锥！&lt;/p>
&lt;p>如果 $C$ 是一个非空的闭凸集，那么 $C$ 的 recession cone 是它的 barrier cone 的对偶锥。&lt;/p>
&lt;h2 id="extreme-points">Extreme points&lt;/h2>
&lt;p>称 $x$ 是凸集 $C$ 的极点，如果不存在两点 $x_1, x_2\;(x_1 \neq x_2)$，使得
$$
x = \frac{1}{2}(x_1 + x_2)
$$&lt;/p>
&lt;ul>
&lt;li>闭区间 $[a, b]$ 的极点是 $a$ 和 $b$&lt;/li>
&lt;li>单位球的任意一点都是极点&lt;/li>
&lt;li>半平面、仿射集没有极点&lt;/li>
&lt;/ul>
&lt;p>极点的等价定义：&lt;/p>
&lt;ul>
&lt;li>$x$ 是 $C$ 的一个 face&lt;/li>
&lt;li>$C\backslash \{x\}$ 是凸的&lt;/li>
&lt;/ul>
&lt;p>一般用 $\operatorname{ext} C$ 来表示 $C$ 的极点集。注意算子 $\operatorname{ext}$ 不满足单调性。&lt;/p>
&lt;p>凸紧集必然存在极点。令 $\bar{x}$ 是连续函数 $x \to \| x \|^2$ 在 $C$ 上的最小值点，容易验证 $\bar{x}$ 是极点。&lt;/p>
&lt;p>如果一个集合不包含一条直线，那么它必然包含一个极点。&lt;/p>
&lt;p>如果 $F$ 是 $C$ 的一个 face，那么 $\operatorname{ext} F \subseteq \operatorname{ext} C$ .&lt;/p>
&lt;p>&lt;strong>H. Minkowski 定理&lt;/strong> ：对凸紧集 $C \subset \mathrm{R}^n$ 满足：$C = \operatorname{conv} (\operatorname{ext} C)$&lt;/p>
&lt;p>Converse Minkowski's Theorem：假设 $C$ 是非空凸紧集, 集合 $D \subseteq C$ 满足 $\overline{\operatorname{conv}} D=C$，则有 $\operatorname{ext} C \subset \operatorname{cl} D$&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Krein–Milman theorem&lt;/strong> — A compact convex subset of a Hausdorff locally convex topological vector space is equal to the closed convex hull of its extreme points.&lt;/p>
&lt;p>这是 Minkowski 定理在无穷维的推广。&lt;/p>
&lt;/blockquote>
&lt;h3 id="face">Face&lt;/h3>
&lt;p>Let $C$ be a convex set. A face of $C$ is a subset $F$ of $C$ such that&lt;/p>
&lt;ol>
&lt;li>$F$ is convex, and&lt;/li>
&lt;li>given any line segment $L \subseteq C$, if $\operatorname{ri}(L) \cap F \neq \emptyset$, then $L \subseteq F$.&lt;/li>
&lt;/ol>
&lt;p>Faces that are nonempty and proper subsets of $C$ are called &lt;em>proper faces&lt;/em>.&lt;/p>
&lt;ul>
&lt;li>The intersection of two faces of $C$ is a face of $C$&lt;/li>
&lt;li>A face of a face of $C$ is a face of $C$&lt;/li>
&lt;li>Any proper face of $C$ lies on its relative boundary $\operatorname{rbd}(C)$&lt;/li>
&lt;/ul>
&lt;h3 id="exposed-face">Exposed Face&lt;/h3>
&lt;p>A set $F \subseteq C$ is an exposed face of $C$ if there is a supporting hyperplane $H$ of $C$ such that $F = C\cap H$&lt;/p>
&lt;p>&lt;em>An exposed face is a face.&lt;/em>&lt;/p>
&lt;img src="https://i.stack.imgur.com/Jak91.png" alt="A special face" style="zoom:99%;" />
&lt;p>For polyhedra, all the faces are exposed.&lt;/p>
&lt;h2 id="hyperplane-seperation">Hyperplane seperation&lt;/h2>
&lt;p>引入了相对拓扑之后，超平面分离定理还能有进一步的细化。&lt;/p>
&lt;p>对于超平面 $H = \{x\mid a^T x = b\}$，凸集的分离根据其程度可分为以下三种：&lt;/p>
&lt;ul>
&lt;li>weak seperation&lt;/li>
&lt;/ul>
&lt;p>$$
\sup_{x \in C_{1}} a^{T} x \leq \inf_{x \in C_{2}} a^{T} x
$$&lt;/p>
&lt;ul>
&lt;li>proper seperation&lt;/li>
&lt;/ul>
&lt;p>$$
\sup _{x_{1} \in C_{1}} a^{T} x_{1} \leq \inf _{x_{2} \in C_{2}} a^{T} x_{2} \, ,
\quad \inf _{x_{1} \in C_{1}} a^{T} x_{1}&amp;lt;\sup _{x_{2} \in C_{2}} a^{T} x_{2}
$$&lt;/p>
&lt;ul>
&lt;li>strict seperation&lt;/li>
&lt;/ul>
&lt;p>$$
\sup_{x \in C_{1}} a^{T} x &amp;lt; \inf_{x \in C_{2}} a^{T} x
$$&lt;/p>
&lt;p>恰当分离指的是在弱分离的基础上，并且超平面不同时包含 $C_1$ 和 $C_2$ 。&lt;/p>
&lt;img src="../../figures/Convex-set-2/image-20220404141849352.png" alt="" style="zoom:44%;" />
&lt;p>弱分离存在很平凡的情况，比如上图(c)。&lt;/p>
&lt;h3 id="强分离">强分离&lt;/h3>
&lt;p>&lt;strong>闭凸集与集合外一点的强分离性&lt;/strong> 是投影定理的直接推论，分离超平面的法向直接就是投影方向。&lt;/p>
&lt;p>两个不相交的闭凸集可以被强分离，当且仅当它们的回收锥不相交。&lt;/p>
&lt;blockquote>
&lt;p>$\operatorname{rec}(C_1) \cap \operatorname{rec}(C_2) = \emptyset \Rightarrow C_1 - C_2$ is closed. 对 $0$ 和 $C_1 - C_2$ 用点与闭凸集的强分离定理。&lt;/p>
&lt;/blockquote>
&lt;h3 id="恰当分离">恰当分离&lt;/h3>
&lt;p>&lt;strong>两个凸集可以被恰当分离当且仅当它们的相对内点集不相交。&lt;/strong>&lt;/p>
&lt;h2 id="近似的凸集表示定理">近似的凸集表示定理&lt;/h2>
&lt;blockquote>
&lt;p>HDP&lt;/p>
&lt;/blockquote>
&lt;h2 id="对偶范数的进一步理解">对偶范数的进一步理解&lt;/h2>
&lt;p>当我们在研究 $X = \mathrm{R}^n$ 的时候（此时没有引入范数），我们会发现 $\mathrm{R}^n$ 的对偶空间 $X^\ast$ 和 $\mathrm{R}^n$ 是线性同构的（只需要纯代数的证明），如果我们在 $\mathrm{R}^n$ 中引入范数，那么它的对偶空间的元素，也会有随之定义的算子范数，&lt;strong>这个对偶空间里的算子范数就是对偶范数&lt;/strong>！&lt;/p>
&lt;p>在 $\mathrm{R}^n$ 空间中，范数 $\| \cdot \|$ 的对偶范数被定义为：&lt;/p>
&lt;p>$$
\|z\|_{\ast}=\sup \left\{z^{T} x \mid\|x\| \leq 1\right\} \\
$$&lt;/p>
&lt;p>我们知道 $(\mathrm{R}^n, \|\cdot \|)$ 是一个 Banach space，该空间上每一个元素 $z$ 都能诱导出一个 linear functional：&lt;/p>
&lt;p>$$
f_z(x) = z^T x \\
$$&lt;/p>
&lt;p>这个 linear functional $f_z$ 的算子范数就是 $z$ 的对偶范数！所以，&lt;strong>$(\mathrm{R}^n, \| \cdot \|)$ 的对偶空间是 $(\mathrm{R}^n, \|\cdot \|_\ast)$&lt;/strong>。&lt;/p>
&lt;blockquote>
&lt;p>$\mathrm{R}^n$ 上的所有线性泛函都具有形式：$f(x) = a^T x$.&lt;/p>
&lt;p>见：https://math.stackexchange.com/questions/3377554/show-that-any-linear-function-f-mathbbrn-to-mathbbr-is-of-the-form-fx&lt;/p>
&lt;/blockquote>
&lt;p>根据对偶范数的定义可以得到不等式：&lt;/p>
&lt;p>$$
\|x \| \cdot \|z\|_\ast \leq |x^T z| \\
$$&lt;/p>
&lt;p>在矩阵空间 $\mathrm{R}^{m\times n}$ 上，类似的，有限维线性空间与其对偶空间都是线性同构的，假如我们在 $\mathrm{R}^{m\times n}$ 引入谱范数（spectrum norm），即矩阵最大的奇异值，类似可以在矩阵空间上构造线性泛函：&lt;/p>
&lt;p>$$
f_Z (X) = \operatorname{tr}(Z^T X) \\
$$&lt;/p>
&lt;p>这个由矩阵 $Z$ 诱导出的线性泛函，它的算子范数：&lt;/p>
&lt;p>$$
\|Z\|_{2 \ast}=\sup \left\{\operatorname{tr}\left(Z^{\top} X\right) \mid\|X\|_{2} \leq 1\right\} = \sum_i\sigma_i(Z) \\
$$&lt;/p>
&lt;p>就是核范数（nuclear norm），即矩阵所有奇异值的和。&lt;/p>
&lt;h2 id="支撑函数">支撑函数&lt;/h2>
&lt;p>一个集合 $C$ 的支撑函数（support function）定义为：&lt;/p>
&lt;p>$$
S_C(y) = \sup\, \{y^Tx \mid x \in C\}
$$&lt;/p>
&lt;p>容易看到，支撑函数是一个齐次函数（homogeneous），并且不论集合 $C$ 是什么，$S_C$ 都是凸函数！&lt;/p>
&lt;p>例子：&lt;/p>
&lt;ul>
&lt;li>单点集 $\{a_0\}$ 的支撑函数是 $S(y) = a_0^T y$&lt;/li>
&lt;li>半径为 1 的单位球的支撑函数是 $S(y) = \| y\|_2$&lt;/li>
&lt;li>$\mathcal{U} = \{x \mid \| x - \mu \| \leq 1\}$ 的支撑函数是 $S_{\mathcal{U}} (y) = y^T \mu + \| y\|_\ast$ （这个在鲁棒优化里面有应用）&lt;/li>
&lt;/ul>
&lt;p>下面这个定理说明，一个闭凸集被它的支撑函数完全刻画：&lt;/p>
&lt;p>&lt;strong>如果 $C$ 和 $D$ 是闭凸集，那么 $C = D$ 当且仅当 $S_C = S_D$&lt;/strong>。&lt;/p>
&lt;p>假如 $S_C = S_D$，先证明 $D \subseteq C$；假如说存在一点 $x_0 \in D, x_0 \notin C$，因为 $C$ 是闭集，而闭集和闭集外一点是可以找到超平面严格分割的，于是，存在 $a \neq 0$，使得 $a^T x_0 &amp;gt; b$，且 $a^T x &amp;lt; b, \,\forall x \in C$，这意味着：&lt;/p>
&lt;p>$$
\sup _{x \in C} a^{T} x \leq b&amp;lt;a^{T} x_{0} \leq \sup _{x \in D} a^{T} x \\
$$&lt;/p>
&lt;p>与 $S_C = S_D$ 矛盾，这就证明了 $D \subseteq C$，于是 $D=C$。&lt;/p>
&lt;h3 id="支撑函数的几何意义">支撑函数的几何意义&lt;/h3>
&lt;img src="../../figures/Convex-Set/convex-optimization-theory-support.png" alt="support function" style="zoom:67%;" />
&lt;p>对于每一个 $a \in \mathrm{R}^n$ ，借助支撑函数都能找到包含 $C$ 的一个支撑超平面：&lt;/p>
&lt;p>$$
H_a = \{ y \mid y^T a \leq S_C(a)\} \\
$$&lt;/p>
&lt;p>这个超平面以 $a$ 为法向量。&lt;/p>
&lt;p>在上图中 $y$ 引出的支撑超平面就是虚线标的那条。&lt;/p>
&lt;p>&lt;strong>利用支撑函数可以证明，一个闭凸集等于它所有的支撑半平面的交集。&lt;/strong>&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/" term="凸优化" label="凸优化"/></entry><entry><title type="text">在 Jupyter 中安装 R 和 Julia</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/jupyter-r-julia/"/><id>https://allenz-me.github.io/posts/coding/jupyter-r-julia/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2022-01-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">R 安装如下包 1 2 3 4 install.packages(c(&amp;#39;rzmq&amp;#39;,&amp;#39;repr&amp;#39;,&amp;#39;IRkernel&amp;#39;,&amp;#39;IRdisplay&amp;#39;), repos = c(&amp;#39;http://irkernel.github.io/&amp;#39;, getOption(&amp;#39;repos&amp;#39;))) IRkernel::installspec() 参见：https://irkernel.github……</summary><content type="html">&lt;p>&lt;strong>R&lt;/strong>&lt;/p>
&lt;p>安装如下包&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="nf">install.packages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;rzmq&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;repr&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;IRkernel&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#39;IRdisplay&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">repos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;http://irkernel.github.io/&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nf">getOption&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#39;repos&amp;#39;&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;span class="n">IRkernel&lt;/span>&lt;span class="o">::&lt;/span>&lt;span class="nf">installspec&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>参见：&lt;a href="https://irkernel.github.io/installation/">https://irkernel.github.io/installation/&lt;/a>&lt;/p>
&lt;p>&lt;strong>Julia&lt;/strong>&lt;/p>
&lt;p>首先确保 &lt;code>jupyter&lt;/code> 命令在当前环境变量下。&lt;/p>
&lt;p>安装1.5以上版本后，在julia终端中，依次执行：&lt;code>import Pkg&lt;/code>、&lt;code>Pkg.add(&amp;quot;IJulia&amp;quot;)&lt;/code>、&lt;code>Pkg.build(&amp;quot;IJulia&amp;quot;)&lt;/code>。&lt;/p>
&lt;p>如果遇到联网或者下载速度的问题，考虑设置国内的镜像源，依次执行：&lt;code>Pkg.add(&amp;quot;JuliaZH&amp;quot;)&lt;/code>、&lt;code>using JuliaZH&lt;/code>、&lt;code>JuliaZH.set_mirror(&amp;quot;BFSU&amp;quot;)&lt;/code>。&lt;/p>
&lt;p>执行 &lt;code>versioninfo()&lt;/code> 命令，如果观察到 &lt;code>JULIA_PKG_SERVER&lt;/code> 的地址是BFSU的镜像源，就代表配置成功了。&lt;/p>
&lt;p>Julia 的常用包有：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">Plots
DataFrames &lt;span class="c1"># 数据分析&lt;/span>
JuMP
Flux &lt;span class="c1"># 机器学习&lt;/span>
Distributions
LaTeXStrings
PyCall
PyPlot
Zygote
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h4 id="jupyter">Jupyter&lt;/h4>
&lt;p>查看Jupyter当前支持的kernel：&lt;code>jupyter kernelspec list&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">Available kernels:
julia-1.0 /home/user/.local/share/jupyter/kernels/julia-1.0
julia-1.1 /home/user/.local/share/jupyter/kernels/julia-1.1
python3 /home/user/anaconda3/share/jupyter/kernels/python3
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>接着执行 &lt;code>jupyter kernelspec uninstall julia-1.0&lt;/code> 即可删去不用的kernel。&lt;/p>
&lt;p>参考：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://stackoverflow.com/questions/44914176/how-to-remove-previous-version-from-jupyter/45211705">https://stackoverflow.com/questions/44914176/how-to-remove-previous-version-from-jupyter/45211705&lt;/a>&lt;/li>
&lt;/ul></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/></entry><entry><title type="text">Markov Chain</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/lecture4-cont/"/><id>https://allenz-me.github.io/posts/analysis/lecture4-cont/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-12-21T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">离散时间的 Markov 链 记 $\mathcal{F}_n = \sigma(X_1, X_2, \dots, X_n)$，定义满足如下条件的随机过程 $\{X_n, \, n\geq 1\}$ 为离散时间的……</summary><content type="html">&lt;h2 id="离散时间的-markov-链">离散时间的 Markov 链&lt;/h2>
&lt;p>记 $\mathcal{F}_n = \sigma(X_1, X_2, \dots, X_n)$，定义满足如下条件的随机过程 $\{X_n, \, n\geq 1\}$ 为离散时间的 Markov 链：&lt;/p>
&lt;p>$$
P(X_{n+1} \in B \mid \mathcal{F}_n) = P(X_{n+1} \in B \mid \sigma(X_n)) \quad a.s.
$$&lt;/p>
&lt;p>更为通俗的定义是：
$$
P(X_{n+1}=j \mid X_{n}=i, \cdots, X_{1}=i_{1}, X_{0}=i_{0})=P(X_{n+1}=j \mid X_{n}=i)
$$
即 $n+1$ 时刻的状态分布只跟 $n$ 时刻的状态有关，而与 $n-1$ 时刻之前的状态无关。&lt;/p>
&lt;p>一般都会假定状态转移的概率与时间无关，如果 Markov 链只有可列个（countable）状态，就可以用 $p_{ij}$ 来表示从 $i$ 转移到 $j$ 的概率。对于有限个状态，我们能得到一个 Markov 矩阵。&lt;/p>
&lt;h3 id="chapman-kolmogorov-方程">Chapman-Kolmogorov 方程&lt;/h3>
&lt;p>$$
p_{i j}^{(n+m)}=\sum_{l \in \mathcal{S}} p_{i l}^{(n)} \times p_{l j}^{(m)}, \forall n, m \in \mathbb{Z}_{+}
$$&lt;/p>
&lt;h3 id="状态类">状态类&lt;/h3>
&lt;p>如果对某个 $n\geq0$，有 $p_{ij}^n &amp;gt; 0$，则说 $j$ 是可从 $i$ 到达的（accessible），可相互到达的两个状态称为互通的（communicate）。&lt;/p>
&lt;p>&lt;strong>互通是一个等价关系&lt;/strong>，由此，状态集 $\mathcal{S}$ 可以划分为几个等价类（商集）。&lt;/p>
&lt;p>如果 Markov 链只有一个类，就说它是&lt;strong>不可约&lt;/strong>的（irreducible）。&lt;/p>
&lt;h4 id="周期性">周期性&lt;/h4>
&lt;p>状态 $i$ 称为具有周期 $d$，若只要 $n$ 不能被 $d$ 整除时就有 $P^n_{ii}=0$，且 $d$ 是具有此性质的最大整数。&lt;/p>
&lt;p>周期为1的状态称为非周期的（aperiodic）。&lt;/p>
&lt;p>例如：对于一维的 random walk，所有的类都是互通的，并且有周期为 2.&lt;/p>
&lt;p>周期性也是等价类的一个性质！&lt;/p>
&lt;h4 id="常返性">常返性&lt;/h4>
&lt;p>状态 $i$ 是常返的（recurrent state），如果从 $i$ 出发的过程以概率 1 的会返回 $i$， 否则就说它是暂态的（transient state）。&lt;/p>
&lt;blockquote>
&lt;p>如果 $p_{ii}=1$，则 $i$ 是吸收态（absorbing state），这是一种特殊的 recurrent state. 这个类只有一个元素&lt;/p>
&lt;/blockquote>
&lt;p>记 $f_{ij}^n$ 为开始处在 $i$ 而转移到 $j$ 在时刻 $n$ 首次发生的概率，$f_{ij} = \sum_{n=1}^{\infty} f_{ij}^n$ 是从 $i$ 迟早转移到 $j$ 的概率。从而，$i$ 是常返当且仅当 $f_{ii}=1$.&lt;/p>
&lt;p>状态 $j$ 是常返的，当且仅当：
$$
\sum_{n=1}^\infty P^n_{jj} = \infty
$$
这意味着 $X_0 = j$ 下，访问 $j$ 的次数的期望是 $\infty$.&lt;/p>
&lt;p>另一方面，假设 $j$ 是暂态的，则返回次数是以均值为 $1/(1-f_{jj})$ 为均值的几何随机变量。&lt;/p>
&lt;p>依旧考虑一维的随机徘徊：$P_{i, i+1} = 1 - P_{i, i-1}= p$.&lt;/p>
&lt;p>显然所有的状态都互通，所以要么都是暂态，要么都是常返，所以只考虑状态 $0$.&lt;/p>
&lt;p>对此，有：
$$
P_{00}^{2n+1} = 0, \quad P_{00}^{2n} = \frac{(2n)!}{n!n!} p^n(1-p)^n
$$
欲判断 $\sum_{n=1}^{\infty} P_{00}^{2n}$ 这个级数的收敛性&lt;/p>
&lt;p>...&lt;/p>
&lt;p>进一步，定义 $\mu_{jj} = \sum_{n=1}^\infty n f_{ii}^n$ ，易知如果 $j$ 是暂态，那么 $\mu_{jj} = \infty$.&lt;/p>
&lt;p>现在，如果 $j$ 是常返且 $\mu_{jj} &amp;lt; \infty$ 就说 $j$ 是正常返（positive recurrent），否则就称为零常返（null recurrent）。&lt;/p>
&lt;p>暂态、正常返和零常返都是等价类的性质。&lt;/p>
&lt;h3 id="极限定理">极限定理&lt;/h3>
&lt;p>正常返的非周期状态称为遍历的（ergodic）。&lt;/p>
&lt;p>一个不可约的非周期的 Markov 链必属于下列两类之一：&lt;/p>
&lt;ol>
&lt;li>一切状态或都是暂态，或都是零常返。此时 $\displaystyle\lim_{n \to \infty} P_{ij}^n = 0$，且不存在平稳分布。&lt;/li>
&lt;li>一切状态都是正常返，且 $\pi_j = \displaystyle \lim_{n \to \infty} P_{ij}^n &amp;gt; 0$，是唯一的平稳分布。&lt;/li>
&lt;/ol>
&lt;p>对于第2类我们称这样的 Markov 链为遍历链。&lt;/p>
&lt;p>注意到第1类必然存在无穷多个状态，比如1维随机徘徊就是一个典型的例子。&lt;/p>
&lt;h3 id="类之间的转移">类之间的转移&lt;/h3>
&lt;p>常返类是一个封闭的类，即，如果 $R$ 是一个常返类，且 $i \in R, j \notin R$，则 $P_{ij} = 0$.&lt;/p>
&lt;p>根据定义，这个命题是好理解的。&lt;strong>在一个 Markov 链，只可能出现暂态转移到常返，不可能出现常返转移到暂态。&lt;/strong>&lt;/p>
&lt;h3 id="分支过程">分支过程&lt;/h3>
&lt;h2 id="连续时间的-markov-链">连续时间的 Markov 链&lt;/h2>
&lt;p>定义在 $(\Omega, \mathcal{F}, \mathbb{P}, (\mathcal{F_t})_{t\in T})$ 的连续时间随机过程 $X_t$ 是 Markov 过程，如果：&lt;/p>
&lt;p>$$
P(X_{t+s} \in B \mid \mathcal{F}_t) = P(X_{t+s} \in B \mid \sigma(X_t)) \quad a.s.
$$&lt;/p>
&lt;p>更为通俗的形式是：
$$
{P}(X(t+s)=j \mid X(s)=i, X(u)=x(u), 0 \leqslant u&amp;lt;s)={P}(X(t+s)=j \mid X(s)=i)
$$
在专门研究连续时间 Markov 链的时候，依然会假定至多只有可数多个状态，&lt;/p>
&lt;blockquote>
&lt;p>布朗运动是一种连续时间的 Markov 链，但是状态数量有不可数个。&lt;/p>
&lt;/blockquote>
&lt;h3 id="转移速率">转移速率&lt;/h3>
&lt;p>以 $T_i$ 记一个 Markov 链进入状态 $i$ 且在离开之前所经历的时间，根据 Markov 性质，易知：
$$
{P}\left(T_{i}&amp;gt;s+t \mid T_{i}&amp;gt;s\right)={P}\left(T_{i}&amp;gt;t\right)
$$
这说明 $T_i$ 具有无记忆性，必须服从指数分布。&lt;/p>
&lt;p>即，连续时间 Markov 链在每个状态 $i$ 停留的时间服从参数（速率）为 $v_i$ 的指数分布。如果 $v_i = 0$，则这个状态是吸收的。$v_i$ 越大，则在状态 $i$ 停留的平均时间越短。如果 $v_i = \infty$，那么 $i$ 是一个瞬时状态。&lt;/p>
&lt;p>称一个连续时间的 Markov 链是正则的（regular），如果它在任意有限长时间段内转移次数以概率1的有限。&lt;/p>
&lt;p>例：$P_{i, i+1} = 1, v_i = i^2$ 就是一个非正则的 Markov 链。&lt;/p>
&lt;p>对 $i\neq j$，定义 $q_{ij} = v_i P_{ij}$ 为从 $i$ 转移到 $j$ 的速率。&lt;/p>
&lt;h3 id="生灭过程">生灭过程&lt;/h3>
&lt;p>用一个连续时间的 Markov 过程来表示一个群体的数量，如果状态 $i \geq 0$ 只能转移到状态 $i+1$ 或者 $i-1$，这就是一个生灭过程（birth and death process）。&lt;/p>
&lt;p>令：
$$
\lambda_i = q_{i, i+1}, \quad \mu_i = q_{i, i-1}
$$
值 $\lambda_i, \mu_i$ 分别成为出生率、死亡率。易知：
$$
v_i = \lambda_i + \mu_i, \quad P_{i, i+1} = \frac{\lambda_i}{\lambda_i + \mu_i} = 1 - P_{i, i-1}
$$
如果 $\mu_i = 0, \forall i$ 就说这是一个纯生过程（pure birth），Poisson 过程是最简单的纯生过程，它有常数的出生率 $\lambda_i = \lambda$.&lt;/p>
&lt;p>如果出生率正比于种群数量，即 $\lambda_i = i \lambda$，这种纯生过程叫做 Yule 过程。&lt;/p>
&lt;h3 id="kolmogorov-微分方程">Kolmogorov 微分方程&lt;/h3>
&lt;p>记：
$$
P_{ij}(t) = P(X(t+s) = j \mid X(s) = i)
$$
为处于状态 $i$ 并在一个时间 $t$ 后处在状态 $j$ 的概率。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/></entry><entry><title type="text">泊松过程及其模拟</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/possion/"/><id>https://allenz-me.github.io/posts/analysis/possion/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-12-20T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Poisson 过程的定义 Poisson 过程是一个计数过程，$N(t)$ 表示到时刻 $t$ 为止发生事件的数量。参数为……</summary><content type="html">&lt;h2 id="poisson-过程的定义">Poisson 过程的定义&lt;/h2>
&lt;!-- 两个等价的定义，揭示了Poisson过程的主要性质 -->
&lt;p>Poisson 过程是一个计数过程，$N(t)$ 表示到时刻 $t$ 为止发生事件的数量。参数为 $\lambda$ 的 Poisson 过程的定义是：&lt;/p>
&lt;p>定义一：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>$N(0) = 0$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$N(t)$ 具有独立增量性，即 $N(t+s) - N(t)$ 与 $N(t)$ 独立&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$N(t)$ 具有平稳增量性，即 $N(t_2 + s) - N(t_1 + s)$ 与 $N(t_2) - N(t_1)$ 具有相同分布。且 $N(t+s) - N(s)$ 服从参数为 $\lambda t$ 的 Poisson 分布，即
$$
P(N(t+s) - N(s) = n) = e^{-\lambda t} \frac{(\lambda t)^n}{n !}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>定义二：&lt;/p>
&lt;ul>
&lt;li>$N(0)=0$&lt;/li>
&lt;li>$N(t)$ 有平稳增量和独立增量&lt;/li>
&lt;li>$P(N(h)=1) = \lambda h + o(h)$&lt;/li>
&lt;li>$P(N(h) \geq 2) = o(h)$&lt;/li>
&lt;/ul>
&lt;p>定义二暗含 Poisson 过程在极短的事件内至多发生一次，&lt;/p>
&lt;p>此外，Poisson 过程也是一种特殊的 Renewal Process。令 $S_n = \displaystyle\sum_{i=1}^n X_i$，其中 $X_i, \mathrm{i.i.d}$ 服从参数为 $\lambda$ 的指数分布，则其对应的计数过程 $N(t) = \sup\, \{n : S_n \leq t\}$ 就是 Poisson 过程。&lt;/p>
&lt;h2 id="到达时间间隔与等待时间的分布">到达时间间隔与等待时间的分布&lt;/h2>
&lt;p>记 $T_1$ 为第一个事件的发生时间，则 $P(T_1 &amp;gt; t) = P(N(t) = 0) = e^{-\lambda t}$.&lt;/p>
&lt;p>记 $T_2$ 为第一个事件发生到第二个事件发生的时间间隔，那么：&lt;/p>
&lt;p>$$
P(T_2 &amp;gt; t + s \mid T_1 = s) = P(N(t+s) - N(s) = 0) = P(N(t) = 0) = e^{-\lambda t}
$$&lt;/p>
&lt;p>所以 $T_2$ 与 $T_1$ 独立，且它们都服从参数为 $\lambda$ 的指数分布。&lt;/p>
&lt;p>另一个有趣的事情是 $S_n = \displaystyle\sum_{i=1}^n T_i \sim \Gamma(n, \lambda)$&lt;/p>
&lt;p>注意到&lt;/p>
&lt;p>$$
P\left(S_{n} \leqslant t\right)=P(N(t) \geqslant n)=\sum_{j=n}^{\infty} \mathrm{e}^{-\lambda t} \frac{(\lambda t)^{j}}{j !}
$$&lt;/p>
&lt;p>求导有：&lt;/p>
&lt;p>$$
f_{S_n}(t)=-\sum_{j=n}^{\infty} \lambda \mathrm{e}^{-\lambda t} \frac{(\lambda t)^{j}}{j !}+\sum_{j=n}^{\infty} \lambda \mathrm{e}^{-\lambda t} \frac{(\lambda t)^{j-1}}{(j-1) !}=\lambda \mathrm{e}^{-\lambda t} \frac{(\lambda t)^{n-1}}{(n-1) !}
$$&lt;/p>
&lt;p>小结：Poisson 分布事件的到达时间间隔服从参数为 $\lambda$ 的指数分布，第 $n$ 个事件到达的时刻 $S_n$ 服从 $\Gamma(n, \lambda)$。独立增量性和平稳增量性暗含时间间隔的无记忆性！&lt;/p>
&lt;h2 id="到达时间的分布">到达时间的分布&lt;/h2>
&lt;p>给定 $N(t)=n$，则 $n$ 个事件的到达时间 $S_1, S_2, \dots, S_n$ 与 $n$ 个独立的 $(0, t)$ 上均匀分布随机变量的次序统计量有相同的分布。&lt;/p>
&lt;h2 id="poisson-过程的分解">Poisson 过程的分解&lt;/h2>
&lt;p>如果事件分为两类，I型或者II型，在 $s$ 时刻的概率分别为 $p(s), 1-p(s)$，那么泊松随机变量 $N(t)$ 可分解为 $N(t) = N_1(t) + N_2(t)$，其中 $N_1(t)$ 具有均值 $\lambda t \cdot \frac{1}{t}\int_0^t p(s)\mathrm{ds}$.&lt;/p>
&lt;p>进一步，如果 $p(s)=p$ 是常数，那么 $N_1(t), N_2(t)$ 是参数为 $\lambda t p, \lambda t(1-p)$ 的泊松过程，且二者是独立的。&lt;/p>
&lt;!-- 【infinite server queue】 -->
&lt;h2 id="非时齐-poisson-过程">非时齐 Poisson 过程&lt;/h2>
&lt;p>非时齐的泊松过程允许到达速率是 $t$ 的函数，把平稳增量这个条件放松了。&lt;/p>
&lt;ul>
&lt;li>$N(0)=0$,&lt;/li>
&lt;li>$\{N(t), t \geqslant 0\}$ 有独立增量&lt;/li>
&lt;li>$P(N(t+h)-N(t) \geqslant 2)=o(h)$&lt;/li>
&lt;li>$P(N(t+h)-N(t)=1)=\lambda(t) h+o(h)$&lt;/li>
&lt;/ul>
&lt;p>令：&lt;/p>
&lt;p>$$
m(t)=\int_{0}^{t} \lambda(s) \mathrm{d} s
$$&lt;/p>
&lt;p>则 $N(t+s) - N(t)$ 服从均值为 $m(t+s) - m(t)$ 的泊松分布。&lt;/p>
&lt;p>对于非时齐泊松分布，其到达时间间隔不一定是独立同分布的。&lt;/p>
&lt;p>注意到：&lt;/p>
&lt;p>$$
P(T_1 &amp;gt; t) = P(N(t) = 0) = e^{-m(t)}
$$&lt;/p>
&lt;!-- 【补全】 -->
&lt;p>$$
f_{S_{n}}(t)=\lambda(t) e^{-m(t) \frac{[m(t)]^{n-1}}{(n-1)!}}
$$&lt;/p>
&lt;h2 id="poisson-分布的正态近似">Poisson 分布的正态近似&lt;/h2>
&lt;p>当 $\lambda \to \infty$ 时，$\operatorname{Possion}(\lambda) \to_d \mathcal{N}(\lambda, \lambda)$&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>Poisson 分布：$P(X=k) = \displaystyle e^{-\lambda} \frac{\lambda^k }{k!}$&lt;/p>
&lt;p>Gamma 分布 $\Gamma (\alpha, \lambda)$：$f(x)=\displaystyle \frac{\lambda^{\alpha} x^{\alpha-1}}{\Gamma (\alpha)}e^{-\lambda x} .\quad (\alpha &amp;gt; 0)$&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="指数分布">指数分布&lt;/h2>
&lt;p>指数分布是一种连续型的分布，参数为 $\lambda$ 的指数分布的概率密度函数为&lt;/p>
&lt;p>$$
f(x) = \lambda e^{-\lambda x} \quad (x \geq 0)
$$&lt;/p>
&lt;p>其累计分布函数为：&lt;/p>
&lt;p>$$
F(x) = 1 - e^{-\lambda x} \quad (x \geq 0) \qquad
\bar{F}(x) = e^{-\lambda x} \quad (x \geq 0)
$$&lt;/p>
&lt;p>期望 $\mathbb{E}[X] = \displaystyle\frac{1}{\lambda}$，方差 $\operatorname{Var}(X) = \displaystyle\frac{1}{\lambda^2}$.&lt;/p>
&lt;p>定义一个连续型&lt;strong>非负&lt;/strong>随机变量的失效率函数为 $r(t) = \displaystyle\frac{f(t)}{1-F(t)} \simeq \lim_{\mathrm{d}t \to 0} {P}(X \in(t, t+\mathrm{d} t) \mid X&amp;gt;t)$. 指数函数的失效率函数为常数 $\lambda$.&lt;/p>
&lt;p>失效率 $r(t)$ 可以解释为，生命分布为 $F$ 的个体活到 $t$ 岁且恰好在此年龄死亡的概率。&lt;/p>
&lt;p>&lt;strong>失效率函数可以唯一决定原分布&lt;/strong>：&lt;/p>
&lt;p>$$
F(t)=1-\exp \left\{-\int_{0}^{t} r(x) \mathrm{d} x\right\}
$$&lt;/p>
&lt;p>指数分布的几点性质：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>指数分布具有无记忆性，且是唯一具有无记忆性的连续型分布。
$$
\mathbb{P}\{X&amp;gt;s+t \mid X&amp;gt;t\}=\mathbb{P}\{X&amp;gt;s\} \quad \forall s, t \geq 0
$$&lt;/p>
&lt;p>或者：
$$
\mathbb{P}\{X&amp;gt;s+t\}=\mathbb{P}\{X&amp;gt;s\} \mathbb{P}\{X&amp;gt;t\}\quad \forall s, t \geq 0
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$n$ 个独立的参数为 $\lambda$ 的指数分布的和服从 Gamma 分布 $\Gamma (n, \lambda)$，其密度函数为：
$$
f(x) = \frac{\lambda^\alpha x^{1-\alpha}}{\Gamma(\alpha)} e^{-\lambda x}
$$
当 $n$ 是整数的时候，也可以叫做 Erlang 分布。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>指数分布、Erlang 分布经常用于建模排队中的 service time. 除了此之外，还有&lt;/p>
&lt;p>hyper-exponential 密度函数形式为 $f_{X}(x)=\sum_{i=1}^{n} f_{Y_{i}}(x) p_{i}$，其中 $Y_i \sim \operatorname{\lambda_i}, \;p_i$ 是一组离散概率分布。它的 coefficient of variation 比指数分布大。&lt;/p>
&lt;p>hypo-exponential 是若干个独立的指数分布之和，但不必是同分布的。如果 $X_1 \sim \operatorname{\lambda_1}, X_2 \sim \operatorname{\lambda_2}, \lambda_1 \neq \lambda_2$，那么：
$$
X_1 + X_2 \sim \frac{\lambda_1 \lambda_2}{\lambda_1 - \lambda_2} \left( e^{-\lambda_2 x } - e^{-\lambda_1 x} \right)
$$
Coxian 是不定个独立的指数分布之和。$Y=\sum_{j=1}^{N} X_{j}$&lt;/p>
&lt;p>等&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>
&lt;p>$X_i$ 服从参数为 $\lambda_i$ 的指数分布，则 $\min_i X_i$ 服从参数为 $\sum_{i} \lambda_i$ 的指数分布。
$$
P(\min_i X_i \geq t) = P(X_1 \geq t, \dots, X_n \geq t) = \bar{F}^n(t) = e^{- \sum_i \lambda_i t}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果 $X_1, \, X_2$ 分别是参数为 $\lambda_1, \lambda_2$ 的指数分布，那么 $P(X_1 &amp;lt; X_2) = \displaystyle \frac{\lambda_1}{\lambda_1 + \lambda_2}$. （使用全概率公式）&lt;/p>
&lt;p>推论：${P}\left(X_{i}=\displaystyle\min_{j} X_{j}\right)=P\left(X_i &amp;lt; \displaystyle\min_{j\neq i} X_j\right)=\displaystyle\frac{\lambda_{i}}{\sum_{j=1}^{n} \lambda_{j}}$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="泊松过程的模拟">泊松过程的模拟&lt;/h2>
&lt;h3 id="齐次泊松过程homogeneous-poisson-process">齐次泊松过程（Homogeneous Poisson Process）&lt;/h3>
&lt;p>参数为 $\lambda$ 的泊松过程 $\{N(t), \; t \geq 0\}$ 是取整数值的连续时间随机过程。$N(t)$ 表示到时刻 $t$ 为止发生的事件个数，两次事件到达的时间间隔服从指数分布 $\operatorname{Exp}(\lambda)$。在 $T$ 时刻发生的事件数服从参数为 $\lambda T$ 的泊松分布。泊松过程在极短的时间内只会发生一次事件。&lt;/p>
&lt;p>模拟泊松过程有两种方法，给定发生次数确定发生时间，或者 给定时间求发生次数和分布。&lt;/p>
&lt;p>方法一：&lt;/p>
&lt;p>给定泊松过程的事件次数，我们可以用 $n$ 个指数分布 $\operatorname{Exp}(\lambda)$ 的随机数 $X_i, i = 1, \dots, n$ 来得到 $n$ 个事件的发生时间 $S_{k}=\sum_{i=1}^{k} X_{i}, \quad k=1,2, \cdots, n$。&lt;/p>
&lt;p>模拟并画出一条轨道的 Python 代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">lam&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.5&lt;/span> &lt;span class="c1"># 参数为1.5的泊松过程&lt;/span>
&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="c1"># 事件次数为10&lt;/span>
&lt;span class="c1"># 10 个参数为 lam 的指数分布随机数&lt;/span>
&lt;span class="n">r&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exponential&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">lam&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># 10个事件的发生时刻&lt;/span>
&lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hstack&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cumsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;span class="c1"># 画出&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">plot&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;r&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xlim&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ceil&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">())])&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ylim&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>一次运行的结果如下图：&lt;/p>
&lt;img src="../../figures/Possion/image-20211130171135097.png" style="zoom:67%;" />
&lt;p>方法二：&lt;/p>
&lt;p>上面这个方法是指定发生的次数求时间，这个方法则是，给定时间 $T$，我们知道 $[0, T]$ 内事件发生的次数是一个参数为 $\lambda T$ 的随机变量，设这个随机变量是 $N$；现在已知时间 $T$ 内事件发生了 $N$ 次，根据泊松过程的性质，我们可以生成 $N$ 个独立的 $(0,1)$ 上均匀分布随机数 $U_1, U_2, \dots, U_N$，从小到大排序为 $U_{(1)}, U_{(2)}, \dots, U_{(N)}$，则 $\left(T U_{(1)}, T U_{(2)}, \ldots, T U_{(N)}\right)$即为时刻 $T$ 之前的所有事件的到来时间。&lt;/p>
&lt;p>模拟并画出一条轨道的 Python 代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">lam&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.5&lt;/span> &lt;span class="c1"># 参数为1.5的泊松过程&lt;/span>
&lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="c1"># 时间到T&lt;/span>
&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">poisson&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lam&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># T时间内发生的次数&lt;/span>
&lt;span class="c1"># 生成n个[0, T]均匀分布随机数并排序&lt;/span>
&lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hstack&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">plot&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;r&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">plot&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;r&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xlim&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">])&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ylim&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>一次运行的结果如下图：&lt;/p>
&lt;img src="../../figures/Possion/image-20211130171108714.png" style="zoom:67%;" />
&lt;h3 id="非齐次泊松过程nonhomogeneous-poisson-process">非齐次泊松过程（Nonhomogeneous Poisson Process）&lt;/h3>
&lt;p>为了生成强度为 $\lambda(t),\: t \geq 0$ 的非齐次的泊松过程到时刻 $T$ 为止的状态，如果 $\lambda (t) $ 满足 $\lambda(t) \leq M, \: \forall t \in [0, T]$，则可以使用拒绝采样法，按照生成参数为 $M$ 的齐次泊松过程的方法去生成各个事件到来时刻，但是以 $\displaystyle\frac{\lambda (t)}{M}$ 的概率实际记录各个时刻。&lt;/p>
&lt;p>模拟并画出一条轨道的 Python 代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">lam&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">lambda&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="c1"># 非齐次泊松过程的强度函数&lt;/span>
&lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;span class="n">M&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">poisson&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">M&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">r&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="c1"># 以 lam(t)/M 的概率保留每个时刻&lt;/span>
&lt;span class="n">t&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">filter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">M&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">lam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">r&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">plot&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;r&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xlim&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">])&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ylim&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>一次运行的结果如下图：&lt;/p>
&lt;img src="../../figures/Possion/image-20211130171055830.png" style="zoom:67%;" />
&lt;h3 id="复合泊松过程compound-poisson-process">复合泊松过程（Compound Poisson Process）&lt;/h3>
&lt;p>复合泊松过程是一个复合的随机过程：
$$
W=\sum_{i=1}^{N(t)} Y_i
$$
当 $Y=1, a.s.$ 时，这个复合泊松过程就变成了普通的泊松过程。复合泊松过程每次“跳”的距离不再是常数 1，而是一个随机变量。这意味着我们可以用类似于模拟泊松过程的方法，来模拟复合泊松过程。所以，也有两种方法，以下仅展示方法一。&lt;/p>
&lt;p>模拟并画出一条轨道的 Python 代码，以下令 $Y \sim \mathcal{N}(0, 1)$。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">lam&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="c1"># 参数为1的泊松过程&lt;/span>
&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="c1"># 事件次数为10&lt;/span>
&lt;span class="c1"># 10 个参数为 lam 的指数分布随机数&lt;/span>
&lt;span class="n">r&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exponential&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">lam&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># 每次跳的幅度服从 N(0, 1)&lt;/span>
&lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># 10个事件的发生时刻&lt;/span>
&lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hstack&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cumsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;span class="c1"># 画出&lt;/span>
&lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">plot&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;r&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">j&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xlim&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ceil&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">())])&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>一次运行的结果如下图：&lt;/p>
&lt;img src="../../figures/Possion/image-20211130171016681.png" style="zoom: 67%;" /></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/><category scheme="https://allenz-me.github.io/tags/%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83/" term="泊松分布" label="泊松分布"/></entry><entry><title type="text">Farkas 引理、线性规划的对偶</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/farkas/"/><id>https://allenz-me.github.io/posts/operations/farkas/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-12-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Farkas 引理 Farkas 引理在对偶理论中有重要的应用，并且还被用来证明KKT条件。这个引理代数形式可……</summary><content type="html">&lt;h2 id="farkas-引理">Farkas 引理&lt;/h2>
&lt;p>Farkas 引理在对偶理论中有重要的应用，并且还被用来证明KKT条件。这个引理代数形式可以有多种，但是它的几何意义是恒定而简明的。&lt;/p>
&lt;p>Farkas 引理的内容是，给定矩阵 $A$ 和向量 $b$，以下两组式子有且仅有一个是可行的：&lt;/p>
&lt;ol>
&lt;li>$Ax = b, x \geq 0$&lt;/li>
&lt;li>$A^T y \geq 0, b^T y &amp;lt; 0$&lt;/li>
&lt;/ol>
&lt;p>一个矩阵 $A$ 的所有列向量可以张成一个锥 $K = \{Ax \mid x \geq 0\}$，另一个向量 $b$，与这个锥的位置关系只有两种：在锥内，不然就在锥外。所以 $b$ 在锥内和 $b$ 在锥外时成立的代数关系，恰好只有一个也必然有一个是成立的，同时另一个就不成立了。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>如果 $b \in K$ ，那么可以找到一组非负的系数，使得 $b$ 能被 $A$ 的列向量线性表示，即存在 $x \geq 0, Ax=b$。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果 $b \notin K$ ，那么根据点与凸集的分离定理，$b$ 和 $A$ 的列向量张成的锥，可以被一个（过原点的）超平面分离，设这个超平面的法向量为 $y$，那么可以有：$A^T y \geq 0, b^T y &amp;lt; 0$。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这个定理也可以通过替代定理的方式去证明。考虑一个线性规划问题和它的对偶问题：&lt;/p>
&lt;p>$$
(\mathcal{P})\quad
\begin{array}{ll}
\operatorname{minimize} &amp;amp; \quad 0 \\
\text {subject to } &amp;amp; A x =b, x \geq 0
\end{array} \qquad
(\mathcal{D})\quad
\begin{array}{ll}
\operatorname { maximize } \; -b^T y \\
\text {subject to } \; A^{T} y \geq 0
\end{array}
$$&lt;/p>
&lt;ul>
&lt;li>
&lt;p>如果(1)可行，那么两个LP的最优值都是0，那么对于任意满足 $A^T y \geq 0$ 的 $y$ 都成立 $-b^T y \leq 0$，即(2)不可行。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果(1)不可行，那么(P)问题的最优值是 $+\infty$，所以(D)问题最大值无上界，所以存在 $y$ 使得 $-b^T y &amp;gt; 0, A^Ty \geq 0$，即(2)可行。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>除此以外，Farkas引理还有纯代数的证明：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>如果 $\exists\, x \geq 0$，使得 $Ax=b$，那么如果 $A^T y \geq 0$，就有 $b^T y=x^T A^T y \geq 0$ ；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果存在 $y$ 使得 $A^T y\geq 0$ 并且 $b^T y &amp;lt; 0$，那么如果 $b \geq 0$，$x^T A^T y \geq 0$，从而 $Ax \neq b$ 。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>如果我们从锥的角度去看的话，Farkas 引理实质上说明了 $K = K^{\circ\circ}$&lt;/p>
&lt;p>A finite generated cone equals its bipolar cone
$$
b \in K \Leftrightarrow b^T y \leq 0 \, \text{ for all } y \in K^\circ = \{y \mid A^T y \leq 0\} \Leftrightarrow b \in K^{\circ\circ}
$$&lt;/p>
&lt;/blockquote>
&lt;h2 id="kkt-定理">KKT 定理&lt;/h2>
&lt;p>对于一般的非线性规划：&lt;/p>
&lt;p>$$
\begin{aligned}
\min \; &amp;amp; f(\boldsymbol{x}) \\
\text {s.t. } &amp;amp; g_{i}(\boldsymbol{x}) \leq 0, \quad i=1, \ldots, m \\
&amp;amp; h_{i}(\boldsymbol{x})=0, \quad i=1, \ldots, n
\end{aligned}
$$&lt;/p>
&lt;p>$\boldsymbol{x}^\ast \in \mathcal{X}$ 是局部极小点 (local minimum)，并满足一定的 constraint qualification，则存在 $\lambda_1, \dots, \lambda_m \geq 0$ 和 $\mu_1, \dots, \mu_n$ 使得：
$$
\begin{aligned}
\nabla f\left(\boldsymbol{x}_{\ast}\right)+\sum_{i=1}^{m} \lambda_{i} \nabla g_{i}\left(\boldsymbol{x}_{\ast}\right)+\sum_{i=1}^{n} \mu_{i} \nabla h_{i}\left(\boldsymbol{x}_{\ast}\right)&amp;amp;=\mathbf{0} \\
\lambda_{i} g_{i}\left(\boldsymbol{x}_{\ast}\right)&amp;amp;=0, \;\;i=1, \ldots, m
\end{aligned}
$$
&lt;strong>以上定理给出了局部最优的必要条件，没有凸性的要求。&lt;/strong>&lt;/p>
&lt;p>$\boldsymbol{x}^\ast$ 处的 Linearized cone 是：
$$
L_{\mathcal{X}} (\boldsymbol{x}^\ast) = \left\{ d \,\middle\vert\,
\begin{aligned}
\nabla g_i(\boldsymbol{x}^\ast)^T d \leq 0 \\
\nabla h_i(\boldsymbol{x}^\ast)^T d = 0
\end{aligned}
\right\}
$$&lt;/p>
&lt;p>在满足 CQ 条件的情况下，$L_{\mathcal{X}}(\boldsymbol{x}^\ast) = T_{\mathcal{X}}(\boldsymbol{x}^\ast)$ 即可行方向锥恰好与切锥相同，由 $\boldsymbol{x}^\ast$ 是局部极小得到：
$$
\nabla f (\boldsymbol{x}^\ast) ^T d \geq 0, \quad \forall d \in T_{\mathcal{X}}(\boldsymbol{x}^\ast)
$$
记：
$$
A=\left(\begin{array}{c}
-\nabla g_{i}(\bar{x})^{T} \\
-\nabla h_{j}(\bar{x})^{T} \\
\nabla h_{j}(\bar{x})^{T}
\end{array}\right) \in \mathrm{R}^{(|I|+2 p) \times n}
$$
则有：
$$
\nabla f (\boldsymbol{x}^\ast) ^T d \geq 0, Ad \geq 0
$$
这说明：
$$
\exists\left(\begin{array}{l}
\lambda \\
\mu_{+} \\
\mu_{-}
\end{array}\right) \geq 0,\;\;\text{s.t. } A^{T}\left(\begin{array}{l}
\lambda \\
\mu_{+} \\
\mu_{-}
\end{array}\right)=\nabla f(\bar{x})
$$
令 $\mu = \mu_{+} - \mu_{-}$ 即可。&lt;/p>
&lt;h2 id="线性规划强对偶性">线性规划强对偶性&lt;/h2>
&lt;p>设线性规划&lt;/p>
&lt;p>$$
(\mathcal{P})\quad
\begin{array}{ll}
\operatorname{minimize} &amp;amp; \quad c^T x \\
\text {subject to } &amp;amp; A x =b, x \geq 0
\end{array} \qquad
(\mathcal{D})\quad
\begin{array}{ll}
\operatorname { maximize } \; b^T y \\
\text {subject to } \; A^{T} y \leq c
\end{array}
$$&lt;/p>
&lt;p>的原问题(P)和对偶问题(D)分别具有最优值 $p^\ast$ 和 $d^\ast$。&lt;/p>
&lt;p>弱对偶性是显然的，因为如果 $x^\ast \geq 0$ 满足 $Ax^\ast=b$ 是原问题的最优值，那么 $y^T b=y^T A x^\ast \leq c^T x^\ast=p^\ast$，从而 $d^\ast \leq p^\ast$。&lt;/p>
&lt;p>弱对偶性可以推广到取值为无穷的情况：&lt;/p>
&lt;ul>
&lt;li>如果 $p^\ast=-\infty$，原问题最优值是无界的，则必然有 $d^\ast=-\infty$，也就是对偶问题不可行。&lt;/li>
&lt;li>如果 $d^\ast=+\infty$，对偶问题最优值无界，也必然成立 $p^\ast=+\infty$，也就是说，原问题不可行。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>也可能出现情况：$d^\ast = -\infty, p^\ast = + \infty$，即原问题和对偶问题都不可行！&lt;/p>
&lt;p>如线性规划的原问题：
$$
\begin{aligned}
\min \; &amp;amp; x_{1}+2 x_{2} \\
\text { s.t. } &amp;amp; x_{1}+x_{2}=1 \\
&amp;amp; 2 x_{1}+2 x_{2}=3
\end{aligned}
$$
和对偶问题：
$$
\begin{aligned}
\max\; &amp;amp; y_{1}+3 y_{2} \\
\text { s.t. } &amp;amp; y_{1}+2 y_{2}=1 \\
&amp;amp; y_{1}+2 y_{2}=2
\end{aligned}
$$
都不可行！&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Farkas引理可以用来论证线性规划的强对偶性，强对偶性建立在原问题或对偶问题的某一个是可行的基础上。&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>当两个问题都不可行，即有 $d^\ast = - \infty, \; p^\ast = + \infty$，强对偶性不再成立！&lt;/p>
&lt;p>&lt;strong>线性规划强对偶性指的是：如果原问题可行，并且最优值有界，那么对偶问题也可行，且两个最优值相等。&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>设 $\epsilon &amp;gt; 0$，以及：&lt;/p>
&lt;p>$$
\hat{A}=\left(\begin{array}{l}
A \\
c^{T}
\end{array}\right) \quad \hat{b}=\left(\begin{array}{l}
\: b \\
p^{\ast}
\end{array}\right) \quad \hat{b}_{\epsilon}=\left(\begin{array}{c}
b \\
p^{\ast}-\epsilon
\end{array}\right)
$$&lt;/p>
&lt;p>现在 $p^\ast-\epsilon$ 是原问题的一个严格下界，所以 $\{\hat A x =\hat b , x\geq 0\}$ 是可行的，而 $\{\hat A x =\hat b_\epsilon , x\geq 0\}$ 是不可行的。应用两次 Farkas 引理，设 $\hat{y}=\left(\begin{array}{c}
y \
\alpha
\end{array}\right)$，我们有：&lt;/p>
&lt;p>$$
\left\{\begin{array}{l}
\hat{A}^{T}\left(\begin{array}{l}
y \\
\alpha
\end{array}\right)=A^{T} y+\alpha c \geq 0 \\
\hat{b}_{\epsilon}^{T}\left(\begin{array}{l}
y \\
\alpha
\end{array}\right)=b^{T} y+\alpha\left(p^{\ast}-\xi\right)=\hat b^{T}\left(\begin{array}{l}
y \\
\alpha
\end{array}\right) -\alpha \epsilon&amp;lt;0 \\
\hat b^{T}\left(\begin{array}{l}
y \\
\alpha
\end{array}\right) \geq 0
\end{array}\right.
$$&lt;/p>
&lt;p>可知 $\alpha &amp;gt; 0$、$-\alpha &amp;lt; 0$，从而&lt;/p>
&lt;p>$$
\left\{\begin{array}{l}
A^{T}\left(\displaystyle\frac{y}{-\alpha}\right) \leq c \\
b^{T}\left(\displaystyle\frac{y}{-\alpha}\right)&amp;gt;p^{\ast}-\epsilon
\end{array}\right.
$$&lt;/p>
&lt;p>这样其实找到了对偶问题的一个可行点 $\displaystyle\frac{y}{-\alpha}$，从而：&lt;/p>
&lt;p>$$
p^{\ast}-\epsilon &amp;lt; d^{\ast} \leq p^{\ast}
$$&lt;/p>
&lt;p>因为 $\epsilon$ 是任意的，所以 $d^\ast=p^\ast$，强对偶性成立！&lt;/p>
&lt;h3 id="互补松弛条件">互补松弛条件&lt;/h3>
&lt;p>从强对偶性出发，关于对偶变量，我们能够得到以下等式：&lt;/p>
&lt;p>$$
\left\{\begin{array}{l}
c^{T} x=b^{T} y \\
A x=b \\
A^{T} y+\lambda=c \\
x \geqslant 0, \lambda \geqslant 0
\end{array}\right.
$$&lt;/p>
&lt;p>上面的 $\lambda$ 是约束条件 $x \geqslant 0$ 的对偶变量。我们有：&lt;/p>
&lt;p>$$
y^T Ax=y^T b \Rightarrow (c-\lambda)^T x=c^T x \Rightarrow \lambda ^T x = 0
$$&lt;/p>
&lt;p>因为 $x \geqslant 0, \lambda \geqslant 0$，所以 $\lambda_i, x_i$ 必有一为0，其中 $\lambda _i$ 即为 $(A_i^T y - c_i)$ ，$A_i$ 是矩阵 $A$ 的第 $i$ 列。&lt;/p>
&lt;h3 id="强互补松弛">强互补松弛&lt;/h3>
&lt;p>对于线性规划式(*)，还成立强互补松弛性（strict complementary slackness）：&lt;/p>
&lt;ul>
&lt;li>$x^\ast_j &amp;gt; 0$&lt;/li>
&lt;li>$A_j^T y^\ast &amp;lt; c_j$&lt;/li>
&lt;/ul>
&lt;p>以上两个关系有且仅有一个成立。其中 $(x^\ast, y^\ast)$ 是原问题和对偶问题的一组最优解。&lt;/p>
&lt;blockquote>
&lt;p>意思就是，&lt;strong>总存在一组最优解使得强互补松弛条件（两个不等式中的一个）成立&lt;/strong>。&lt;/p>
&lt;/blockquote>
&lt;p>$A^T _j y^\ast &amp;lt; c_j \Rightarrow x_j^\ast = 0$ 是容易的，直接用互补松弛就可以了；而 $x^\ast_j = 0 \Rightarrow A^T_j y^\ast&amp;lt;c_j$ 证明起来有点困难。&lt;/p>
&lt;h3 id="对偶问题与影子价格">对偶问题与影子价格&lt;/h3>
&lt;p>要理解透影子价格，必须对线性规划的对偶有充分的认识！&lt;/p>
&lt;p>不妨考虑资源约束下的不等式线性规划 ($b \geq 0$)：
$$
(\mathcal{P})\quad
\begin{array}{ll}
\operatorname{minimize} &amp;amp; \quad c^T x \\
\text {subject to } &amp;amp; A x \geq b, x \geq 0
\end{array} \qquad
(\mathcal{D})\quad
\begin{array}{ll}
\operatorname { maximize } \; b^T y \\
\text {subject to } \; A^{T} y \leq c, y \geq 0
\end{array}
$$
对于最大化对偶问题(D)，可以看成是资源约束下的最大化利润问题，假如说 $(\bar{x}, \bar{y})$ 是一组最优解，如果资源约束增大为 $c_\epsilon = c + \epsilon$，在保证最优性和可行性的条件下，目标函数值增大 $\epsilon^T \bar{x}$ .&lt;/p>
&lt;h2 id="farkas-引理的应用">Farkas 引理的应用&lt;/h2>
&lt;p>利用 Farkas 引理能够得到一个有用的推论：&lt;/p>
&lt;ol>
&lt;li>$Ax \leq b$&lt;/li>
&lt;li>$A^{T} y \geq 0, b^{T} y&amp;lt;0, y \geq 0$&lt;/li>
&lt;/ol>
&lt;p>有且仅有一个是可行的。注意到 $Ax \leq b \Leftrightarrow Ax + s = b, s \geq 0$，对
$$
[A \;\; I] \left[\begin{aligned} x \\ s \end{aligned}\right] = b
$$
使用 Farkas 引理，对应的另一组约束条件是
$$
\begin{bmatrix}
A^T \\
I
\end{bmatrix} y \geq 0, \, b^T y &amp;lt; 0 \Rightarrow A^T y \geq 0, b^T y &amp;lt; 0, y \geq 0
$$
得证！&lt;/p>
&lt;blockquote>
&lt;p>与之相关的还有 Gordan 定理：
(1) $Ax &amp;lt; 0$
(2) $A^Ty=0, \:y\succeq 0, \:y\neq 0$
只能有一个有解！&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>总结：本文内容比较多，主要阐述了 Farkas 引理和线性规划的对偶，这两者可以说互为等价的关系。从强对偶性出发，我们可以轻松得到互补松弛条件。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/tags/duality/" term="Duality" label="Duality"/></entry><entry><title type="text">收敛速度的三种形式</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/convergence-rate/"/><id>https://allenz-me.github.io/posts/operations/convergence-rate/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-04-24T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">当我们设计迭代算法的时候，总是不可避免地要提到收敛速度这个概念。 比如，我们常说牛顿法……</summary><content type="html">&lt;p>当我们设计迭代算法的时候，总是不可避免地要提到收敛速度这个概念。&lt;/p>
&lt;p>比如，我们常说牛顿法是平方收敛的，弦截法是1.618阶收敛的，梯度下降法是线性收敛的，随机梯度下降法是次线性收敛的。&lt;/p>
&lt;p>在数值分析或者优化方法中，我们经常能看到如下这一段话：&lt;/p>
&lt;p>$$
\lim _{k \rightarrow \infty} \frac{\left\|x^{(k+1)}-x^{\ast}\right\|}{\left\|x^{(k)}-x^{\ast}\right\|}=C
$$&lt;/p>
&lt;ul>
&lt;li>如果 $C=1$, 我们就说序列 $x^{(k)}$ 是次线性收敛 (sublinear convergence) 到 $x^{\ast}$ 的。&lt;/li>
&lt;li>如果 $0&amp;lt;C&amp;lt;1$, 那么就说是线性收敛 (linear convergence) 。&lt;/li>
&lt;li>如果 $C=0$ ， 那么就说是超线性收敛 (superlinear convergence)。&lt;/li>
&lt;/ul>
&lt;p>在超线性收敛的基础上, 如果对 $p&amp;gt;1$ 成立 $\lim\limits_{k \rightarrow \infty} \displaystyle\frac{\left\|x^{(k+1)}-x^{\ast}\right\|}{\left\|x^{(k)}-x^{\ast}\right\|^{p}}=C$ 为常数, 就说序列 $x^{(k)}$ 是 $p$ 阶收敛到 $x^{\ast}$ 的。&lt;/p>
&lt;p>此处 $\|\cdot\|$ 表示一种范数, 若范数具有等价性, 则通常选取为 2 范数。&lt;/p>
&lt;p>&lt;strong>这样的一套定义其实是非常抽象的&lt;/strong>，我敢说正常人第一次接触这个东西肯定没法正确想象平方收敛到底比线性收敛快多少。要在脑子里直观地想象它们的收敛速度，最好的方式是找几个例子，再画个图，来比较一下。&lt;/p>
&lt;p>令:&lt;/p>
&lt;ul>
&lt;li>$a_{k}=1 / k$，这里 $a_{k}$ 是次线性收敛的&lt;/li>
&lt;li>$b_{k}=1 / k^{2}$，这里 $b_{k}$ 也是次线性收敛的&lt;/li>
&lt;li>$c_{k}=1 / 2^{k}$，这里 $c_{k}$ 是线性收敛的&lt;/li>
&lt;li>$d_{k}=1 / 2^{2^{k}}$，这里 $d_{k}$ 是平方收敛的&lt;/li>
&lt;/ul>
&lt;p>这四个序列都以0为极限，可以感受到， 哪怕是&lt;strong>普通的线性收敛都已经达到指数级别的收敛速度了&lt;/strong>（这也是我们为什么把线性收敛也叫指数收敛的原因！）超线性收敛的效果更是难以想象！&lt;/p>
&lt;p>画出四个数列的对数图：&lt;/p>
&lt;img src="../../figures/convergence-rate/conv.png" alt="conv" style="zoom: 50%;" />
&lt;p>代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">matplotlib.pyplot&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">plt&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">style&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">use&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;ggplot&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">semilogy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">semilogy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">semilogy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">semilogy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">6&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">6&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">legend&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="sa">r&lt;/span>&lt;span class="s2">&amp;#34;$1/k$&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">r&lt;/span>&lt;span class="s2">&amp;#34;$1/k^2$&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">r&lt;/span>&lt;span class="s2">&amp;#34;$1/2^k$&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">r&lt;/span>&lt;span class="s2">&amp;#34;$1/2^{2^k}$&amp;#34;&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>线性收敛在对数坐标图上已经是一条直线了，平方收敛就更是一条急速向下的曲线，这也就是为什么牛顿法能够在短短四五步迭代就达到我们期望的误差限的原因。&lt;/p>
&lt;p>至于次线性收敛，则一般&lt;strong>是以多项式的速度逼近&lt;/strong>，如 $\displaystyle\frac{1}{\sqrt{k}},\; \displaystyle\frac{1}{k^{3}}$ 这些都是次线性收敛的。但即便都是次线性收敛，其实快慢也是可以有区别的。上面这种描述收敛性的方式无法对次线性收敛进行更加精确的比较！&lt;/p>
&lt;h4 id="收敛速度的其它表示方法">收敛速度的其它表示方法&lt;/h4>
&lt;p>优化里面，很多算法都还不能达到线性收敛（指数速度收敛）这样的速度，为了比较各种次线性收敛，就必须要&lt;strong>引入更精细的表示收敛速度的方式&lt;/strong>！&lt;/p>
&lt;p>比如, 在某一条件下, 以 $t$ 为固定学习率的梯度下降法的函数值序列满足:
$$
f\left(x^{(k)}\right)-f^{\ast} \leq \frac{\left\|x^{(0)}-x^{\ast}\right\|_{2}^{2}}{2 t k}
$$
这时候我们就会说序列 $f\left(x^{(k)}\right)$ 收敛到 $f^{\ast}$ 的速度是 $O(1 / k)$ 的，这里的 $1 / k$ 表示误差 $k$ 越大，误差越小；如果 $k$ 扩大2倍, 也就是迭代步数乘以 2 , 误差缩小为原来的一半。&lt;/p>
&lt;p>另一种表示这种收敛速度的方式是 $O(1 / \epsilon), \; 1 / \epsilon$ 的意思是, 如果我们希望将误差控制在 $\epsilon$ 内，令 $\displaystyle\frac{\left\|x^{(0)}-x^{\ast}\right\|_{2}^{2}}{2 t k} \leq \epsilon$，得到迭代次数 $k \geq \displaystyle\frac{\left\|x^{(0)}-x^{\ast}\right\|_{2}^{2}}{2 t \epsilon}$ 是 $O(1 / \epsilon)$ 阶的。&lt;strong>换句话说, 要让误差减少为原来的一半, 就必须将迭代次数扩大一倍!&lt;/strong>&lt;/p>
&lt;p>更好一点的算法, 比如 $O\left(1 / k^{2}\right)$ 的 (对应是 $O(1 / \sqrt{\epsilon})$ ), 它也是次线性收敛的, 但是速度要 比 $O(1 / k)$ 快很多。&lt;/p>
&lt;p>用这两种方式我们就能对次线性收敛进行比较啦！&lt;/p>
&lt;p>同时：&lt;/p>
&lt;ul>
&lt;li>线性收敛的表示: $O\left(\gamma^{-k}\right)(\gamma&amp;gt;1)$ ；或者 $O(\log \displaystyle\frac{1}{\epsilon})$&lt;/li>
&lt;li>平方收敛的表示: $O(\gamma^{-2^{k}})(\gamma&amp;gt;1)$； 或者 $O(\log \log \displaystyle\frac{1}{\epsilon})$&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>总结：本文用图比较了各种收敛性的收敛速度，并给出了收敛速度三种表示方式的描述和分析。&lt;/strong>&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/></entry><entry><title type="text">熵与信息</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/entropy/"/><id>https://allenz-me.github.io/posts/analysis/entropy/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-02-07T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">熵 熵，在统计学或信息论里，我们称其为信息熵；在物理学领域，一般指热力学熵。信息熵和热……</summary><content type="html">&lt;h2 id="熵">熵&lt;/h2>
&lt;p>熵，在统计学或信息论里，我们称其为信息熵；在物理学领域，一般指热力学熵。信息熵和热力学熵是统一的。熵是体系混乱度的度量，熵越大，说明这个体系越混乱。&lt;/p>
&lt;p>在统计学和信息论中，熵被用来度量不确定性，熵的意义是信息量的多少。对于一个随机试验（不同事件的可能的概率为$p_1,p_2, ..., p_n$），希望找到一个函数 $H$ 来度量这个随机试验的不确定性。香农认为，熵 $H$ 需要满足三个条件：&lt;/p>
&lt;ol>
&lt;li>是 $p_i$ 的连续函数。&lt;/li>
&lt;li>对于 $n$ 个等概率结果的试验，$H$ 是 $n$ 的单调上升函数。&lt;/li>
&lt;li>一个试验分成两个相继试验，未分之前的 $H$ 是既分之后的 $H$ 的加权和。（可加性）&lt;/li>
&lt;/ol>
&lt;p>可以证明满足以上三个条件的 $H$ 具有下列形式：&lt;/p>
&lt;p>$$
H=-C\sum^n_{i=1}p_i\log p_i
$$&lt;/p>
&lt;p>其中 $C$ 是正的常数。&lt;/p>
&lt;h2 id="事件的自信息">事件的自信息&lt;/h2>
&lt;p>自信息表示某一事件发生时所带来的信息量的多少，当事件发生的概率越大，则自信息越小。七月飘雪这一事件所包含的信息要高于腊月下雪。&lt;/p>
&lt;p>一个随机事件的自信息等于其概率的负对数（$-\log p$）。自信息满足可加性，即两个独立事件的自信息等于两个事件单独的自信息。&lt;/p>
&lt;h2 id="信息熵information-entropy">信息熵（information entropy）&lt;/h2>
&lt;p>自信息一般用来描述一个随机事件的信息量大小。&lt;strong>信息熵则是用来描述整个随机分布所带来的信息量平均值。&lt;/strong> 离散型随机变量 $X$ 取不同值的可能概率为 $p_1,p_2, ..., p_n$，那么 $X$ 的信息熵为：&lt;/p>
&lt;p>$$
H(X)=-\sum_{x \in \mathcal{X}} p(x) \log {p(x)}= - \sum_{i=1}^n p_i \log p_i
$$&lt;/p>
&lt;p>信息熵只与随机变量取值的概率有关，而与它具体的取值无关。而诸如随机变量的方差、变异系数等，是与随机变量的具体取值有关的。&lt;/p>
&lt;p>容易看出，$H(X)$ 在 $p_1=p_2=...=p_n=\frac{1}{n}$ 时取最大值，且其最大值等于 $\log |\mathcal{X}|$，其中$|\mathcal{X}|=n$ 是 $X$ 取值的数量。&lt;/p>
&lt;p>随机变量的信息熵的数值，在信息论中，实际反映的是平均的二进制编码长度，也反映了信息的极限压缩程度。
比如说，如果 $X$ 有 $4$ 种可能的取值，我们就可以用 00、01、10、11 去分别表示；如果 $X$ 有 $3$ 种可能的取值，我们就可以用 00、01、1 去分别表示，我们会发现，后者的平均编码长度更短。&lt;/p>
&lt;h2 id="联合熵joint-entropy">联合熵（joint entropy）&lt;/h2>
&lt;p>离散型随机变量 $X, Y$ 的联合熵定义如下：&lt;/p>
&lt;p>$$
H(X, Y)=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y)
$$&lt;/p>
&lt;p>它反映了描述一对随机变量平均所需要的信息量。以上定义容易推广到多个随机变量的情况。事实上，这也是随机向量的熵的定义。&lt;/p>
&lt;p>联合信息量小于等于独立观察信息量之和：&lt;/p>
&lt;p>$$
H(X, Y) \leqslant H(X) + H(Y)
$$&lt;/p>
&lt;h2 id="条件熵conditional-entropy">条件熵（conditional entropy）&lt;/h2>
&lt;p>若 $(X, Y) \sim p(x, y)$，则条件熵 $H(Y\mid X)$ 定义为：&lt;/p>
&lt;p>$$
\begin{aligned}
H(Y \mid X) &amp;amp;=\sum_{x \in \mathcal{X}} p(x) H(Y \mid X=x) \\
&amp;amp;=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(y|x) \\
\end{aligned}
$$&lt;/p>
&lt;p>可以认为是已知随机变量 $X$ 后，$Y$ 的信息量，所以有：$H(Y \mid X) \leqslant H(Y)$。当我们知道了统计相关性的变量之后，就可以减少不确定性。&lt;/p>
&lt;blockquote>
&lt;p>并不意味着 $H(Y \mid X=x) \leqslant H(Y)$。&lt;/p>
&lt;/blockquote>
&lt;p>如果 $X, Y$ 统计独立，那么&lt;/p>
&lt;p>$$
H(X\mid Y)=H(X), \quad H(Y\mid X) = H(Y)
$$&lt;/p>
&lt;p>这时候知道 $X$ 不会给 $Y$ 提供任何信息，知道 $Y$ 也不会给 $X$ 提供任何信息。&lt;/p>
&lt;p>$(X, Y)$ 的联合熵可以看作是两个连续实验得到的信息：先观察 $X$，在已知 $X$ 的基础上观察 $Y$。于是我们能够得到如下等式：&lt;/p>
&lt;p>$$
H(X, Y)=H(X) + H(Y \mid X)
$$&lt;/p>
&lt;p>由此我们可以得到：&lt;/p>
&lt;p>$$
H(X)+H(Y) - H(X, Y) = H(Y)-H(Y\mid X) = H(X) - H(X\mid Y)
$$&lt;/p>
&lt;p>从而，联合观察 $X, Y$ 的信息量，比起独立观察 $X, Y$ 的信息量之和，少的那一部分，就是 $X, Y$ 之间相关关系带来的信息损失。应用在随机事件上，容易理解，两个相关性很强的小概率事件同时发生的信息量，要少于某个小概率时间发生信息量的两倍。&lt;/p>
&lt;blockquote>
&lt;p>类似地，不难解释下面的等式：
$$
H(X, Y \mid Z) = H(X \mid Z) + H(Y \mid X, Z)
$$&lt;/p>
&lt;/blockquote>
&lt;h2 id="互信息mutual-information">互信息（mutual information）&lt;/h2>
&lt;p>定义离散型随机变量 $X$ 与 $Y$ 之间的互信息为：&lt;/p>
&lt;p>$$
\begin{aligned}
I(X, Y)=&amp;amp; H(X)-H(X \mid Y)=H(Y)-H(Y \mid X) \\
=&amp;amp;H(X)+H(Y)-H(X, Y) \\
=&amp;amp; H(X, Y)-H(X \mid Y)-H(Y \mid X)
\end{aligned}
$$&lt;/p>
&lt;p>也可以直接定义互信息：&lt;/p>
&lt;p>$$
I(X, Y) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}
$$&lt;/p>
&lt;p>互信息满足对称性。它表示，在知道 $Y$ 之后，$X$ 的信息量减少了多少；&lt;strong>换言之，$Y$ 能为 $X$ 提供多少信息量&lt;/strong>。或者说，$X, Y$ 相关关系所蕴含的信息量。&lt;/p>
&lt;p>易知，若 $X, Y$ 统计独立，那么 $I(X, Y)=0$。$X, Y$ 不能为对方提供任何信息。&lt;/p>
&lt;blockquote>
&lt;p>决策树ID3算法的训练规则：$g(D, A)=H(D)-H(D \mid A)$，本质上就是找到一个与类别的互信息最大的特征，这个特征与类别的相关关系包含了最多的信息。&lt;/p>
&lt;/blockquote>
&lt;h2 id="信息散度information-divergence">信息散度（information divergence）&lt;/h2>
&lt;p>信息散度被用来衡量两个分布之间的相似度，最常用的、也是与香农体系相容的，是KL散度（&lt;em>Kullback-Leibler divergence&lt;/em>），也叫相对熵（&lt;em>relative entropy&lt;/em>）、鉴别信息（&lt;em>information for discrimination&lt;/em>）。KL散度用于刻画使用理论分布$q(x)$拟合真实分布$p(x)$时产生的信息损失：&lt;/p>
&lt;p>$$
D_{KL}(p \lVert q)=\sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}
$$&lt;/p>
&lt;p>&lt;strong>KL散度具有非负性。&lt;/strong> 两个分布越接近，那么它们的KL散度值越小。如果 $p=q$，那么 $D_{KL}(p \lVert q)=0$。值得注意的是，尽管KL散度能够衡量两个分布之间的相似度，但它并不是一个分布之间&lt;em>距离&lt;/em>，&lt;strong>KL散度不满足距离公理中的对称性和三角不等式&lt;/strong>。&lt;/p>
&lt;p>离散型随机变量的熵与KL散度之间存在关系：&lt;/p>
&lt;p>$$
H(X)=\log |\mathcal{X}| - D(p \lVert u)
$$&lt;/p>
&lt;p>其中 $\mathcal{X}$ 是 $X$ 的值域集，$u$ 是 $\mathcal{X}$ 上的等概均匀分布，并且我们知道，等概分布 $u$ 的信息熵就是 $\log|\mathcal{X}|$，这个式子背后的意义是好理解的。&lt;/p>
&lt;p>互信息与KL散度满足关系：&lt;/p>
&lt;p>$$
I(X, Y) = D(p(x, y) \lVert p(x)p(y))
$$&lt;/p>
&lt;p>它表示，假定 $X, Y$ 独立，这种假定与真实情况的差别有多大。&lt;/p>
&lt;h2 id="交叉熵cross-entropy">交叉熵（cross entropy）&lt;/h2>
&lt;p>交叉熵表示，如果用错误的编码方式 $q(x)$ 去编码真实分布 $p(x)$ 所需要的平均编码长度：&lt;/p>
&lt;p>$$
H(p, q) = - \sum_{x \in \mathcal{X}} p(x) \log q(x)
$$&lt;/p>
&lt;p>交叉熵常用作机器学习的损失函数，它也能够衡量两个分布的相似程度，交叉熵越小，分布越相似。&lt;/p>
&lt;blockquote>
&lt;p>按照我的理解，每个样本都是独立的随机变量，$X_1, X_2, ... X_n = 0 \text{ or } 1$ 是样本标签，$Y_1, Y_2, ... Y_n$ 是概率模型的输出，此时 $P(Y_i=1)=1-P(Y_i=0)=y_{pred_i}$，交叉熵损失函数就是 $\sum_{i=1}^n H(X_i, Y_i)$。&lt;/p>
&lt;/blockquote>
&lt;p>交叉熵与KL散度之间存在关系：&lt;/p>
&lt;p>$$
D_{KL}(p\lVert q) = H(p, q) - H(p)
$$&lt;/p>
&lt;p>因此寻找最优的拟合分布 $q$ 既是优化交叉熵也是优化信息散度。由此我们还得到了信息散度的另一种解释：用非真实分布 $q$ 得到的平均码长比真实分布 $p$ 得到的平均码长多出的比特数。&lt;/p>
&lt;h2 id="微分熵">微分熵&lt;/h2>
&lt;p>对于连续型随机变量，我们总能找到一列离散型随机变量去无限地逼近它，然后连续型随机变量的信息熵，就定义为这列离散型随机变量的极限：&lt;/p>
&lt;p>$$
H(X) = - \int_{-\infty}^{+\infty}p(x) \log p(x) \mathrm{d} x
$$&lt;/p>
&lt;blockquote>
&lt;p>事实上，连续型随机变量所具有的信息是无穷大的，以上仅是一种形式上的定义。&lt;/p>
&lt;/blockquote>
&lt;p>由此我们还能够类似地定义联合熵：&lt;/p>
&lt;p>$$
H(X, Y) = - \iint_{\mathbf{R}^2} p(x, y) \log p(x, y) \mathrm{d} x \mathrm{d} y
$$&lt;/p>
&lt;p>条件熵：&lt;/p>
&lt;p>$$
H(X\mid Y) = -\iint _{\mathbf{R}^2}p(x, y)\log p(x| y) \mathrm{d} x \mathrm{d} y
$$&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>注意：微分熵可能为负值。&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;p>$$
\begin{array}{l}
X: p(x)=\left\{\begin{array}{cc}
\frac{1}{b-a} &amp;amp; a \leq x \leq b, &amp;amp; b-a&amp;lt;1 \\
0 &amp;amp; x&amp;gt;b, &amp;amp; x&amp;lt;a
\end{array}\right. \\
H(X)=-\int_{a}^{b} \frac{1}{b-a} \log \frac{1}{b-a} d x=\log (b-a)&amp;lt;0
\end{array}
$$&lt;/p>
&lt;p>对于正态分布 $\mathcal{N}(\mu, \sigma^2)$ 的随机变量 $X$，其微分熵为：$H(X)=\frac{1}{2} \log 2\pi e \sigma^2$，与均值无关。&lt;/p>
&lt;p>若连续型随机变量的均值和方差均被给定，则当其服从正态分布的时候，微分熵最大！（这一点与热力学相同）&lt;/p>
&lt;h2 id="熵的优化">熵的优化&lt;/h2>
&lt;p>信息熵关于 $p$ 是凹函数。因此最大化信息熵是一个凸的问题。&lt;/p>
&lt;p>交叉熵 $H(p, q)$ 关于 $q$ 是凸的，因此给定分布 $p$，寻找 $q$ 最小化交叉熵是凸的问题。&lt;/p>
&lt;p>KL散度 $D_{KL}(p\lVert q)=\sum_{i=1}^n p_i \log \frac{p_i}{q_i}$ 对 $(p, q)$ 是凸的。因此最小化与一个给定分布的KL散度这一问题是凸的。&lt;/p>
&lt;h2 id="fano-不等式">Fano 不等式&lt;/h2>
&lt;p>假设随机变量 $X$ 和 $Y$ 分别是输入、输出信息，我们无法直接观测到 $X$，只能观察到与 $X$ 相关的随机变量 $Y$，根据随机变量 $Y$ 我们可以做出对原随机变量 $X$ 的一个估计 $\hat X=f(Y)$。&lt;/p>
&lt;p>$$
X\rightarrow Y \rightarrow \hat X
$$&lt;/p>
&lt;p>对于任意的估计 $\hat X$，定义错误概率 $P_e=P(\hat X \neq X)$，此时Fano不等式成立：&lt;/p>
&lt;p>$$
H\left(P_{e}\right)+P_{e} \log |\mathrm{X}| \geqslant H(X \mid \hat{X}) \geqslant H(X \mid Y)
$$&lt;/p>
&lt;p>Fano不等式给出了解码器错误概率的下界。&lt;/p>
&lt;hr>
&lt;p>&lt;strong>总结：本文阐述了三个重要的概念：信息熵、互信息、信息散度（相对熵）。信息熵衡量一个随机变量所具有的信息量，互信息衡量两个随机变量的相关关系所蕴含的信息，信息散度衡量两个分布之间的差异程度。&lt;/strong>&lt;/p>
&lt;p>参考资料&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.bilibili.com/video/BV1Fs411g7G7?seid=10007690006652926935">https://www.bilibili.com/video/BV1Fs411g7G7?seid=10007690006652926935&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.csdn.net/pipisorry/article/details/51695283">https://blog.csdn.net/pipisorry/article/details/51695283&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.jianshu.com/p/31a683cddb94">机器学习中的熵、条件熵、相对熵(KL散度)和交叉熵&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Fano%27s_inequality">https://en.wikipedia.org/wiki/Fano%27s_inequality&lt;/a>&lt;/li>
&lt;/ul></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/><category scheme="https://allenz-me.github.io/tags/%E7%86%B5/" term="熵" label="熵"/></entry><entry><title type="text">Concentration of sums of independent random variables</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/hdp2/"/><id>https://allenz-me.github.io/posts/analysis/hdp2/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-01-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">本文总结自 高维概率 第二章 本书第二章主要讲的是多个独立随机变量的和，它们在均值附近的集……</summary><content type="html">&lt;!-- #! https://zhuanlan.zhihu.com/p/413301756
# Concentration of sums of independent random variables -->
&lt;p>本文总结自 高维概率 第二章&lt;/p>
&lt;!-- 本书：
[HDP-book.pdf](https://uploader.shimo.im/f/CwR4dIKrjpcCDJgR.pdf?fileGuid=DGdRyQyhVkdyvcWd)
某大佬做的视频链接：
[https://www.bilibili.com/video/BV1gZ4y1W7ED](https://www.bilibili.com/video/BV1gZ4y1W7ED)
[https://www.bilibili.com/video/BV1kQ4y1P7y9](https://www.bilibili.com/video/BV1kQ4y1P7y9) -->
&lt;p>本书第二章主要讲的是多个独立随机变量的和，它们在均值附近的集中程度。&lt;/p>
&lt;h2 id="probability-concentration">Probability Concentration&lt;/h2>
&lt;p>现在我们要考虑的是一个probability concentration的问题，也可以说是去研究概率分布的尾巴，就像这样：&lt;/p>
&lt;p>$$
\mathbb{P}\{|X-\mu|&amp;gt;t\} \leq \text { something small. }\\
$$&lt;/p>
&lt;p>在随机变量方差存在的情况下，我们有切比雪夫不等式。它利用到了随机变量的方差，而且它的界是紧的，即我们总可以构造出一个随机变量使切比雪夫不等式取等号。但是，对于一些常见的概率分布而言，无论是正态分布还是指数分布等，切比雪夫不等式给出的界都显得太过粗糙。&lt;/p>
&lt;p>最为经典的正态分布自然第一个成为我们的“靶子”，容易证明正态分布的尾巴是一个以指数速度 $\frac{1}{t}e^{-t^2/2}$ 收敛于 0 的。如果 $g \sim \mathcal{N}(0, 1)$，那么：&lt;/p>
&lt;p>$$
\left(\frac{1}{t}-\frac{1}{t^{3}}\right) \cdot \frac{1}{\sqrt{2 \pi}} e^{-t^{2} / 2} \leq \mathbb{P}\{g \geq t\} \leq \frac{1}{t} \cdot \frac{1}{\sqrt{2 \pi}} e^{-t^{2} / 2}\\
$$&lt;/p>
&lt;p>这不难从正态分布的CDF推出。虽然中心极限定理告诉我们任何分布的标准化都趋向于正态分布，但是 CLT 的误差界却是 $1/\sqrt{N}$ 的（Berry-Esseen CLT），能保证的收敛速度很慢，我们无法借助它来严格推导任意分布的尾巴，这逼迫我们寻找新的方法。&lt;/p>
&lt;h2 id="hoeffding-inequality">Hoeffding Inequality&lt;/h2>
&lt;p>从对称伯努利分布的和出发，即设 $\mathbb{P}\{X=-1\}=\mathbb{P}\{X=1\}=\frac{1}{2}$，那么 $X$ 的矩母函数：&lt;/p>
&lt;p>$$
\mathbb{E}[e^{\lambda X}] = \frac{e^\lambda + e^{-\lambda}}{2} \leq e^{\frac{\lambda^2}{2}} \\
$$&lt;/p>
&lt;p>从而：&lt;/p>
&lt;p>$$
\mathbb{P}\left\{S_{n} = \sum_{i=1}^n X_i \geq t\right\}=\mathbb{P} \left\{e^{\lambda S_{n}} \geq e^{\lambda t}\right\} \leq \frac{E\left[e^{\lambda S_{n}}\right]}{e^{\lambda t}} = \frac{ \prod_{i=1}^n E\left[e^{\lambda X_{i}}\right]}{e^{\lambda t}} \leq e^{\frac{n}{2}\lambda^2 - \lambda t} \\
$$&lt;/p>
&lt;p>取 $\lambda = \displaystyle\frac{t}{n}$，就有 $\mathbb{P}\{S_n \geq t \} \leq e^{-\frac{t^2}{2n}}$ ；同时不难得到 $\mathbb{P}\{S_n \geq \sqrt{-2n \ln \epsilon}\} \leq \epsilon$，即 $S_n \sim O(\sqrt{n})$。&lt;/p>
&lt;p>我们借助矩母函数，说明了对称伯努利分布的和的尾巴是指数速度趋于0的。使用相同的方法，可以得到&lt;strong>针对有界随机变量的Hoeffding不等式&lt;/strong>：&lt;/p>
&lt;p>$$
\forall t &amp;gt; 0, \;\; X_i \in [m_i, M_i] \quad\Longrightarrow \quad\mathbb{P}\left\{\sum_{i=1}^{N}\left(X_{i}-\mathbb{E} X_{i}\right) \geq t\right\} \leq \exp \left(-\frac{2 t^{2}}{\sum_{i=1}^{N}\left(M_{i}-m_{i}\right)^{2}}\right)\\
$$&lt;/p>
&lt;p>在推导的过程中，主要用到了 Hoeffding 引理 $\mathbb{E}[e^{\lambda (X - \mathbb{E}[X])}] \leq e^{\lambda^2(M-m)^2/8}$ 和技巧：&lt;/p>
&lt;p>$$
\mathbb{P}(X \geq t) = \mathbb{P}(e^{\lambda X} \geq e^{\lambda t}) \leq \frac{\mathbb{E}[e^{\lambda X}]}{e^{\lambda t}}, \;\forall \lambda \geq 0 \Longrightarrow \mathbb{P}(X \geq t) \leq \min_{\lambda \geq 0} \frac{\mathbb{E}[e^{\lambda X}]}{e^{\lambda t}}\\
$$&lt;/p>
&lt;p>$E[e^{\lambda X}]$ 是 $X$ 的矩母函数，独立随机变量和的矩母函数等于其矩母函数的乘积！&lt;/p>
&lt;p>注意到正态分布也成立 Hoeffding 不等式，但正态分布并不是有界的，这说明 Hoeffding 不等式可以在某些条件下推广到无界的分布，这也是我们提出次高斯分布的原因。&lt;/p>
&lt;p>通过 Hoeffding 不等式我们知道，有界随机变量的和，以及正态分布的尾巴是以 $e^{-t^2}$ 这一速度趋于0的，但并不是所有的分布都能达到这种速度。比如我们熟知的指数分布，它就是 $e^{-t}$ 级别的收敛速度。&lt;/p>
&lt;p>Chernoff 不等式对于0-1伯努利分布的和给出了一个更好的界，由二项分布的泊松近似可知参数为 $\lambda$ 的泊松分布的右尾满足：$\forall t &amp;gt; \lambda, \; \mathbb{P}\{X \geq t\} \leq e^{-\lambda}\left(\frac{e \lambda}{t}\right)^{t}$，这说明泊松分布的尾的收敛速度是 $e^{-t \ln t}$ 级别的。这个尾巴也可以从上面提到的技巧来证明。&lt;/p>
&lt;h2 id="sub-gaussian-distribution">Sub-gaussian Distribution&lt;/h2>
&lt;p>正态分布可能是以 $e^{-t^2}$ 为尾概率的最为典型的分布了，我们都知道正态分布出现均值外3个标准差的概率是很小的，它的集中度是很高的，这种良好性质又足以拓展到其它的分布，我想这也是次高斯分布提出的原因之一。&lt;/p>
&lt;p>次高斯分布一般都是值零均值的分布，零均值也能方便我们讨论问题，它有若干条等价定义：&lt;/p>
&lt;ul>
&lt;li>$X$ 的尾巴满足：&lt;/li>
&lt;/ul>
&lt;p>$$
\mathbb{P}\{|X| \geq t\} \leq 2 \exp \left(-t^{2} / K_{1}^{2}\right) \quad \text { for all } t \geq 0 \\
$$&lt;/p>
&lt;ul>
&lt;li>$X$ 的矩满足：&lt;/li>
&lt;/ul>
&lt;p>$$
\|X\|_{L^{p}}=\left(\mathbb{E}|X|^{p}\right)^{1 / p} \leq K_{2} \sqrt{p} \quad \text { for all } p \geq 1 \\
$$&lt;/p>
&lt;ul>
&lt;li>$X^2$ 的矩母函数满足：&lt;/li>
&lt;/ul>
&lt;p>$$
\mathbb{E} \exp \left(\lambda^{2} X^{2}\right) \leq \exp \left(K_{3}^{2} \lambda^{2}\right) \quad \text { for all } \lambda \text { such that }|\lambda| \leq \frac{1}{K_{3}} \\
$$&lt;/p>
&lt;ul>
&lt;li>$X^2$ 的矩母函数在某一点有界：&lt;/li>
&lt;/ul>
&lt;p>$$
\mathbb{E} \exp \left(X^{2} / K_{4}^{2}\right) \leq 2 \\
$$&lt;/p>
&lt;ul>
&lt;li>$X$ 的矩母函数满足：&lt;/li>
&lt;/ul>
&lt;p>$$
\mathbb{E} \exp (\lambda X) \leq \exp \left(K_{5}^{2} \lambda^{2}\right) \quad \text { for all } \lambda \in \mathbb{R} \\
$$&lt;/p>
&lt;p>除去分布的尾巴，&lt;strong>矩母函数/高阶矩也能刻画分布的集中程度&lt;/strong>。期望不存在的分布一定不是次高斯（次指数）分布！&lt;/p>
&lt;p>对于次高斯分布还有次高斯模（sub-gaussian norm）的概念：&lt;/p>
&lt;p>$$
\|X\|_{\psi_{2}}=\inf \left\{t&amp;gt;0: \mathbb{E} \exp \left(X^{2} / t^{2}\right) \leq 2\right\}\\
$$&lt;/p>
&lt;p>次高斯模有界的分布都是次高斯分布。&lt;/p>
&lt;blockquote>
&lt;p>次高斯模和后面定义的次指数模都是相应函数族空间中的范数。&lt;/p>
&lt;/blockquote>
&lt;p>类似于正态分布方差具有可加性，次高斯随机变量的和的次高斯模满足：&lt;/p>
&lt;p>$$
\left\|\sum_{i=1}^{N} X_{i}\right\|_{\psi_{2}}^{2} \leq C \sum_{i=1}^{N}\left\|X_{i}\right\|_{\psi_{2}}^{2}\\
$$&lt;/p>
&lt;p>这个性质使得我们可以推出次高斯分布和的&lt;strong>广义Hoeffding不等式&lt;/strong>：&lt;/p>
&lt;p>$$
\mathbb{P}\left\{\left|\sum_{i=1}^{N} X_{i}\right| \geq t\right\} \leq 2 \exp \left(-\frac{c t^{2}}{\sum_{i=1}^{N}\left\|X_{i}\right\|_{\psi_{2}}^{2}}\right)\\
$$&lt;/p>
&lt;p>这就是说，次高斯分布和次高斯分布的和，它们的尾巴都具有以 $e^{-t^2}$ 的速度趋近于0的良好性质！&lt;/p>
&lt;p>另外，对于非零均值的分布，因为&lt;/p>
&lt;p>$$
\|X-\mathbb{E} X\|_{\psi_{2}} \leq\|X\|_{\psi_{2}}+\|\mathbb{E} X\|_{\psi_{2}} \Rightarrow \|X-\mathbb{E} X\|_{\psi_{2}} \leq C\|X\|_{\psi_{2}}\\
$$&lt;/p>
&lt;p>一样可以写出其零均值化后的形式。于是，广义的Hoeffding不等式与之前针对有界随机变量的Hoeffding不等式实现了优美的统一。&lt;/p>
&lt;p>&lt;em>&lt;strong>McDiarmid 1998&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>注：有界随机变量的 Hoeffding 不等式的另一种推广是 McDiarmid’s inequality，如果&lt;/em> $f:\mathrm{R}^n \to \mathrm{R}$ &lt;em>是可测函数，&lt;/em> $X$ &lt;em>是&lt;/em> $n$ &lt;em>维随机向量，并且：&lt;/em>
$$
\left|f\left(x_{1}, \ldots, x_{i}, \ldots, x_{n}\right)-f\left(x_{1}, \ldots, x_{i}^{\prime}, \ldots, x_{n}\right)\right| \leq c_{i}, \forall i =1, 2, ..., n\\
$$&lt;/p>
&lt;p>&lt;em>那么：&lt;/em>&lt;/p>
&lt;p>$$
\mathbb{P}\{f(X)-\mathbb{E} f(X) \geq t\} \leq \exp \left(-\frac{2 t^{2}}{\sum_{i=1}^{n} c_{i}^{2}}\right)\\
$$&lt;/p>
&lt;p>&lt;em>注意到取&lt;/em> $f(X)=\sum_{i=1}^n X_i$ &lt;em>就刚好是 Hoeffding 不等式。&lt;/em>&lt;/p>
&lt;h2 id="sub-exponential-distributions">Sub-exponential Distributions&lt;/h2>
&lt;p>次高斯分布拓展了高斯分布的尾概率的性质，但它还不够广泛，并不是所有的分布都有那么细的尾巴。比如说，正态分布的平方和，卡方分布，和指数分布等，它们的尾巴就只是 $e^{-t}$ 阶的。&lt;/p>
&lt;p>仿照次高斯分布，我们不难定义次指数分布，我觉得指数分布可以说是这类分布中最典型的了，&lt;/p>
&lt;p>同样地，还有次指数模的概念：&lt;/p>
&lt;p>$$
\|X\|_{\psi_{1}}=\inf \,\{t&amp;gt;0: \mathbb{E} \exp (|X| / t) \leq 2\}\\
$$&lt;/p>
&lt;p>结合两个定义，&lt;strong>次高斯分布的平方恰好就是次指数分布&lt;/strong>，并且$\left\|X^{2}\right\|_{\psi_{1}}=\|X\|_{\psi_{2}}^{2}$。两个次高斯分布的积是次指数的，并且$\|X Y\|_{\psi_{1}} \leq\|X\|_{\psi_{2}}\|Y\|_{\psi_{2}}$。&lt;/p>
&lt;p>次指数分布有Bernstein不等式来刻画它的集中程度：&lt;/p>
&lt;p>$$
\mathbb{P}\left\{\left|\sum_{i=1}^{N} X_{i}\right| \geq t\right\} \leq 2 \exp \left[-c \min \left(\frac{t^{2}}{\sum_{i=1}^{N}\left\|X_{i}\right\|_{\psi_{1}}^{2}}, \frac{t}{\max _{i}\left\|X_{i}\right\|_{\psi_{1}}}\right)\right]\\
$$&lt;/p>
&lt;p>该不等式也容易由$\|X-\mathbb{E} X\|_{\psi_{1}} \leq C\|X\|_{\psi_{1}}$推广到非零均值随机变量。&lt;/p>
&lt;p>Bernstein不等式的意思是，在均值附近，衰减速度接近高斯分布的衰减速度，远离均值的部分，则是以指数分布的尾巴衰减的，这与泊松分布（二项分布的极限）是类似的。Bernstein不等式下面这种形式反映了该思想：&lt;/p>
&lt;p>$$
\mathbb{P}\left\{\left|\frac{1}{\sqrt{N}} \sum_{i=1}^{N} X_{i}\right| \geq t\right\} \leq\left\{\begin{array}{ll}2 \exp \left(-c t^{2}\right), &amp;amp; t \leq C \sqrt{N} \\2 \exp (-t \sqrt{N}), &amp;amp; t \geq C \sqrt{N}\end{array}\right.\\
$$&lt;/p>
&lt;p>当$|X| \leq K$是有界随机变量的时候，Bernstein不等式还可以写为如下的形式：&lt;/p>
&lt;p>$$
\mathbb{P}\left\{\left|\sum_{i=1}^{N} X_{i}\right| \geq t\right\} \leq 2 \exp \left(-\frac{t^{2} / 2}{\sigma^{2}+K t / 3}\right)\\
$$&lt;/p>
&lt;p>本章的最后一节还提到了两个重要的不等式，其中一个是 McDiarmid 不等式，已经介绍过了，另一个是 Bennett 不等式：&lt;/p>
&lt;p>$$
\mathbb{P}\left\{\sum_{i=1}^{N}\left(X_{i}-\mathbb{E} X_{i}\right) \geq t\right\} \leq \exp \left(-\frac{\sigma^{2}}{K^{2}} h\left(\frac{K t}{\sigma^{2}}\right)\right)\\
$$&lt;/p>
&lt;p>其中 $| X_i - \mathbb{E}[X_i] | \leq K$，$\sigma^2 = \sum_{i=1}^N \operatorname{var}(X_i), \; h(u) = (1 + u) \ln (1 + u) - u.$&lt;/p>
&lt;h2 id="further-thoughts">Further Thoughts&lt;/h2>
&lt;p>本章主要讲的是如何去刻画随机变量的集中程度，由此提出了次高斯分布和次指数分布的这两个概念，它们是我们熟悉的正态分布和指数分布的推广。&lt;/p>
&lt;p>如果一个分布的尾巴比指数分布还要厚，比如它是多项式速度 $t^{-k}$ 衰减到0的，统计学上称它为重尾分布（heavy-tail）。比如柯西分布、对数正态分布、某些帕累托分布。&lt;/p>
&lt;p>（离散）鞅可以由一串零均值独立随机变量相加而得，如果 $X_0=0$，那么鞅差序列 $\{X_n - X_{n-1}, n \geq 1\}$是互不相关的（而不是独立的）。在此基础上有 Azuma 不等式：&lt;/p>
&lt;p>如果 $| X_i - X_{i-1} | \leq c_i$，那么成立：&lt;/p>
&lt;p>$$
P(|X_n - X_0 | \geq a) \leq 2e^{-a^2 / (2 \sum_i c_i^2)}\\
$$&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/><category scheme="https://allenz-me.github.io/categories/%E9%AB%98%E7%BB%B4%E6%A6%82%E7%8E%87/" term="高维概率" label="高维概率"/><category scheme="https://allenz-me.github.io/tags/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/" term="正态分布" label="正态分布"/><category scheme="https://allenz-me.github.io/tags/%E6%AC%A1%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/" term="次高斯分布" label="次高斯分布"/></entry><entry><title type="text">Random vectors in high dimensions</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/hdp3/"/><id>https://allenz-me.github.io/posts/analysis/hdp3/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-01-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">本书第三章主要讲的是高维空间中的随机向量。 第二章介绍了随机变量的集中程度，自然我们会……</summary><content type="html">&lt;!-- # Random vectors in high dimensions -->
&lt;p>本书第三章主要讲的是高维空间中的随机向量。&lt;/p>
&lt;p>第二章介绍了随机变量的集中程度，自然我们会进一步关注随机向量的集中程度。本章研究的主要内容就是维数非常高的随机向量 $X = (X_1, X_2, \dots, X_n) \in \mathbb{R}^n$。数据科学中高维分布无处不在！&lt;/p>
&lt;h2 id="concentration-of-the-norm">Concentration of the norm&lt;/h2>
&lt;p>高维的随机向量 $X$ 的样本是欧式空间中的一个点，因此我们会用&lt;strong>到原点的欧式距离$\|X \|_2$来刻画它的集中程度&lt;/strong>。&lt;/p>
&lt;p>首先从最简单的随机向量考虑起，如果 $X=(X_1, X_2, ..., X_n)$ 各个分量都是独立零均值且单位方差的随机变量，则：&lt;/p>
&lt;p>$$
\mathbb{E}\|X\|_{2}^{2}=\mathbb{E} \sum_{i=1}^{n} X_{i}^{2}=\sum_{i=1}^{n} \mathbb{E} X_{i}^{2}=n \\
$$&lt;/p>
&lt;p>这说明大致上 $\lVert X \rVert_2 \sim \sqrt{n}$。 &lt;strong>如果 $X$ 的各个分量都是次高斯随机变量，那么这足以保证 $X$ 大致分布在以 $\sqrt{n}$ 为半径的球面附近了。&lt;/strong> 此时成立不等式：&lt;/p>
&lt;p>$$
\mathbb{P}\left\{\left|\|X\|_{2}-\sqrt{n}\right| \geq t\right\} \leq 2 \exp \left(-\frac{c t^{2}}{K^{4}}\right) \quad \text { for all } t \geq 0 \\
$$&lt;/p>
&lt;p>这意味着，$X$ 的样本点都集中在半径为 $\sqrt{n}$ 的球面附近！&lt;/p>
&lt;h2 id="isotropic-random-vectors">Isotropic random vectors&lt;/h2>
&lt;p>不失一般性，只考虑那些零均值的随机向量，这时候它们的协方差矩阵表示为 $\mathbb{E}XX^T$。&lt;/p>
&lt;p>协方差矩阵一定是一个半正定矩阵。高斯分布协方差矩阵的特征值和特征向量揭示了分布情况，在最大特征值对应的特征向量方向，分布得更加广泛一些。如果随机向量 $X$ 的协方差矩阵只有几个较大的特征值，而剩余的特征值都很小，说明该随机向量只在很小的几个维度包含信息。&lt;/p>
&lt;p>定义 &lt;strong>Isotropic random vectors&lt;/strong> 是那些零均值并且满足 $\Sigma=\mathbb{E}XX^T=\mathrm{I}_n$ 的随机向量。这种随机向量的各个分量之间没有相关性，但并不代表它们是独立的。对于一般的随机向量，可以用变换 $Z:=\Sigma^{-1/2}(X - \mu)$ 来标准化，这可以消除分量之间的相关性。&lt;/p>
&lt;p>各向同随机向量有一个等价的定义：
$$
X \text{ is isotropic} \Longleftrightarrow \mathbb{E}\langle X, x \rangle ^2 = \|x \|_2^2 \;\; \text{ for all } x \in \mathbb{R}^n
$$
这说明，各向同随机向量投影到任何一个一维分量上都具有单位方差。&lt;/p>
&lt;p>各向同随机向量只是一种抽象的存在，但是它所具有的性质非常让人惊讶，而这些性质都可以落到更具体的分布之上。&lt;/p>
&lt;p>首先，是 $\mathbb{E}\| X\|_2^2=\operatorname{tr}(\mathbb{E}XX^T)=n$，这是一个显而易见的长度上的性质。&lt;/p>
&lt;p>接着，&lt;strong>设 $X, Y$ 都是各向同随机向量，那么 $X, Y$ 的夹角，会呈现出随着维数 $n$ 的增加而趋于正交的特点&lt;/strong>。&lt;/p>
&lt;p>$$
\mathbb{E}\langle X, Y\rangle^{2}=\mathbb{E}_{Y} \mathbb{E}_{X}\left[\langle X, Y\rangle^{2} \mid Y\right] = \mathbb{E}\langle X, Y\rangle^{2}=\mathbb{E}_{Y}\|Y\|_{2}^{2} = n \\
$$&lt;/p>
&lt;p>所以 $|\langle X, Y \rangle| \sim \sqrt{n},\, \|X\| \sim \sqrt{n}, \|Y\| \sim \sqrt{n}$，即有：$| \langle X, Y \rangle| \sim \displaystyle\frac{1}{\sqrt{n}}$。如果$X_i \sim X,\; Y_i \sim Y$，这两个高维空间中的样本点，随着维数的增加，其夹角的余弦值是趋于0的。&lt;/p>
&lt;p>举个例子，当 $n$ 很大的时候，从 ${N}(0, \mathrm{I}_n)$ 中抽两个样本点，这两个点几乎是正交的。&lt;/p>
&lt;p>最后是关于两个各向同随机向量距离的性质，结合上述两点，联系勾股定理不难注意到: $\mathbb{E}\| X - Y \|_2^2 = 2n$。&lt;/p>
&lt;br>
&lt;p>接下来是各向同随机向量的例子：&lt;/p>
&lt;ul>
&lt;li>球面上的均匀分布：$X \sim \operatorname{Unif}\left(\sqrt{n} S^{n-1}\right)$&lt;/li>
&lt;/ul>
&lt;p>注意，该分布不包含球的内部。这是一个典型的，各分量不相关，但是不独立的随机向量。&lt;/p>
&lt;blockquote>
&lt;p>它各向同的证明要用到曲线积分。&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>
&lt;p>高维对称伯努利分布：$X \sim \operatorname{Unif}\left(\{-1,1\}^{n}\right)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>标准的多元正态分布：$g \sim N\left(0, \mathrm{I}_{n}\right)$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>正态分布的一大特点就是不相关即独立。&lt;strong>另一个重要性质是，任意的多元正态分布投影在某条直线上，得到的仍然是正态分布，这是后续定义次高斯随机向量的出发点。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Similarity of normal and spherical distributions&lt;/strong>&lt;/p>
&lt;p>虽然低维的标准正态分布，给我们的感觉是，集中在原点附近的球内，但是，随着维数的增加，它的次高斯性开始显现了，本文最开始提到，$g \sim N(0, \mathrm{I}_n)$ 在 $n$ 很大的时候集中在半径为 $\sqrt{n}$ 的球面上。&lt;/p>
&lt;p>$$
{N} (0, \mathrm{I}_n) \to \operatorname{Unif}\left(\sqrt{n} S^{n-1}\right)\: \text{ as }\: n \to \infty \\
$$&lt;/p>
&lt;img src="../../figures/hdp3/image-20220513162418407.png" alt="image-20220513162418407" style="zoom:50%;" />
&lt;p>如上图所示，左边表示低维的标准高斯分布，右边表示高维的标准高斯分布。&lt;/p>
&lt;blockquote>
&lt;p>也可以用另一种方式去得到&lt;/p>
&lt;p>$g \sim N(0, \mathrm{I}_n)$，则 $\|g\|_2^2 \sim \chi^2(n)$&lt;/p>
&lt;p>因为：$(\|g\|_2^2-n)/\sqrt{2n} \to N(0, 1)$，而当 $n$ 特别大的时候，标准差 $\sqrt{2n}$ 相对于均值 $n$ 来说非常小，所以 $\|g\|$ 可以看成是 $\sqrt{n}$ 的球壳上的均匀分布&lt;/p>
&lt;/blockquote>
&lt;p>另外两个例子，分别是用在信号处理和凸几何中的。一个是 &lt;em>frame&lt;/em>，它是非常稀疏的离散各向同分布。另一个讲的是，对于$\mathrm{R}^n$中每个内点非空的凸集$K$，都可以对应一个零均值随机向量$X$。如果$\Sigma = \mathbb{E} XX^T$，那么$Z \sim \operatorname{Unif}\left(\Sigma^{-1 / 2} K\right)$是一个各向同随机向量。线性变换$\Sigma ^ {-1/2}$可以把凸集$K$的形状变得更加规整。&lt;/p>
&lt;h2 id="sub-gaussian-distributions-in-higher-dimensions">Sub-gaussian distributions in higher dimensions&lt;/h2>
&lt;p>次高斯分布，本质是正态分布集中性质的推广，它的定义思想，来自正态分布的一条重要性质：投影在任意一个方向上都是正态分布，并且分布被这些投影所决定！&lt;/p>
&lt;p>因此，&lt;strong>定义 $X$ 是次高斯随机向量，当且仅当对任意的 $x \in \mathrm{R}^n$，$\langle X, x \rangle$ 是次高斯随机变量&lt;/strong>，且定义其次高斯模为：&lt;/p>
&lt;p>$$
\|X\|_{\psi_{2}}=\sup _{x \in S^{n-1}}\|\langle X, x\rangle\|_{\psi_{2}} \\
$$&lt;/p>
&lt;p>如果随机向量 $X=(X_1, \dots, X_n) \in \mathbb{R}^n$ 的各个分量都是次高斯随机变量，那么它自然能成为次高斯随机向量，此时有 $\| X \|_{\psi_2} \leq C \max \|X_i\|_{\psi_2}$ 。分量是 sub-gaussian $\Rightarrow$ 整体是 sub-gaussian。&lt;/p>
&lt;p>对于任意的 $n$，可以找到常数 $C$，使得 $X \sim N(0, \mathrm{I}_n)$ 的次高斯模 $\|X \|_{\psi_2} \leq C$。&lt;/p>
&lt;h2 id="application-grothendiecks-inequality-and-semideﬁnite-programming">Application: Grothendieck’s inequality and semideﬁnite programming&lt;/h2>
&lt;h3 id="grothendiecks-inequality">Grothendieck’s inequality&lt;/h3>
&lt;p>令 $A = [a_{ij}] \in \mathbb{R}^{m \times n}$，$A$ 可以看做是 $(\mathbb{R}^m, \| \cdot \|_p)$ 到 $(\mathbb{R}^m, \| \cdot \|_q)$ 的一个线性算子，这里 $0 \leq p, q \leq\infty$，在这种意义下，$A$ 的 $p \to q$ 范数是：
$$
\| A\|_{p \to q} = \max_{\| x\|_p = 1} \|A x\|_q
$$
注意到 $\| A\|_{p \to q} \leq \|A\|_{\infty \to 1}$ 。&lt;/p>
&lt;p>一种计算 $\|A \|_{\infty \to 1}$ 的方法是解一个整数二次规划问题：
$$
\begin{aligned}
\max \;&amp;amp; \sum_{i, j} a_{i j} x_{i} y_{j} \\
\text {s.t. } &amp;amp; (x, y) \in\{-1,1\}^{m+n}
\end{aligned}
$$
它可以放松为以下的半定规划：
$$
\begin{aligned}
\max \;&amp;amp; \sum_{i, j} a_{i j}\langle x^{(i)}, y^{(j)}\rangle \\
\text{s.t. }\, &amp;amp; x^{(1)}, \ldots x^{(m)}, y^{(1)}, \ldots, y^{(n)} \text{ are unit vectors in } \left(\mathbb{R}^{d},\|\cdot\|_{2}\right)
\end{aligned}
$$
那么，一个随之而来的问题是，这样的近似效果能有多好？&lt;/p>
&lt;p>Grothendieck’s Inequality 回答了这个问题，存在常数 $C$ 使得对任意的 $A \in \mathbb{R}^{m \times n}$ 和任意的 Hilbert 空间 $H$，成立：
$$
\|A\|_{\infty \to 1} \leq \max _{\substack{\text { unit vectors } \\ x^{(i)}, y^{(j)} \in H}} \sum_{i, j} a_{i j}\langle x^{(i)}, y^{(j)}\rangle_{H} \leq C\|A\|_{\infty \rightarrow 1}
$$
Grothendieck’s constant 指的就是最小的满足上式的常数 $C$，其具体的值仍然是一个开放问题，但它大致介于 $\pi/2 \approx 1.57$ 到 $\pi/(2\ln (1+\sqrt2)) \approx 1.78$ 之间。&lt;/p>
&lt;p>&lt;strong>Symmetric matrices&lt;/strong>&lt;/p>
&lt;p>设 $A$ 是一个对称的半正定矩阵，如果对于任何 $x_i \in \{-1, 1\}$，都有：
$$
\left| \sum_{i,j} a_{ij} x_i x_j \right| \leq 1
$$
则对任何 Hilbert 空间 $H$ 和任意单位向量 $u_i, v_j \in H$，成立：
$$
\left| \sum_{i, j} a_{ij} \langle u_i, v_j \rangle \right| \leq 2C
$$&lt;/p>
&lt;p>$C$ 是 Grothendieck’s constant。&lt;/p>
&lt;h4 id="grothendiecks-identity">Grothendieck’s identity&lt;/h4>
&lt;p>令 $x$ 与 $y$ 是 $(\mathbb{R}^d, \|\cdot \|_2) \; (d \geq 2)$ 上的单位向量，$z$ 是单位球上随机一点，则：
$$
\mathbb{E}[\operatorname{sign}(\langle x, z\rangle) \operatorname{sign}(\langle y, z\rangle)]=\frac{2}{\pi} \arcsin (\langle x, y\rangle)
$$
其中 $\operatorname{sign}(x) \in \{-1, 1\}$ 。&lt;/p>
&lt;p>实际上，给定 $x$ 和 $y$，问题本质上是分析随机给出一个超平面，$x, y$ 位于超平面同侧或是异侧的概率。&lt;/p>
&lt;p>设 $\theta$ 是 $x, y$ 的夹角（锐角），则：
$$
\begin{aligned}
\mathbb{E}[\operatorname{sign}(\langle x, z\rangle) \operatorname{sign}(\langle y, z\rangle)] &amp;amp; = \mathbb{P}(x, y\text{ lie in same half}) − \mathbb{P}(x, y \text{ lie in different halves}) \\
&amp;amp; = 1 - 2 \mathbb{P}(x, y \text{ lie in different halves}) \\
&amp;amp; = 1 - \frac{2\theta}{\pi} \\
&amp;amp; = \frac{2}{\pi} \arcsin (\langle x, y \rangle )
\end{aligned}
$$
&lt;strong>modiﬁed version of Grothendieck’s Identity&lt;/strong>&lt;/p>
&lt;p>如果 $g \sim N(0, \mathrm{I}_n)$，$u, v$ 是单位向量，则：
$$
\mathbb{E} [\operatorname{sign}\langle g, u\rangle \operatorname{sign}\langle g, v\rangle] =\frac{2}{\pi} \arcsin \langle u, v\rangle
$$&lt;/p>
&lt;h3 id="semidefinite-relaxation">Semidefinite relaxation&lt;/h3>
&lt;p>对于如下 NP-hard 的优化问题：
$$
\begin{aligned}
\max \; &amp;amp; x^T A x \\
\text{s.t. } &amp;amp; x_i \in \{-1, 1\} \;\text{ for } i = 1, \dots, n
\end{aligned} \tag{1}
$$
它有等价的形式：
$$
\begin{aligned}
\max \; &amp;amp; \langle A, xx^T\rangle \\
\text{s.t. } &amp;amp; x_i \in \{-1, 1\} \;\text{ for } i = 1, \dots, n
\end{aligned}
$$
用矩阵变量 $X$ 替代 $xx^T$，$x_i=\pm 1$ 推出 $X$ 的对角线元素都是1，原问题进一步的转换形式是：
$$
\begin{aligned}
\max \; &amp;amp; \langle A, X\rangle \\
\text{s.t. } &amp;amp; X_{ii}=1 \;\text{ for } i = 1, \dots, n \\
&amp;amp; \text{rank}(X) = 1
\end{aligned}
$$
$X$ 是一个秩一的半正定矩阵。记 $\mathrm{I}_i$ 表示对角线第 $i$ 个元素为1、其它元素为0的矩阵，则 $X_{ii}= 1 \Leftrightarrow \langle \mathrm{I}_i, X \rangle = 1$，于是得到原问题的 semidefinite form 的表达：
$$
\begin{aligned}
\max \; &amp;amp; \langle A, X\rangle \\
\text{s.t. } &amp;amp; \langle \mathrm{I}_i, X\rangle = 1 \;\text{ for } i = 1, \dots, n \\
&amp;amp; X \in \mathrm{S}^n_{+}, \; \text{rank}(X)=1
\end{aligned}
$$
上式与 (1) 是完全等价的。将秩为一这个条件去除，就可以得到原问题的半定松弛问题：
$$
\begin{aligned}
\max \; &amp;amp; \langle A, X\rangle \\
\text{s.t. } &amp;amp; \langle \mathrm{I}_i, X\rangle = 1 \;\text{ for } i = 1, \dots, n \\
&amp;amp; X \in \mathrm{S}^n_{+}
\end{aligned} \tag{2}
$$
$X$ 是半正定矩阵，&lt;strong>因此它可以看成是一组基向量的 Gram 矩阵&lt;/strong>，设这组基为 $\{x_1, x_2, \dots, x_n\}$，则 $X_{ii}=1$ 意味着 $\|x_i\|_2 = 1$，则式(2)还有等价的形式：
$$
\begin{aligned}
\max \; &amp;amp; \sum_{i, j = 1}^n a_{ij} \langle x_i, x_j \rangle \\
\text{s.t. } &amp;amp; \|x_i \|_2 = 1 \;\text{ for } i = 1, \dots, n
\end{aligned}
$$&lt;/p>
&lt;h3 id="guarantee-of-relaxation">Guarantee of relaxation&lt;/h3>
&lt;p>给定一个半正定矩阵 $A$，用 $\text{INT}(A)$ 表示整数规划问题 (1) 的最优值，$\text{SDP}(A)$ 表示半定松弛问题 (3) 的最优值，则：
$$
\text{INT}(A) \leq \text{SDP}(A) \leq 2 C \cdot \text{INT}(A)
$$
其中 $C$ 是 Grothendieck’s constant。可以暂时认为 $C \approx 1.78$&lt;/p>
&lt;h2 id="application-maximum-cut-for-graphs">Application: Maximum cut for graphs&lt;/h2>
&lt;p>对于无向图 $G=(V, E)$，将节点分成两部分，连接这两部分的边就称为一个割 (cut)。图 $G$ 的最大割，记为 $\text{MAX-CUT}(G)$ ，是割的最大值。计算最大割是 NP-hard 的。&lt;/p>
&lt;p>设 $A$ 是 $G$ 的邻接矩阵 (adjacency matrix)，用 $x_i \in\{-1, 1\}, i = 1, \dots, n$ 标记节点的类别，则图 $G$ 的割为：
$$
\text{CUT}(G, x) = \frac{1}{4} \sum_{i, j= 1}^n A_{ij} (1 - x_i x_j)
$$
最大割就是：
$$
\operatorname{MAX}-\operatorname{CUT}(G)=\frac{1}{4} \max \left\{\sum_{i, j=1}^{n} A_{i j}\left(1-x_{i} x_{j}\right): x_{i}=\pm 1 \text { for all } i\right\} \text {. }
$$&lt;/p>
&lt;p>&lt;strong>0.5-approximation&lt;/strong>&lt;/p>
&lt;p>对 $x$ 取期望，得：
$$
\mathbb{E} \operatorname{CUT}(G, x)=\frac{1}{4} \sum_{i, j=1}^{n} A_{i j}=\frac{1}{2}|E|
$$&lt;/p>
&lt;h2 id="kernel-trick-and-tightening-of-grothendiecks-inequality">Kernel trick, and tightening of Grothendieck’s inequality&lt;/h2>
&lt;h3 id="tensor">Tensor&lt;/h3>
&lt;p>张量是多维数组，定义张量 $A=[a_{i_1i_2...i_k}]^{n_1 \times n_2 \times \cdots\times n_k}$ 与 $B=[b_{i_1i_2...i_k}]^{n_1 \times n_2 \times \cdots\times n_k}$ 的内积为：
$$
\langle A, B \rangle = \sum_{i_1, \dots, i_k} a_{i_1 \dots i_k} b_{i_1 \dots i_k}
$$
这个定义与通常的矩阵内积和向量内积都是相容的。&lt;/p>
&lt;p>对向量 $u \in \mathbb{R}^n$，定义其 $k$ 阶张量积为：
$$
u^{\otimes k} = [u_{i_1} u_{i_2} \cdots u_{i_k}] \in \mathbb{R}^{n \times n \times \cdots \times n }
$$
当 $k=2$ 时，$u \otimes u = uu^T \in \mathbb{R}^{n \times n}$。&lt;/p>
&lt;p>注意到 $\langle u^{\otimes k}, v^{\otimes k} \rangle = \langle u, v\rangle^k $&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/><category scheme="https://allenz-me.github.io/categories/%E9%AB%98%E7%BB%B4%E6%A6%82%E7%8E%87/" term="高维概率" label="高维概率"/><category scheme="https://allenz-me.github.io/tags/%E9%9A%8F%E6%9C%BA%E5%90%91%E9%87%8F/" term="随机向量" label="随机向量"/><category scheme="https://allenz-me.github.io/tags/%E6%AC%A1%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/" term="次高斯分布" label="次高斯分布"/></entry><entry><title type="text">Random matrices</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/hdp4/"/><id>https://allenz-me.github.io/posts/analysis/hdp4/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-01-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">本书第四章的研究对象是随机矩阵。 Preliminaries on matrices 这一部分回顾了算子范数和奇异值分解的内容。 Singular value……</summary><content type="html">&lt;p>本书第四章的研究对象是随机矩阵。&lt;/p>
&lt;h2 id="preliminaries-on-matrices">Preliminaries on matrices&lt;/h2>
&lt;p>这一部分回顾了算子范数和奇异值分解的内容。&lt;/p>
&lt;h3 id="singular-value-decomposition">Singular value decomposition&lt;/h3>
&lt;p>对于 $m \times n$ 的矩阵 $A$，它的 singular value decomposition (SVD) 是：
$$
A=\sum_{i=1}^{r} s_{i} u_{i} v_{i}^{\top} = U\Sigma V^T, \quad \text {where} \;\; r=\operatorname{rank}(A)
$$
$u_i$ 是 $AA^T$ 的正交的特征向量，$v_i$ 是 $A^TA$ 的正交的特征向量；$s_{i}(A)=\sqrt{\lambda_{i}\left(A A^{\top}\right)}=\sqrt{\lambda_{i}\left(A^{\top} A\right)}$&lt;/p>
&lt;p>把奇异值从大到小排序，$s_1(A)$ 是 $A$ 最大的奇异值，$s_r(A)$ 是 $A$ 最小的非零奇异值。&lt;/p>
&lt;blockquote>
&lt;p>$\text{rank}(A) = \text{rank}(AA^T) = \text{rank}(A^T A)$&lt;/p>
&lt;p>$AA^T$ 同 $A^T A$ 有相同的非零特征值&lt;/p>
&lt;/blockquote>
&lt;h3 id="spectral-norm">spectral norm&lt;/h3>
&lt;h3 id="heading">&lt;/h3>
&lt;p>定义矩阵 $A, B \in \mathbb{R}^{m \times n}$ 的内积为
$$
\langle A, B \rangle = \operatorname{tr}(A^TB) = \sum_{i=1}^m \sum_{j=1}^n A_{ij}B_{ij}
$$
由内积诱导的范数叫做 Frobenius 范数：
$$
\|A\|_{F}=\sqrt{\langle A, A \rangle }=\left(\sum_{i=1}^{m} \sum_{j=1}^{n}\left|A_{i j}\right|^{2}\right)^{1 / 2}
$$
用奇异值来表示的话：
$$
\|A\|_{F}=\left(\sum_{i=1}^{r} s_{i}(A)^{2}\right)^{1 / 2} .
$$&lt;/p>
&lt;h3 id="low-rank-approximation">Low-rank approximation&lt;/h3>
&lt;h3 id="approximate-isometries">Approximate isometries&lt;/h3>
&lt;h2 id="nets-covering-numbers-and-packing-numbers">Nets, covering numbers and packing numbers&lt;/h2>
&lt;p>令 $(T, d)$ 是一个度量空间，$K \subset T, \epsilon &amp;gt; 0$&lt;/p>
&lt;p>$K$ 的一个 $\epsilon$-net 指的是子集 $N \subseteq K$，使得以 $N$ 中的点为圆心，$\epsilon$ 为半径的若干个圆能覆盖 $K$，也就是说：
$$
\forall x \in K, \;\;\; \exists x_{0} \in \mathcal{N} \quad \text{s.t.} \;\; d\left(x, x_{0}\right) \leq \varepsilon
$$
$K$ 的一个 $\epsilon$-separated 指的是子集 $P \subseteq K$，使得以 $P$ 中的点为圆心，$\epsilon/2$ 为半径的若干个圆不相交，也就是说：
$$
\forall x, y \in P \subseteq K \;\;\; \text{s.t.} \;\; d(x, y) &amp;gt; \epsilon
$$&lt;/p>
&lt;blockquote>
&lt;p>以上定义基于 $(T, d)$ 是一个赋范线性空间&lt;/p>
&lt;/blockquote>
&lt;p>一般地，选度量 $d$ 为欧式空间的二范数。&lt;/p>
&lt;p>基数最小的 $\epsilon$-net 的基数记为 $N(K, \epsilon)$，基数最大的 $\epsilon$-separated 的基数记为 $P(K, \epsilon)$；分别称作 covering number 和 packing number。&lt;/p>
&lt;p>&lt;strong>Equivalence of covering and packing numbers&lt;/strong>&lt;/p>
&lt;p>成立下述关系：
$$
P(K, 2\epsilon) \leq N(K, \epsilon) \leq P(K, \epsilon)
$$
&lt;strong>Covering numbers and volume&lt;/strong>&lt;/p>
&lt;p>记 $B_2^n$ 为 $n$ 为欧几里得球，则：
$$
\frac{|K|}{\left|\varepsilon B_{2}^{n}\right|} \leq {N}(K, \epsilon) \leq {P}(K, \epsilon) \leq \frac{\left|\left(K+(\varepsilon / 2) B_{2}^{n}\right)\right|}{\left|(\varepsilon / 2) B_{2}^{n}\right|}
$$
$| \cdot |$ 表示 volume。&lt;/p>
&lt;p>&lt;strong>Covering numbers of the Euclidean ball&lt;/strong>
$$
\left(\frac{1}{\epsilon}\right)^{n} \leq {N}\left(B_{2}^{n}, \epsilon\right) \leq\left(\frac{2}{\varepsilon}+1\right)^{n}
$$&lt;/p>
&lt;h3 id="hamming-distance">Hamming distance&lt;/h3>
&lt;p>令 $H = \{0, 1\}^n$ 表示所有0-1组成的序列，在其上定义度量：
$$
d_H(x, y) = \# \{i : x(i) \neq y(i)\}
$$
这构成了一个度量空间 $(H, d_H)$&lt;/p>
&lt;p>&lt;strong>Covering and packing numbers of the Hamming cube&lt;/strong>&lt;/p>
&lt;p>令 $K = \{0, 1\}^n, m \in [0, n]$ 则：
$$
\left. {2^{n}} \middle / {\displaystyle\sum_{k=0}^{m}\left(\begin{array}{c}
n \\
k
\end{array}\right)} \right. \leq \mathcal{N}\left(K, d_{H}, m\right) \leq \mathcal{P}\left(K, d_{H}, m\right) \leq \left. {2^{n}} \middle/ {\displaystyle\sum_{k=0}^{[m / 2\rfloor}\left(\begin{array}{l}
n \\
k
\end{array}\right)} \right.
$$&lt;/p>
&lt;h2 id="application-error-correcting-codes">Application: error correcting codes&lt;/h2>
&lt;h3 id="metric-entropy">Metric entropy&lt;/h3>
&lt;p>记 $(T, d)$ 是一个度量空间，$K \subset T$，令 $C(K, \epsilon)$ 为编码 $K$ 达到精度 $\epsilon$ 所需要的最少的 bit 数，则：
$$
\log _{2} {N}(K, \epsilon) \leq {C}(K, \epsilon) \leq \log _{2} {N}(K,\epsilon / 2)
$$&lt;/p>
&lt;p>&lt;strong>Error correcting code&lt;/strong>&lt;/p>
&lt;p>给定整数 $k, n, r$，两个映射：
$$
E: \{0, 1\}^k \to \{0, 1\}^n \quad \text{and} \quad D:\{0, 1\}^n \to \{0, 1\}^k
$$
称为能改正 $r$ 个 bit 错误的 encoding 和 decoding maps，如果
$$
D(y) = x \;\; \text{ for } y \text{ differs from } E(x) \text{ in at most } r \text{ bits}
$$&lt;/p>
&lt;blockquote>
&lt;p>例：如果 $x:= fill \; the \; glass$&lt;/p>
&lt;p>$r=2$，假设有 $y:=bill \; the \; class$ 是错误信息&lt;/p>
&lt;p>一种简单的做法是使用冗余(redundancy)&lt;/p>
&lt;p>如 $E(x):= fill \; the \; glass\;fill \; the \; glass\;fill \; the \; glass\;fill \; the \; glass\;fill \; the \; glass\;$&lt;/p>
&lt;p>采用多数规则，如果原始信息 $x$ 重复 $2r+1$ 次，那么至多可以抗击 $r$ 个字符的错误&lt;/p>
&lt;p>但是这样做的效率非常低，它用 $n=(2r+1)k$ 个字符去编码长度为 $k$ 的原始字符&lt;/p>
&lt;/blockquote>
&lt;p>$$
\log _{2} {P}\left(\{0,1\}^{n}, d_{H}, 2 r\right) \geq k
$$&lt;/p>
&lt;p>定义 $R:=k/n, \delta := r/n$，则：
$$
1 - f(2 \delta) \leq R \leq 1 - f(\delta) , \quad f(t) = t \log_2(e/t)
$$&lt;/p>
&lt;h3 id="application-community-detection-in-networks">Application: community detection in networks&lt;/h3>
&lt;h3 id="application-covariance-estimation-and-clustering">Application: covariance estimation and clustering&lt;/h3>
&lt;p>对于一个&lt;strong>零均值&lt;/strong>的次高斯随机向量 $X$，我们想通过样本 $(X_1, X_2, \dots, X_m)$ 来估计协方差矩阵 $\Sigma = \mathbb{E} XX^T$，要达到一定的精确度，$m$ 至少应为多大呢？&lt;/p>
&lt;p>样本协方差矩阵是：
$$
\Sigma_{m}=\frac{1}{m} \sum_{i=1}^{m} X_{i} X_{i}^{\top}
$$
因为知道均值，所以分母是 $m$ 而不是 $m-1$；$\Sigma_m$ 是一个 unbiased estimator，即 $\mathbb{E}\, \Sigma_m = \Sigma$ .&lt;/p>
&lt;h4 id="covariance-estimation">Covariance estimation&lt;/h4>
&lt;p>令 $X$ 是一个次高斯随机向量，记存在 $K &amp;gt; 0$ 使得&lt;/p>
&lt;p>$$
\|\langle X, x\rangle\|_{\psi_{2}} \leq K\|\langle X, x\rangle\|_{L^{2}} \;\;\text { for any } x \in \mathbb{R}^{n}
$$&lt;/p>
&lt;p>则对任意的正整数 $m$，成立：&lt;/p>
&lt;p>$$
\mathbb{E}\left\|\Sigma_{m}-\Sigma\right\| \leq C K^{2}\left(\sqrt{\frac{n}{m}}+\frac{n}{m}\right)\|\Sigma\|
$$
这说明，要使平均相对误差小于 $\epsilon$
$$
\mathbb{E}\left\|\Sigma_{m}-\Sigma\right\| \leq \varepsilon\|\Sigma\|
$$
$m$ 的大小应该是
$$
m \asymp \varepsilon^{-2} n
$$&lt;/p>
&lt;h4 id="application-clustering-of-point-sets">Application: clustering of point sets&lt;/h4></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/><category scheme="https://allenz-me.github.io/categories/%E9%AB%98%E7%BB%B4%E6%A6%82%E7%8E%87/" term="高维概率" label="高维概率"/></entry><entry><title type="text">Concentration without independence</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/hdp5/"/><id>https://allenz-me.github.io/posts/analysis/hdp5/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-01-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Lipschitz functions 令 $(X, d_X)$ 和 $(Y, d_Y)$ 分别是度量空间，$f: X\to Y$ 是一个 Lipschitz 函数，如果： $$ d_Y(f(u), f(v)) \leq L \cdot d_X(u, v) \;\; \text{ for……</summary><content type="html">&lt;p>&lt;strong>Lipschitz functions&lt;/strong>&lt;/p>
&lt;p>令 $(X, d_X)$ 和 $(Y, d_Y)$ 分别是度量空间，$f: X\to Y$ 是一个 Lipschitz 函数，如果：
$$
d_Y(f(u), f(v)) \leq L \cdot d_X(u, v) \;\; \text{ for all } u, v \in X
$$
所有 $L$ 的下确界叫做 $f$ 的 Lipschitz norm (constant)，记作 $\|f\|_{\text{Lip}}$&lt;/p>
&lt;p>Concentration of Lipschitz functions on the sphere&lt;/p>
&lt;p>这一章首先证明了一个定理，记 $X \sim \text{Unif}(\sqrt{n} S^{n-1})$，如果 $f$ 是 $\sqrt{n} S^{n-1}$ 上的一个 Lipschitz 函数，那么 $f(X)$ 是集中在 $\mathbb{E} f(X)$ 附近的。即证明了：&lt;/p>
&lt;p>$$
\|f(X)-\mathbb{E} f(X)\|_{\psi_{2}} \leq C\|f\|_{\text {Lip }}
$$&lt;/p>
&lt;p>这说明：&lt;/p>
&lt;p>$$
\mathbb{P}\left(|f(X)-\mathbb{E} f(X)| \geq t\right) \leq 2 \exp \left(-\frac{c t^{2}}{\|f\|_{\text {Lip }}^{2}}\right)
$$&lt;/p>
&lt;p>在这个定理证明过程中，用到了高维空间一个非常反直觉的结果。&lt;/p>
&lt;h2 id="application-johnson-lindenstrauss-lemma">Application: Johnson-Lindenstrauss Lemma&lt;/h2>
&lt;h4 id="矩阵函数">矩阵函数&lt;/h4>
&lt;p>对于一个 $p$ 次的多项式函数
$$
f(x)=a_{0}+a_{1} x+\cdots+a_{p} x^{p}
$$
可以类似定义其矩阵函数：
$$
f(X)=a_{0} I+a_{1} X+\cdots+a_{p} X^{p}
$$
这里 $X$ 是一个方阵。&lt;/p>
&lt;p>比如，如果 $f$ 是 $X$ 的 minimal polynomial / characteristic polynomial，那么 $f(X) = 0$ .&lt;/p>
&lt;p>甚至我们可以定义幂级数的矩阵函数：&lt;/p>
&lt;p>$$
f(x) = \sum_{i=0}^\infty a_i x^i \;\; \Rightarrow \;\; f(X) = \sum_{i=0}^\infty a_i X^i
$$&lt;/p>
&lt;p>&lt;strong>如果 $f$ 的收敛半径为 $r$，且 $X$ 的最大特征值的模长小于 $r$，那么 $f(X)$ 收敛！&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>如果 $X$ 的 Jordan 分解是 $X=P^{-1}JP$，那么 $f(X) = P^{-1} f(J)P$&lt;/p>
&lt;p>$J$ 可以分解为若干 Jordon 块， $J = \operatorname{diag} \{J_1, J_2, \dots, J_k\} \Longrightarrow J^m = \text{diag}\{J_1^m, J_2^m, \dots, J_k^m\}$&lt;/p>
&lt;p>如果 $J_i$ 是 $r$ 阶方阵
$$
J_i = \begin{pmatrix}
\lambda_i &amp;amp; 1 &amp;amp; &amp;amp; \\
&amp;amp;\lambda_i &amp;amp; 1 &amp;amp; \\
&amp;amp;&amp;amp;\ddots &amp;amp; \ddots \\
&amp;amp;&amp;amp; &amp;amp; \lambda_i &amp;amp;1 \\
&amp;amp;&amp;amp;&amp;amp; &amp;amp; \lambda_i \\
\end{pmatrix}_{r \times r}
$$
则
$$
f(J_i) = \begin{pmatrix}
f(\lambda_i) &amp;amp; \displaystyle\frac{1}{1!} f^\prime (\lambda_i) &amp;amp; \displaystyle\frac{1}{2!} f^{(2)} (\lambda_i) &amp;amp; \cdots &amp;amp; \displaystyle\frac{1}{(r-1)!} f^{(r-1)} (\lambda_i) \\
&amp;amp; f(\lambda_i) &amp;amp; \displaystyle\frac{1}{1!} f^\prime (\lambda_i) &amp;amp; \cdots &amp;amp; \displaystyle\frac{1}{(r-2)!} f^{(r-2)} (\lambda_i) \\
&amp;amp; &amp;amp; f(\lambda_i) &amp;amp; \cdots &amp;amp; \displaystyle\frac{1}{(r-3)!} f^{(r-3)} (\lambda_i) \\
&amp;amp;&amp;amp;&amp;amp; \ddots &amp;amp; \vdots \\
&amp;amp;&amp;amp;&amp;amp;&amp;amp; f(\lambda_i)
\end{pmatrix}
$$
所以
$$
f(X) = f(P^{-1} J P) = P^{-1}f(J) P = P^{-1} \,\text{diag}\{f(J_1) , f(J_2), \dots, f(J_k)\} \, P
$$&lt;/p>
&lt;p>上面这个式子也可以用来定义矩阵函数！&lt;/p>
&lt;p>如果 $X$ 是正定矩阵，那么可以这样定义 $\log X$ .&lt;/p>
&lt;/blockquote>
&lt;p>由复分析知道：
$$
\begin{gather}
\mathrm{e}^{z}=1+\frac{1}{1 !} z+\frac{1}{2 !} z^{2}+\frac{1}{3 !} z^{3}+\cdots, \\
\sin z=z-\frac{1}{3 !} z^{3}+\frac{1}{5 !} z^{5}-\frac{1}{7 !} z^{7}+\cdots, \\
\cos z=1-\frac{1}{2 !} z^{2}+\frac{1}{4 !} z^{4}-\frac{1}{6 !} z^{6}+\cdots,
\end{gather}
$$
这3个级数在整个复平面上收敛，于是对一切方阵，定义
$$
\begin{gather}
e^X = I + \frac{1}{1!} X + \frac{1}{2!} X^2 + \frac{1}{3!} X^3 + \cdots \\
\sin X = X - \frac{1}{3!} X^3 + \frac{1}{5!} X^5 -\frac{1}{7!}X^7 + \cdots \\
\cos X = I - \frac{1}{2!} X^2 + \frac{1}{4!} X^4 - \frac{1}{6!} X^6 + \cdots
\end{gather}
$$
若 $X$ 的特征值的模长都小于 1，也可定义
$$
\ln (I + X) = X - \frac{1}{2} X^2 + \frac{1}{3} X^3 - \frac{1}{4} X^4 + \cdots
$$
矩阵函数在分析数学中有重要的应用。&lt;/p>
&lt;p>矩阵函数的计算方法可能与数值函数有区别，如果 $XY=YX$，那么 $e^X \cdot e^Y = e^{X+Y}$；但是 $e^X \cdot e^Y = e^{X+Y}$ 并不必然成立。&lt;/p>
&lt;blockquote>
&lt;p>如对 $A= \begin{pmatrix} 0 &amp;amp; 0 \\ 1 &amp;amp; 1 \\ \end{pmatrix}, B = \begin{pmatrix} 1 &amp;amp; 1 \\ 0 &amp;amp; 0 \\ \end{pmatrix}$，可验证 $e^A \cdot e^B \neq e^{A + B}$ .&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Golden-Thompson inequality&lt;/strong>&lt;/p>
&lt;p>对于对称矩阵 $A, B$ 来说
$$
\operatorname{tr} (e^{A+B} ) \leq \operatorname{tr} (e^Ae^B )
$$
注意：$\operatorname{tr}\left(e^{A+B+C}\right) \leq \operatorname{tr}\left(e^{A} e^{B} e^{C}\right)$ 一般不成立。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/><category scheme="https://allenz-me.github.io/categories/%E9%AB%98%E7%BB%B4%E6%A6%82%E7%8E%87/" term="高维概率" label="高维概率"/></entry><entry><title type="text">纳什均衡与相关均衡</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/operations/nash-corr/"/><id>https://allenz-me.github.io/posts/operations/nash-corr/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-01-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">策略 博弈论中的策略有纯策略和混合策略之分。但是它们可以有统一的数学逻辑。 参与人集合$……</summary><content type="html">&lt;!-- #! https://zhuanlan.zhihu.com/p/367426959
# 纳什均衡与相关均衡 -->
&lt;h2 id="策略">策略&lt;/h2>
&lt;p>博弈论中的策略有纯策略和混合策略之分。但是它们可以有统一的数学逻辑。&lt;/p>
&lt;p>参与人集合$N$，每个人$i\in N$都有一个行动集合$A_i$，记$A=\times_{i=1}^N A_i$是行动集合的笛卡尔积，决策时，参与者$i$从$A_i$中选出一个$a_i$，所有人的决策构成一个行动组合$a=(a_j)_{j\in N}$。&lt;/p>
&lt;p>定义$\Delta (A_i)$表示$A_i$上所有概率分布的集合，则纯策略和混合策略都是$\Delta (A_i)$内的元素。&lt;/p>
&lt;p>我们可以策略看成是一个概率向量($\mathbf{1}^Tp=1$)，则纯策略就是一个“退化”的混合策略了。&lt;/p>
&lt;p>对任一 $a_{-i} \in A_{-i}$ 定义 $B_{i}\left(a_{-i}\right)$ 为参与人 $i$ 在给定 $a_{-i}$ 下最佳行动集合（best response）。&lt;/p>
&lt;p>对于全体参与者，可以定义集值函数$B=\times_{i=1}^N B_i: A\to A$。对纳什均衡，就是：$a^{\ast} \in B\left(a^{\ast}\right)$。翻译过来就是，每个玩家的策略，都是对其他玩家策略的最佳反应。&lt;/p>
&lt;blockquote>
&lt;p>集值函数，其实就是一个多值函数，这时候我们把它的值域看成是一个集类。&lt;/p>
&lt;/blockquote>
&lt;p>因此，纳什均衡的存在性，即等价于集值映射$B$是否存在不动点。&lt;/p>
&lt;h2 id="纳什均衡">纳什均衡&lt;/h2>
&lt;p>最简单的纳什均衡（Nash equilibrium, NE）是纯策略纳什均衡，如果所有的参与者都有&lt;strong>无限策略&lt;/strong>，那么纯策略纳什均衡的存在性是可以保证的。&lt;/p>
&lt;p>Nash定理，作为一个非常著名的定理，说的是任何一个有限策略博弈都存在混合策略纳什均衡。&lt;/p>
&lt;p>除去大家都熟知的纳什均衡，还有其它均衡的概念。&lt;/p>
&lt;h2 id="强纳什均衡">强纳什均衡&lt;/h2>
&lt;p>顾名思义，强纳什均衡（strong Nash equilibrium, SNE）就是比纳什均衡更强的概念，它指的是，在纳什均衡的基础上，没有子联盟愿意改变自己的策略以换来每个人更高的收益。&lt;/p>
&lt;p>囚徒困境存在一个纯策略纳什均衡，但它不是强纳什均衡，因为这两名囚徒可以组成“子联盟”来提高每个人的收益。&lt;/p>
&lt;p>强纳什均衡往往在多人博弈中被研究，比如稳定婚姻问题：考虑$n$个男人和$n$个女人，每个人对异性都有严格的偏好，他们配对以后，如果没有一组夫妻愿意交换他们的伴侣，那么这一组婚姻就被认为是稳定的。&lt;/p>
&lt;p>通俗点说，如果形成了强纳什均衡，就不会存在你和隔壁老王互相觉得对方的老婆更漂亮的情况。&lt;/p>
&lt;h2 id="相关均衡">相关均衡&lt;/h2>
&lt;p>相关均衡（correlated Equilibrium, CE）是一个比纳什均衡更弱的概念。数学一点说，相关均衡就是行动集$A$上的一个概率分布。当然，这个概率分布还需要满足一些条件。&lt;/p>
&lt;p>先解释$A$上的概率分布这件事情，如下是一个对称博弈：&lt;/p>
&lt;p>$$
\begin{array}{|c|c|c|}\hline &amp;amp; d &amp;amp; c \\\hline d &amp;amp; 0,0 &amp;amp; 4,1 \\\hline c &amp;amp; 1,4 &amp;amp; 3,3 \\\hline\end{array}\\\\
$$&lt;/p>
&lt;p>显然，$(1, 4)$和$(4, 1)$是博弈的两个纯策略纳什均衡，此外，每个玩家以$\frac{1}{2}$的概率选$c$或$d$是一个混合策略纳什均衡。&lt;/p>
&lt;p>假如说这时候有一个上帝，对这两个玩家说，你们两个人的行动，一共有4种情况，你们按如下的概率分布去进行吧：&lt;/p>
&lt;p>$$
\begin{array}{|c|c|c|}\hline &amp;amp; d &amp;amp; c \\\hline d &amp;amp; 0 &amp;amp; 3 / 8 \\\hline c &amp;amp; 3 / 8 &amp;amp; 1 / 4 \\\hline\end{array}\\\\
$$&lt;/p>
&lt;p>上帝以相应的概率从$(c, d), (c, c), (d, c)$中选择一个，然后分别建议这两个玩家按照自己的指示去做抉择。&lt;/p>
&lt;p>这时候，两个玩家会遵从上帝的指示吗？&lt;/p>
&lt;p>我们先站在玩家一（row player）的立场上去看一下，“假如玩家二（column player）是听话的，如果上帝让我选$c$，这时候玩家二选$c, d$的概率分别是$\frac{3}{5}, \frac{2}{5}$，我选$c$的期望得益是$\frac{9}{5}$，选$d$的期望得益是$\frac{8}{5}$，所以我会选择听从上帝的指示。又如果上帝让我选$d$，这时候玩家二必然选$c$，我选择$d$的得益是$4$，选$c$的得益是$3$，所以我也会听从。”&lt;/p>
&lt;p>所以，如果有这样一个比较有权威的第三方能够给出一些合理的意见，就可以规避混合策略纳什均衡中出现$(d, d)$双方得益都为0这种情况，从而提高整个系统的收益情况。&lt;/p>
&lt;p>值得注意的是，纳什均衡也是相关均衡，它可以看做是相关均衡的特殊情况。（混合策略）纳什均衡下每个玩家做出选择都是&lt;strong>独立&lt;/strong>的。&lt;/p>
&lt;p>对一个有$n$个玩家，每个玩家两种行为的博弈来说，它的纳什均衡涉及$2n$个数，但是相关均衡有$2^n$个数。&lt;/p>
&lt;p>从数学上说，相关均衡是行动集合$A$上的一个概率分布$\mathcal{D}$，使得对于任何的玩家$i$和行动$a^\ast_i$，都有：&lt;/p>
&lt;p>$$
\mathrm{E}_{a \sim \mathcal{D}}\left[u_{i}(a) \mid a_i\right] \geq \mathrm{E}_{a \sim \mathcal{D}}\left[u_{i}\left(a_{i}^{\ast}, a_{-i}\right) \mid a_{i}\right], \quad \forall i, a^\ast_i\\
$$&lt;/p>
&lt;h2 id="粗糙相关均衡">粗糙相关均衡&lt;/h2>
&lt;p>如果把对$a_i$取条件期望拿掉，就是粗糙相关均衡（coarse correlated equilibrium）：&lt;/p>
&lt;p>$$
\mathrm{E}_{a \sim \mathcal{D}}\left[u_{i}(a) \right] \geq \mathrm{E}_{a \sim \mathcal{D}}\left[u_{i}\left(a_{i}^{\ast}, a_{-i}\right) \right], \quad \forall i, a^\ast_i\\
$$&lt;/p>
&lt;p>相关均衡在收到上帝给的指令后，每个玩家会自发地听从指令。而粗糙相关均衡是知道概率分布之后，哪怕还没接收到一个具体的指令时就表示会服从这个指令。&lt;/p>
&lt;p>以下是粗糙相关均衡的一个示例：&lt;/p>
&lt;p>$$
\begin{array}{|c|c|c|c|}\hline &amp;amp; \mathrm{A} &amp;amp; \mathrm{B} &amp;amp; \mathrm{C} \\\mathrm{A} &amp;amp; (1,1) &amp;amp; (-1,-1) &amp;amp; (0,0) \\\mathrm{B} &amp;amp; (-1,-1) &amp;amp; (1,1) &amp;amp; (0,0) \\\mathrm{C} &amp;amp; (0,0) &amp;amp; (0,0) &amp;amp; (-1.1,-1.1) \\\hline\end{array}\\
$$&lt;/p>
&lt;p>这个博弈行动集上的一个概率分布如下：&lt;/p>
&lt;p>$$
\begin{array}{|c|c|c|c|}\hline &amp;amp; \mathrm{A} &amp;amp; \mathrm{B} &amp;amp; \mathrm{C} \\\mathrm{A} &amp;amp; 1 / 3 &amp;amp; &amp;amp; \\\mathrm{~B} &amp;amp; &amp;amp; 1 / 3 &amp;amp; \\\mathrm{C} &amp;amp; &amp;amp; &amp;amp; 1 / 3 \\\hline\end{array}\\
$$&lt;/p>
&lt;p>首先它不是一个相关均衡，因为上帝如果掷出了$(C, C)$，两个玩家的都会选择不听从。&lt;/p>
&lt;p>但是在上帝随机做选择之前，如果玩家一决定服从上帝的协调，那么玩家二最优的选择也是服从协调。&lt;/p>
&lt;p>容易看出，粗糙相关均衡是一种比相关均衡更弱的概念。它与相关均衡的区别，可以这样理解：你聘请了一个理财顾问，每年都会给你规划资产配置的方案，你看过方案之后决定听从建议（相关均衡），或者没看方案就决定听从建议（粗糙相关均衡）。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>本文总结了博弈论中均衡的概念。从强到弱分别是 SNE &amp;gt; NE &amp;gt; CE &amp;gt; CCE。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E8%BF%90%E7%AD%B9%E4%B8%8E%E4%BC%98%E5%8C%96/" term="运筹与优化" label="运筹与优化"/><category scheme="https://allenz-me.github.io/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" term="博弈论" label="博弈论"/></entry><entry><title type="text">随机个随机变量的和的性质</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/double-random/"/><id>https://allenz-me.github.io/posts/analysis/double-random/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2021-01-01T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">设 $Y_1, Y_2, ...$ 是独立同分布且二阶矩有限的随机变量，$N$ 是一个与 $Y$ 独立且取值为正整数的随机……</summary><content type="html">&lt;!-- #! https://zhuanlan.zhihu.com/p/421364730
# 随机个随机变量的和的性质 -->
&lt;p>设 $Y_1, Y_2, ...$ 是独立同分布且二阶矩有限的随机变量，$N$ 是一个与 $Y$ 独立且取值为正整数的随机变量。令：&lt;/p>
&lt;p>$$
X = Y_1 + Y_2 + ... + Y_N
$$&lt;/p>
&lt;p>这里 $X$ 是随机个独立同分布随机变量的和。那么，$X$ 的方差和期望该如何计算呢？&lt;/p>
&lt;p>首先，由条件期望公式，我们知道 $E[X] = E[E[X|N]]$，在给定 $N$ 时，$E[X|N] = N E[Y]$，所以：&lt;/p>
&lt;p>$$
E[X] = E[N] \cdot E[Y] \\
$$&lt;/p>
&lt;p>计算 $X$ 的方差，需要借助条件方差公式；条件方差，跟条件期望一样，它也是一个随机变量，其定义如下：&lt;/p>
&lt;p>$$
var(X|Y) = E[(X - E[X|Y])^2|Y] = E[X^2|Y] - E^2[X|Y] \\
$$&lt;/p>
&lt;p>条件方差公式如下：&lt;/p>
&lt;p>$$
var(X) = E[var(X|Y)] + var(E[X|Y])
$$&lt;/p>
&lt;p>这个公式把右边展开一下就能证了。&lt;/p>
&lt;p>应用条件方差公式:&lt;/p>
&lt;p>$$
var(X) = E[var(X|N)] + var(E[X|N])
$$&lt;/p>
&lt;p>其中：
$$
E[var(X|N)]=E[var(Y) \cdot N]=var(Y)\cdot E[N] \\
var(E[X|N]) = var(N\cdot E[Y]) = var(N) \cdot E^2[Y] \\
$$&lt;/p>
&lt;p>于是：&lt;/p>
&lt;p>$$
var(X) = var(Y)\cdot E[N] + var(N) \cdot E^2[Y] \\
$$&lt;/p>
&lt;p>现在问题来了，$X$ 的高阶矩怎么计算呢？&lt;/p>
&lt;p>高阶矩没有类似的公式（我不知道），所以，这个计算可能需要我们回归高阶矩计算的内在方法————矩母函数。&lt;/p>
&lt;p>$$
m_X(t) = E[e^X] = E[E[e^X|N]] = E[(m_Y(t))^N] \\
$$&lt;/p>
&lt;p>对 $m_X(t)$ 求导，期望相当于积分，因此应用积分求导公式有：&lt;/p>
&lt;p>$$
m^\prime_X(t) = E[N(m_Y(t))^{N-1}m_Y^\prime (t)]\\
$$&lt;/p>
&lt;p>计算在 $t=0$ 处的值，即有：$E[X] = E[N]E[Y]$。高阶矩可以类似计算，但注意要加上条件：$N, Y$ 的高阶矩也是存在的。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/><category scheme="https://allenz-me.github.io/tags/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/" term="随机变量" label="随机变量"/><category scheme="https://allenz-me.github.io/tags/%E7%9F%A9%E6%AF%8D%E5%87%BD%E6%95%B0/" term="矩母函数" label="矩母函数"/></entry><entry><title type="text">Julia 的函数</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/julia-function/"/><id>https://allenz-me.github.io/posts/coding/julia-function/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2020-10-12T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">函数定义 Julia定义函数的方式非常灵活。标准的定义方式类似于MATLAB： 1 2 3 function……</summary><content type="html">&lt;h2 id="函数定义">函数定义&lt;/h2>
&lt;p>Julia定义函数的方式非常灵活。标准的定义方式类似于MATLAB：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="k">function&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="c"># 等价于 return x + y&lt;/span>
&lt;span class="k">end&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Julia默认把最后一行的内容作为返回值&lt;/strong>，但如果有&lt;code>return&lt;/code>句，则会立刻返回&lt;code>return&lt;/code>语句后面的值。&lt;/p>
&lt;p>此外，也可以用一行来定义简单的函数：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">y&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>此外，由于Julia无缝支持各类unicode，还可以使用特殊字符：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">Σ&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="c"># Σ = \Sigma + &amp;lt;Tab&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>函数的调用方法非常直接：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c"># 5&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以用符号&lt;code>::&lt;/code>来指定返回值类型，程序会再返回时做一个类型转换。（如果转换失败，则会报错）&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="k">function&lt;/span> &lt;span class="n">g&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">::&lt;/span>&lt;span class="kt">Int8&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">y&lt;/span>
&lt;span class="k">end&lt;/span>
&lt;span class="n">Σ&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">::&lt;/span>&lt;span class="kt">Int8&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">y&lt;/span>
&lt;span class="n">typeof&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Σ&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="c"># Int8&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>如果不希望函数有返回值，那么就&lt;code>return nothing&lt;/code>，这里跟Python的&lt;code>return None&lt;/code>比较类似，&lt;code>return&lt;/code>后的&lt;code>nothing&lt;/code>是可以被省略的。值得注意的是，&lt;code>nothing&lt;/code>是一个单例。&lt;/p>
&lt;h2 id="匿名函数">匿名函数&lt;/h2>
&lt;p>在Julia中函数是一等公民，即意味着函数可以作为另一个函数的参数。&lt;/p>
&lt;p>可以通过以下方式定义一个匿名函数：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">x&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">^&lt;/span>&lt;span class="mi">2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="n">map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">^&lt;/span>&lt;span class="mi">2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>匿名函数适合在&lt;code>map&lt;/code>等函数中使用。&lt;/p>
&lt;p>同样，匿名函数也可以有多个参数：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">z&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="函数的多个返回值">函数的多个返回值&lt;/h2>
&lt;p>&lt;strong>Julia使用&lt;code>Tuple&lt;/code>这一数据结构来实现返回多个值的功能。&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;hello&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">6&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="mi">7&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">typeof&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c"># Tuple{Float64,String,Int64}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>一个tuple可以有多种数据类型。跟Python一样，元组并不需要显式地用括号括起来。&lt;/p>
&lt;p>Julia还支持具名元组：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">a&lt;/span> &lt;span class="c"># 2&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>想要让一个函数返回多个值，那么就用一个元组来包含它们就好了：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="k">function&lt;/span> &lt;span class="n">foo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">a&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">b&lt;/span>
&lt;span class="k">end&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Julia对元组有着类似Python的支持，比如：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">foo&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>暂时没看到有讲Julia支持序列解包的。&lt;/p>
&lt;/blockquote>
&lt;p>元组可以作为返回值，也可以作为函数的参数。&lt;/p>
&lt;p>因为元组的存在，所以Julia支持简介的变量交换写法：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="函数的参数">函数的参数&lt;/h2>
&lt;p>Julia支持变长参数：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">bar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">bar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c"># (1, 2, (3,))&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>函数&lt;code>bar&lt;/code>第三个及以后的参数会变成一个元组。&lt;/p>
&lt;p>同时，Julia支持把元组中的元素逐个作为函数参数：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c"># x 也可以是其它序列类型&lt;/span>
&lt;span class="n">bar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c"># (1, 2, (3, 4))&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Julia还支持默认值参数和关键词参数：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">y&lt;/span>
&lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="c"># 参数y必须以关键词参数的形式传递，; 类比python的 *&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="函数的复合">函数的复合&lt;/h2>
&lt;p>在Julia中函数的复合有着非常优美的形式！&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">a&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;span class="n">b&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;span class="n">a&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="c"># 16&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们可以使用上面这个方式来计算函数的复合，但是为了得到复合函数的指针，我们可以使用：&lt;code>f ∘ g&lt;/code>，其中&lt;code>∘&lt;/code>符号用&lt;code>\circ&lt;/code>+&lt;code>&amp;lt;Tab&amp;gt;&lt;/code>打出。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">a&lt;/span> &lt;span class="o">∘&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>此外，Julia还有类似shell中管道的功能，可以&lt;strong>把一个函数的输出作为另一个函数的输入&lt;/strong>，使用符号&lt;code>|&amp;gt;&lt;/code>，如果要广播，那么用&lt;code>.|&amp;gt;&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="mi">1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="mi">10&lt;/span> &lt;span class="o">|&amp;gt;&lt;/span> &lt;span class="n">sum&lt;/span> &lt;span class="o">|&amp;gt;&lt;/span> &lt;span class="n">sqrt&lt;/span> &lt;span class="c"># 等价于 (sqrt ∘ sum)(1:10)&lt;/span>
&lt;span class="c"># 7.416198487095663&lt;/span>
&lt;span class="p">[&lt;/span>&lt;span class="s">&amp;#34;a&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;list&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;of&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;strings&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">.|&amp;gt;&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">uppercase&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reverse&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">titlecase&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">length&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="函数的向量化">函数的向量化&lt;/h2>
&lt;p>把一个单值函数变成一个可以作用在数组上的函数，只需要在调用的时候加&lt;code>.&lt;/code>即可。（功能好比于被&lt;code>np.vectorize&lt;/code>修饰）&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;span class="n">A&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">3.0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">sin&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">f&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>同样的功能也可以借助强大的Julia Macro来实现：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">Y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">3.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">4.0&lt;/span>&lt;span class="p">];&lt;/span>
&lt;span class="nd">@.&lt;/span> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cos&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Y&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="c"># 把该行的所有函数向量化&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>f.(args...)&lt;/code>等价于&lt;code>broadcast(f, args...)&lt;/code>，进行广播运算：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">julia&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">julia&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">A&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">3.0&lt;/span>&lt;span class="p">];&lt;/span>
&lt;span class="n">julia&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mf">4.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">5.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">6.0&lt;/span>&lt;span class="p">];&lt;/span>
&lt;span class="n">julia&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">pi&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="mi">3&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">element&lt;/span> &lt;span class="kt">Vector&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="kt">Float64&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="o">:&lt;/span>
&lt;span class="mf">13.42477796076938&lt;/span>
&lt;span class="mf">17.42477796076938&lt;/span>
&lt;span class="mf">21.42477796076938&lt;/span>
&lt;span class="n">julia&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">B&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="mi">3&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">element&lt;/span> &lt;span class="kt">Vector&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="kt">Float64&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="o">:&lt;/span>
&lt;span class="mf">19.0&lt;/span>
&lt;span class="mf">26.0&lt;/span>
&lt;span class="mf">33.0&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这种向量化的函数会把运算融合起来，&lt;code>sin.(cos.(X))&lt;/code>只会有一个循环，而不是先计算&lt;code>cos(X)&lt;/code>再计算&lt;code>sin(X)&lt;/code>（这样就有两重循环并且有中间变量需要存储）。&lt;/p>
&lt;h2 id="变量的作用域">变量的作用域&lt;/h2>
&lt;p>Julia的函数，同其它语言一样，有自己的一个作用域。可以通过&lt;code>global&lt;/code>关键词来修改外部的作用域。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-julia" data-lang="julia">&lt;span class="n">a&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;span class="n">s&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;span class="k">function&lt;/span> &lt;span class="n">updateS&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">global&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="n">println&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span>
&lt;span class="k">end&lt;/span>
&lt;span class="n">updateS&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c"># 2&lt;/span>
&lt;span class="n">println&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c"># 1; 如果上面函数中没有global，输出的是0&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/tags/julia/" term="Julia" label="Julia"/></entry><entry><title type="text">张量积 Tensor product</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/tensor-prod/"/><id>https://allenz-me.github.io/posts/analysis/tensor-prod/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2020-07-07T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Tensor product of vectors 令 $x \in \mathbb{R}^m, y \in \mathbb{R}^n$，定义 $x, y$ 的张量积 $x \otimes y$ 为 $m \times n$ 的矩阵……</summary><content type="html">&lt;h3 id="tensor-product-of-vectors">Tensor product of vectors&lt;/h3>
&lt;p>令 $x \in \mathbb{R}^m, y \in \mathbb{R}^n$，定义 $x, y$ 的张量积 $x \otimes y$ 为 $m \times n$ 的矩阵 $(x \otimes y)_{ij} = x_iy_j$，即 $x \otimes y = x y^T$ .&lt;/p>
&lt;p>特别地，坐标向量的张量积 $e_i \otimes e_j \in \mathbb{R}^{m \times n}$ 是一个仅在第 $i$ 行第 $j$ 列为 1，其余为 0 的矩阵。&lt;/p>
&lt;p>令 $\mathcal{E}_m = \{e_i\}_{i=1}^m$ 是 $\mathbb{R}^m$ 的一组基，$\mathcal{E}_n = \{e_i\}_{i=1}^n$ 是 $\mathbb{R}^n$ 的一组基，则：
$$
\mathcal{E}_{m, n}=\left\{{e}_{i} \otimes {e}_{j}\right\}_{(i, j)=(0,0)}^{(M-1, N-1)}
$$
是 $\mathbb{R}^{m \times n}$ 的一组基，也就是说，它也是线性空间 $\mathcal{L}(\mathbb{R}^m, \mathbb{R}^n)$ 的基。&lt;/p>
&lt;p>映射 $F(x, y) = x \otimes y$ 是双线性的 (bi-linear)。&lt;/p>
&lt;h3 id="tensor-product-of-matrices">Tensor product of matrices&lt;/h3>
&lt;p>如果 $S \in \mathcal{L}(\mathbb{R}^m), T \in \mathcal{L}(\mathbb{R}^n)$，定义 $S \otimes T$ 是一个 $\mathcal{L}(\mathbb{R}^m, \mathbb{R}^n) \to \mathcal{L}(\mathbb{R}^m, \mathbb{R}^n)$ 的线性算子。&lt;/p>
&lt;p>确定一个线性算子只需要确定它在一组基下的像即可：&lt;/p>
&lt;p>$$
(S \otimes T) (e_i \otimes e_j) = (Se_i \otimes Te_j)
$$&lt;/p>
&lt;p>因为 $(S\otimes T)$ 是线性的，所以如果 $x \in \mathbb{R}^m, y \in \mathbb{R}^n$ ，则 $(S \otimes T) (x \otimes y) = (Sx )\otimes (Ty) = (Sx)(Ty)^T$&lt;/p>
&lt;h3 id="computation">Computation&lt;/h3>
&lt;p>如果 $X \in \mathbb{R}^{m \times n}$，则 $(S \otimes T)X = SXT^T$，注意到：
$$
\begin{aligned}
(S \otimes T)\left(\boldsymbol{e}_{i} \otimes \boldsymbol{e}_{j}\right) &amp;amp;=\left(S \boldsymbol{e}_{i}\right) \otimes\left(T \boldsymbol{e}_{j}\right) \\
&amp;amp;=\left(\operatorname{col}_{i}(S)\right) \otimes\left(\operatorname{col}_{j}(T)\right)=\operatorname{col}_{i}(S)\left(\operatorname{col}_{j}(T)\right)^{T} \\
&amp;amp;=\operatorname{col}_{i}(S) \operatorname{row}_{j}\left(T^{T}\right)=S\left(\boldsymbol{e}_{i} \otimes \boldsymbol{e}_{j}\right) T^{T}
\end{aligned}
$$
此外：
$$
\begin{aligned}
&amp;amp;(S \otimes I) X=S X \\
&amp;amp;(I \otimes T) X=X T^{T}=\left(T X^{T}\right)^{T}
\end{aligned}
$$
因为：
$$
\left(S_{1} \otimes T_{1}\right)\left(S_{2} \otimes T_{2}\right) X=S_{1}\left(S_{2} X T_{2}^{T}\right) T_{1}^{T}=\left(S_{1} S_{2}\right) X\left(T_{1} T_{2}\right)^{T}=\left(\left(S_{1} S_{2}\right) \otimes\left(T_{1} T_{2}\right)\right) X
$$
所以：$(S_1 \otimes T_1)(S_2 \otimes T_2) = (S_1S_2) \otimes (T_1T_2)$&lt;/p>
&lt;h3 id="change-of-bases-in-tensor-products">Change of bases in tensor products&lt;/h3>
&lt;p>如果 $\mathcal{B}_1 = \{v_i\}_{i=1}^m$ 是 $\mathbb{R}^m$ 的一组基，$\mathcal{B}_2 = \{w_j\}_{j=1}^n$ 是 $\mathbb{R}^n$ 的一组基，那么 $\{v_i \otimes w_j\}_{(i, j) = (1, 1)}^{(m, n)}$ 是 $\mathbb{R}^{m \times n}$ 的一组基，记为 $\mathcal{B}_1 \otimes \mathcal{B}_2$ 。&lt;/p>
&lt;p>克罗内克积是两个任意大小的矩阵间的运算，对于 $A_{m\times n}, B_{p\times q}$，其克罗内克积是一个 $mp\times nq$ 的矩阵：
$$
A \otimes B=\left[\begin{array}{ccc}a_{11} B &amp;amp; \cdots &amp;amp; a_{1 n} B \\ \vdots &amp;amp; \ddots &amp;amp; \vdots \\ a_{m 1} B &amp;amp; \cdots &amp;amp; a_{m n} B\end{array}\right]
$$&lt;/p>
&lt;p>&lt;strong>克罗内克积满足双线性性和结合律：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>$A \otimes(B+C)=A \otimes B+A \otimes C \quad(\text { if } B \text { and } C \text { have the same size })$&lt;/li>
&lt;li>$(A+B) \otimes C=A \otimes C+B \otimes C \quad(\text { if } A \text { and } B \text { have the same size })$&lt;/li>
&lt;li>$(k A) \otimes B=A \otimes(k B)=k(A \otimes B)$&lt;/li>
&lt;li>$(A \otimes B) \otimes C=A \otimes(B \otimes C)$&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>克罗内克积通常不满足交换律。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>混合乘积性质&lt;/strong>&lt;/p>
&lt;p>$(\mathbf{A} \otimes \mathbf{B})(\mathbf{C} \otimes \mathbf{D})=\mathbf{A} \mathbf{C} \otimes \mathbf{B} \mathbf{D}$&lt;/p>
&lt;p>&lt;strong>克罗内克和&lt;/strong>&lt;/p>
&lt;p>对 $A_{n\times n}, B_{m\times m}$，定义：&lt;/p>
&lt;p>$$
\mathbf{A} \oplus \mathbf{B}=\mathbf{A} \otimes \mathbf{I}_{m}+\mathbf{I}_{n} \otimes \mathbf{B}
$$&lt;/p>
&lt;p>&lt;strong>克罗内克积与迹和行列式&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>$\operatorname{tr}(\mathbf{A} \otimes \mathbf{B})=\operatorname{tr} \mathbf{A} \operatorname{tr} \mathbf{B} \quad$&lt;/li>
&lt;li>$\operatorname{det}(\mathbf{A} \otimes \mathbf{B})=(\operatorname{det} \mathbf{A})^{m}(\operatorname{det} \mathbf{B})^{n} \quad \mathrm{for}\;\; A_{n\times n}, B_{m\times m}$&lt;/li>
&lt;li>$\operatorname{rank}(\mathbf{A} \otimes \mathbf{B})=\operatorname{rank} \mathbf{A} \operatorname{rank} \mathbf{B}$&lt;/li>
&lt;/ul>
&lt;p>$vec(X)$ 表示矩阵 $X$ 的向量化，它是把 $X$ 的所有列堆起来所形成的列向量。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/></entry><entry><title type="text">路由那些事</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/routing/"/><id>https://allenz-me.github.io/posts/coding/routing/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2020-04-11T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">路由器根据路由表（Routing Table）转发数据包。路由控制分为静态路由和动态路……</summary><content type="html">&lt;p>路由器根据路由表（Routing Table）转发数据包。路由控制分为静态路由和动态路由。静态路由是管理员事先设定好路由信息的方式。动态路由需要路由器进行通信，再根据路由协议得到路由表。&lt;/p>
&lt;p>路由协议分为两大类&lt;/p>
&lt;ul>
&lt;li>&lt;strong>外部网关协议&lt;/strong>（Exterior Gateway Protocol）&lt;/li>
&lt;li>&lt;strong>内部网关协议&lt;/strong>（Interior Gateway Protocol）&lt;/li>
&lt;/ul>
&lt;p>EGP和IGP的关系类似于IP地址网络部分和主机部分的关系。根据EGP在区域网络之间进行路由选择，根据IGP在网络内部进行路由选择。&lt;/p>
&lt;p>最具代表性的路由算法有两种&lt;/p>
&lt;ul>
&lt;li>&lt;strong>距离向量&lt;/strong>（Distance-Vector）
路由器之间互换各自网络的方向及位置的信息，并以这些信息为基础制作路由表。这种方法处理简单，但容易发生路由循环。&lt;/li>
&lt;li>&lt;strong>链路状态&lt;/strong>（Link-State）
通过信息同步，路由器了解网络的整体连接状态/网络拓扑，并根据该图确定通往目标网络的路径。能够保证路由信息的准确性和最优性，但是处理复杂巨大的网络时要付出计算代价。&lt;/li>
&lt;/ul>
&lt;p>常见的内部网关协议有 RIP和 OSPF，外部网关协议有 BGP。&lt;/p>
&lt;h2 id="riprouting-information-protocol">RIP（Routing Information Protocol）&lt;/h2>
&lt;p>RIP广泛应用与LAN。路由器将信息每隔30s向周围的路由器进行广播，广播的内容是自己与某个网络的最短距离。（如果等到6次都没有收到某个路由器的信息，那么就断开与它的连接。）&lt;/p>
&lt;p>&lt;strong>RIP基于距离向量决定路径&lt;/strong>。距离的单位为“跳数”，即经过路由器的个数。如果一个网络有两条路径，那么就选择距离较短的。&lt;/p>
&lt;p>RIP协议存在一些问题，比如循环广播导致无限计数，难以处理环路中突发故障的情况。为此，RIP采用如下几种方法：&lt;/p>
&lt;ul>
&lt;li>最长距离不超过16&lt;/li>
&lt;li>水平分割（split horizon）：路由器不再把受到的路由消息返还给发送端。&lt;/li>
&lt;li>毒性逆转（poisoned reverse）：当网络中发生链路断开的情况，不是不再发送这个消息，二是将这个无法通信的消息传播出去。即，发生一个距离为16的消息。&lt;/li>
&lt;li>触发更新（triggered update）：当路由信息发生变化时，不等待30秒而是立即发送出去。&lt;/li>
&lt;/ul>
&lt;p>使用RIP协议，在一个具有众多环路的复杂的网络中，想要达到一个稳定的状态也是需要花一段时间的，并且，哪怕网络处在稳定的状态，还是要定期交换路由信息，一定程度上是对网络资源的浪费。&lt;/p>
&lt;h2 id="ospfopen-shortest-path-first">OSPF（Open Shortest Path First）&lt;/h2>
&lt;p>&lt;strong>OSPF是链路状态型路由协议&lt;/strong>。路由器之间交换链路状态生成网络拓扑信息，然后再根据拓扑信息生成路由控制表。支持子网掩码。&lt;/p>
&lt;p>RIP的路由选择，要求途中经过的路由器个数越少越好。与之相比，OSPF可以给每一条链路赋予一个权重（每条链路可以具有不同的带宽），并始终选择一个权重最小的路径。&lt;/p>
&lt;p>RIP的包，只有一种类型，一边确认是否连接了网络，一边传送网络信息。而在OSPF中，根据作用的不同可以分为5种：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:left">类型&lt;/th>
&lt;th style="text-align:left">功能&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:left">问候（HELLO）&lt;/td>
&lt;td style="text-align:left">确认向量路由器&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:left">数据库描述（Database Description）&lt;/td>
&lt;td style="text-align:left">链路状态数据库的摘要信息&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:left">链路状态请求（Link State Request）&lt;/td>
&lt;td style="text-align:left">请求从数据库中获取链路状态信息&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">4&lt;/td>
&lt;td style="text-align:left">链路状态更新（Link State Update）&lt;/td>
&lt;td style="text-align:left">更新链路状态数据库中的链路状态信息&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:left">链路状态确认应答（Link State Acknowlegment）&lt;/td>
&lt;td style="text-align:left">链路状态信息更新的确认应答&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>通过上述不同类型的包，OSPF不仅可以大大减少网络流量，还可以达到迅速更新路由信息的目的。&lt;/p>
&lt;p>当网络规模越来越大时，OSPF为了减少计算负荷，引入了区域的概念。区域将若干个网络和主机划分为一个小组，成为一个“自治系统”（AS）。每个区域设置若干个边界路由器与外界相连。&lt;/p>
&lt;h2 id="bgpborder-gateway-protocol">BGP（Border Gateway Protocol）&lt;/h2>
&lt;p>BGP 是连接不同组织机构的一种协议，因此属于外部网关协议，主要用于连接不同ISP。&lt;/p>
&lt;p>ISP、区域网络等会将每个网络编配成一个个自治系统（AS）进行管理。每个自治系统拥有一个16比特的 AS 编号。有了 AS 编号的域，相当于一个“国家”。两个自治系统要想直接通信必须架设专线，不然就要通过其他 AS 进行中转。&lt;/p>
&lt;p>&lt;strong>BGP是路径向量协议&lt;/strong>。根据BGP交换路由信息的路由器叫做BGP扬声器。RIP和OSPF基于最短路径进行数据包转发，而BGP基于 AS 之间的合约进行数据包转发，一般选择 AS 数最少的路径，不过仍然要遵守各个 AS 之间的约定进行路由选择。&lt;/p>
&lt;h2 id="mplsmulti-protocol-label-switching">MPLS（Multi Protocol Label Switching）&lt;/h2>
&lt;p>MPLS是一种新技术，叫做标记交换，它对每个 IP 包做上标记，然后根据这个标记进行转发。&lt;/p>
&lt;p>标记的转发无法在传统的路由器上进行，在MPLS网络中实现标记交换功能的路由器叫做标记交换路由器（LSR， Label Switching Router），与外部网络连接的那部分 LSR 叫做标记边缘路由器（LER，Label Edge Router）。&lt;/p>
&lt;p>MPLS 会给每个数据包的 IP 首部追加32比特的标记，这个标记能决定数据转发的路径。&lt;/p>
&lt;p>MPLS 有着极高的转发速度，处理比传统的路由协议更加简单，可以通过高速的硬件实现转发。&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" term="计算机网络" label="计算机网络"/></entry><entry><title type="text">线段树—Python</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/segment-tree/"/><id>https://allenz-me.github.io/posts/coding/segment-tree/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2020-04-10T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">线段树是能够在 $O(\log n)$ 时间内完成查询数组区间和，以及修改数组某一处的值的数据结构。 本质上……</summary><content type="html">&lt;p>线段树是能够在 $O(\log n)$ 时间内完成查询数组区间和，以及修改数组某一处的值的数据结构。&lt;/p>
&lt;p>本质上是一棵树，因此可以用数组来表示（参考堆是怎么表示的）。&lt;/p>
&lt;p>用一个&lt;strong>长度是原数组长度4倍&lt;/strong>的新数组足矣。&lt;/p>
&lt;p>线段树的各个操作都是递归进行的。&lt;/p>
&lt;p>以下是线段树的一个 Python 实现&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">SegmentTree&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_nums&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nums&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_length&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_array&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">build&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">build&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">left&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">right&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">mid&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">build&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">build&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;将num的idx处的值修改为val&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">assert&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">idx&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_length&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">_update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">left&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">right&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">val&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">mid&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">idx&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">mid&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">query&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;查询数组[begin, end)的和&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">assert&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">begin&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">end&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_length&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_query&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">_query&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">begin&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">begin&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">left&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">end&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">mid&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">mid&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_query&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">begin&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">mid&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_query&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> \
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_query&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># end &amp;lt;= mid&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_query&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>以上刚好50行代码。
附上写过的一个单元测试：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">TreeTest&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unittest&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TestCase&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">testSegmentTree&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1000&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">nums&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">200&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1000&lt;/span>&lt;span class="p">))]&lt;/span>
&lt;span class="n">st&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">SegmentTree&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1000&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">update&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1000&lt;/span>&lt;span class="p">)]&lt;/span> &lt;span class="c1"># (idx, val)&lt;/span>
&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">update&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">update&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">update&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">update&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;span class="n">query&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">sorted&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">))])&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">query&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">query&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">continue&lt;/span>
&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">assertEqual&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">st&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">query&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]),&lt;/span> &lt;span class="nb">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">query&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]:&lt;/span>&lt;span class="n">query&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]]))&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91/" term="线段树" label="线段树"/><category scheme="https://allenz-me.github.io/tags/python/" term="Python" label="Python"/></entry><entry><title type="text">二叉树的非递归遍历</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/binary-tree-nonrecursive/"/><id>https://allenz-me.github.io/posts/coding/binary-tree-nonrecursive/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2020-04-08T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">前序遍历 前序遍历是最简单的，每弹出一个节点，就将该节点的右节点、左节点分别入栈。 1 2……</summary><content type="html">&lt;h2 id="前序遍历">前序遍历&lt;/h2>
&lt;p>前序遍历是最简单的，每弹出一个节点，就将该节点的右节点、左节点分别入栈。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">preorderTraversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">TreeNode&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">st&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">stack&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">cur&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">stack&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">stack&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="中序遍历">中序遍历&lt;/h2>
&lt;p>中序遍历，对二叉搜索树，其实就是从小到大依序输出。对每个节点，先一直向左走，向左走到尽头就弹出并向右走一步，继续上面的步骤。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">inorderTraversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">TreeNode&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[],&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">root&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">st&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="n">cur&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">cur&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">left&lt;/span>
&lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">st&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">right&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="后续遍历">后续遍历&lt;/h2>
&lt;p>后序遍历，利用 &lt;code>pre&lt;/code> 记录上一个访问过的结点，与当前结点比较，如果是当前结点的子节点，说明其左右结点均已访问，将当前结点出栈，更新 &lt;code>pre&lt;/code> 记录的对象。&lt;/p>
&lt;p>另一方面，它可以看成前序遍历“根、右、左”的逆序。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">postorderTraversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">TreeNode&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">root&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="n">pre&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">st&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">st&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">right&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="ow">or&lt;/span> \
&lt;span class="p">(&lt;/span>&lt;span class="n">pre&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">pre&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="n">pre&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">pre&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cur&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">right&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>以下为前序遍历逆序：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">preorderTraversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">TreeNode&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="n">st&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">st&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">stack&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">cur&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">stack&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">stack&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reverse&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" term="二叉树" label="二叉树"/><category scheme="https://allenz-me.github.io/tags/python/" term="Python" label="Python"/></entry><entry><title type="text">非负随机变量的特殊性质</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/analysis/nonnegative-rv/"/><id>https://allenz-me.github.io/posts/analysis/nonnegative-rv/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2020-03-23T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">非负的随机变量有一些特殊的性质。 首先，如果随机变量 $X$ 的期望存在，那么： $$ \mathrm{E}X=\int_0^{\infty}[1-F(x)]\mathrm{d}x-\int_{-\infty}^0F(x)\mathrm{d}x \tag{1} $$ $F(x)$ 是 $X$……</summary><content type="html">&lt;p>非负的随机变量有一些特殊的性质。&lt;/p>
&lt;p>首先，如果随机变量 $X$ 的期望存在，那么：&lt;/p>
&lt;p>$$
\mathrm{E}X=\int_0^{\infty}[1-F(x)]\mathrm{d}x-\int_{-\infty}^0F(x)\mathrm{d}x \tag{1}
$$&lt;/p>
&lt;p>$F(x)$ 是 $X$ 的分布函数。这个式子可以用 $F(x)$ 的曲线与坐标轴围成的面积来辅助记忆。
关于证明，需要用反常积分的理论证明&lt;/p>
&lt;p>$$
\lim_{x\to-\infty}xF(x)=0\; ;\lim_{x\to+\infty}x[1-F(x)]=0
$$&lt;/p>
&lt;p>如果 $X$ 是非负随机变量，那么直接就有&lt;/p>
&lt;p>$$
\mathrm{E}X=\int_0^{\infty}[1-F(x)]\mathrm{d}x
$$&lt;/p>
&lt;p>如果 $X$ 只取非负整数，那么很明显，根据上式，直接就有：&lt;/p>
&lt;p>$$
\mathrm{E}X=\sum_{n=1}^{+\infty}P(X\ge n)=\sum_{n=0}^{+\infty}P(X&amp;gt;n)
$$&lt;/p>
&lt;p>（ps： 如果 $X$ 取全体整数，通过式 (1) 可以很容易得到相似结论）&lt;/p>
&lt;p>一般地，对于随机变量 $X$，如果它的 $n$ 阶矩存在（即 $X\in L^n(p)}$ ）&lt;/p>
&lt;p>$$
\mathrm{E}X^n=\int_0^{\infty}nx^{n-1}[1-F(x)]\mathrm{d}x-\int_{-\infty}^0 nx^{n-1}F(x)\mathrm{d}x \tag{2}
$$&lt;/p>
&lt;p>类似的，如果 $X$ 只取非负整数，那么 (2) 式的第二项为 $0$，根据 $F(x)$ 是一个阶梯函数的情况：&lt;/p>
&lt;p>$$
\mathrm{E}X^n=\sum_{k=0}^{+\infty}[(k+1)^n-k^n]P(X&amp;gt;k)=\sum_{k=1}^{+\infty}[k^n-(k-1)^n]P(X&amp;gt;=k)
$$&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E7%8E%87/" term="分析与概率" label="分析与概率"/></entry><entry><title type="text">Java常用接口类的方法简介</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/java-interface/"/><id>https://allenz-me.github.io/posts/coding/java-interface/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2020-02-26T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">Collection 是 Java 中许多内置类的基类，它是存储“一堆”对象 object 的容器最抽象的表示；Collecti……</summary><content type="html">&lt;img src="../../figures/java-interface/v2-2f459861d963c59dddd56d7466da4381_1440w.jpg" alt="" style="zoom: 80%;" />
&lt;p>&lt;code>Collection&lt;/code> 是 Java 中许多内置类的基类，它是存储“一堆”对象 &lt;code>object&lt;/code> 的容器最抽象的表示；&lt;code>Collection&lt;/code> 接口 继承自 &lt;code>Iterable&lt;/code> 接口 ，提供了序列对象最基本的方法。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">interface&lt;/span> &lt;span class="nc">Collection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">Iterable&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="nf">size&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="nf">isEmpty&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="nf">contains&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Object&lt;/span> &lt;span class="n">o&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">Iterator&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">iterator&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">Object&lt;/span>&lt;span class="o">[]&lt;/span> &lt;span class="nf">toArray&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="o">[]&lt;/span> &lt;span class="nf">toArray&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="o">[]&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="nf">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="nf">remove&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Object&lt;/span> &lt;span class="n">o&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="nf">containsAll&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collection&lt;/span>&lt;span class="o">&amp;lt;?&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="nf">addAll&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collection&lt;/span>&lt;span class="o">&amp;lt;?&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="nf">removeAll&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collection&lt;/span>&lt;span class="o">&amp;lt;?&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="nf">retainAll&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collection&lt;/span>&lt;span class="o">&amp;lt;?&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">void&lt;/span> &lt;span class="nf">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="nf">equals&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Object&lt;/span> &lt;span class="n">o&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="nf">hashCode&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>中间返回 &lt;code>boolean&lt;/code> 类型的添加/删除操作，当改变这个序列的时候返回 &lt;code>true&lt;/code>，没有改变则返回&lt;code>false&lt;/code>。&lt;/p>
&lt;p>另外 &lt;code>Collection&lt;/code> 接口还有几个用 &lt;code>default&lt;/code> 关键字修饰的方法，这里不提。&lt;/p>
&lt;p>诸如 &lt;code>Stack&lt;/code>、&lt;code>ArrayList&lt;/code>、&lt;code>HashSet&lt;/code> 等类都实现了 &lt;code>Collection&lt;/code> 接口，因此这些类在使用的时候可以直接调用 &lt;code>Collection&lt;/code> 定义的接口方法。&lt;/p>
&lt;p>&lt;code>AbstactCollection&lt;/code> 是一个实现 &lt;code>Collection&lt;/code> 的抽象类，基于 &lt;code>add&lt;/code> 方法我们可以很容易地补全 &lt;code>addAll&lt;/code> 方法，有 &lt;code>remove&lt;/code> 方法那么 &lt;code>removeAll&lt;/code> 也就呼之欲出了，&lt;code>AbstractCollection&lt;/code> 正是做了这样一件事情。&lt;/p>
&lt;h3 id="线性表-list">线性表 &lt;code>List&lt;/code>&lt;/h3>
&lt;p>&lt;code>List&lt;/code> 接口继承自 &lt;code>Collection&lt;/code> 接口，除去 &lt;code>Collection&lt;/code> 定义的基本方法以外，还定义了线性表的几种基本操作：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">interface&lt;/span> &lt;span class="nc">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">Collection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">E&lt;/span> &lt;span class="nf">get&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">E&lt;/span> &lt;span class="nf">set&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// return the element previously at the specified position
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">E&lt;/span> &lt;span class="nf">remove&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="nf">indexOf&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Object&lt;/span> &lt;span class="n">o&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="nf">lastIndexOf&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Object&lt;/span> &lt;span class="n">o&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">subList&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">fromIndex&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">toIndex&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>除此之外，还定义了一个 &lt;code>default&lt;/code> 关键字修饰的 &lt;code>void sort()&lt;/code> 方法，所以对于 &lt;code>ArrayList&lt;/code> 和 &lt;code>LinkedList&lt;/code> 对象，可以直接调用 &lt;code>.sort()&lt;/code> 方法进行排序（可传入一个 &lt;code>Comparator&amp;lt;E&amp;gt;&lt;/code>） 。&lt;/p>
&lt;p>&lt;code>AbstractList&lt;/code> 抽象类 类似于 &lt;code>AbstractCollection&lt;/code>， 基于 &lt;code>add&lt;/code>、&lt;code>remove&lt;/code>、&lt;code>Iterator&lt;/code> 实现了 &lt;code>addAll&lt;/code>、&lt;code>removeAll&lt;/code>、&lt;code>toArray&lt;/code>、&lt;code>indexOf&lt;/code> 等方法。&lt;/p>
&lt;p>在Java中，&lt;code>ArrayList &lt;/code>和 &lt;code>LinkedList&lt;/code> 分别是线性表的 &lt;code>顺序表&lt;/code> 和 &lt;code>双链表&lt;/code> 实现。&lt;/p>
&lt;p>&lt;code>LinkedList&lt;/code> 除了是双链表以外，还有其他重要的功能。&lt;/p>
&lt;p>&lt;strong>先看 &lt;code>ArrayList&lt;/code>&lt;/strong>，它的初始化方式有三种&lt;/p>
&lt;ol>
&lt;li>不接收参数&lt;/li>
&lt;li>传入一个整数表示初始容器的容量&lt;/li>
&lt;li>传入一个 &lt;code>Collection&lt;/code> 对象进行拷贝（调用该对象的迭代器将各元素依次拷贝放入 &lt;code>List&lt;/code> 中）&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">ArrayList&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">AbstractList&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="kd">implements&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">RandomAccess&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Cloneable&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">java&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">Serializable&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">ArrayList&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">initialCapacity&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="c1">// 初始容量
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="kd">public&lt;/span> &lt;span class="nf">ArrayList&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">ArrayList&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collection&lt;/span>&lt;span class="o">&amp;lt;?&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="c1">// 拷贝别的容器
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>小tips: 可以用第三种初始化方法返回列表的浅拷贝哦。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>另外，从 &lt;code>Array&lt;/code> 创建 &lt;code>ArrayList&lt;/code> 可以这样子：&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">ArrayList&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ArrayList&lt;/span>&lt;span class="o">&amp;lt;&amp;gt;(&lt;/span>&lt;span class="n">Arrays&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">asList&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">2&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="n">ArrayList&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ArrayList&lt;/span>&lt;span class="o">&amp;lt;&amp;gt;(&lt;/span>&lt;span class="n">Arrays&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">asList&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;a&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;b&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;c&amp;#34;&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="c1">// Arrays.asList(new int[]{1, 2, 3}); 这样是不可以滴
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>ArrayList&lt;/code> 内部维护了一个数组 &lt;code>elementData&lt;/code>，每当容量达到上限时便进行扩容。&lt;/p>
&lt;p>同时，要注意，&lt;code>ArrayList&lt;/code> 是&lt;strong>线程不安全&lt;/strong>的 。&lt;/p>
&lt;h3 id="栈-stack">栈 &lt;code>Stack&lt;/code>&lt;/h3>
&lt;p>&lt;code>Stack&lt;/code> 类继承 &lt;code>Vector&lt;/code> 类，是一个线程安全的栈的实现。&lt;code>Vector&lt;/code> 类现在已经很少用到了。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">Stack&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">Vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">Stack&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{}&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="nf">push&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">item&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">synchronized&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="nf">pop&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">synchronized&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="nf">peek&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">boolean&lt;/span> &lt;span class="nf">empty&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">synchronized&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="nf">search&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Object&lt;/span> &lt;span class="n">o&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>官方建议如果有使用栈的需求的时候，可以实现了 &lt;code>Deque&lt;/code> 接口的 &lt;code>ArrayDeque&lt;/code> 作替代，因为 &lt;code>ArrayDeque&lt;/code> 能提供更多的功能。&lt;/p>
&lt;h3 id="队列-queue">队列 &lt;code>Queue&lt;/code>&lt;/h3>
&lt;p>注意，&lt;strong>&lt;code>Stack&lt;/code> 是一个类而 &lt;code>Queue&lt;/code> 只是一个接口&lt;/strong>。它定义了&lt;/p>
&lt;ul>
&lt;li>&lt;code>offer&lt;/code> 入队&lt;/li>
&lt;li>&lt;code>poll&lt;/code> 出队&lt;/li>
&lt;li>&lt;code>peek&lt;/code> 访问队首&lt;/li>
&lt;/ul>
&lt;p>事实上，队列完全可以使用双端队列去替代，通常我们会使用 &lt;code>LinkedList&lt;/code> 作为队列来使用。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">Queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">queue&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">LinkedList&lt;/span>&lt;span class="o">&amp;lt;&amp;gt;();&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="双端队列-deque">双端队列 &lt;code>Deque&lt;/code>&lt;/h3>
&lt;p>&lt;code>Deque&lt;/code> 也是一个接口，在 Java 中有两个常用的实现类：&lt;code>LinkedList&lt;/code>（基于双链表）和 &lt;code>ArrayDeque&lt;/code>（基于循环数组）。&lt;/p>
&lt;p>这两个实现类的数据结构不同，决定了它们的一些&lt;strong>差异&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;code>LinkedList&lt;/code> &lt;strong>访问队列中间的元素&lt;/strong>需要时间 $O(n)$ ，而 &lt;code>ArrayDeque&lt;/code>只需要 $O(1)$&lt;/li>
&lt;li>&lt;code>ArrayDeque&lt;/code> 内部维护了一个固定大小的数组，所以在添加元素的时候可能需要&lt;strong>扩容&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>先看 &lt;code>Deque&lt;/code> 接口定义的通用操作：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">interface&lt;/span> &lt;span class="nc">Deque&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">Queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kt">void&lt;/span> &lt;span class="nf">addFirst&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">addLast&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kt">boolean&lt;/span> &lt;span class="nf">offerFirst&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="kt">boolean&lt;/span> &lt;span class="nf">offerLast&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">E&lt;/span> &lt;span class="nf">removeFirst&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="nf">removeLast&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">E&lt;/span> &lt;span class="nf">pollFirst&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="nf">pollLast&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">E&lt;/span> &lt;span class="nf">getFirst&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="nf">getLast&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">E&lt;/span> &lt;span class="nf">peekFirst&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="nf">peekLast&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="c1">// 其它方法
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>乍一看 &lt;code>add&lt;/code> 方法和 &lt;code>offer&lt;/code> 方法作用相同、&lt;code>remove&lt;/code> 方法和 &lt;code>poll&lt;/code> 方法、&lt;code>get&lt;/code> 和 &lt;code>peek&lt;/code> 方法都很接近；它们之间的不同在于，用于 &lt;code>Queue&lt;/code> 接口的函数名类型（如 &lt;code>offer&lt;/code>、&lt;code>peek&lt;/code> 等），在数据为空时不会报错（返回 &lt;code>null&lt;/code>） 。&lt;/p>
&lt;p>&lt;code>Deque&lt;/code> 接口的方法加上 &lt;code>List&lt;/code> 接口的方法，就基本组成了 &lt;code>LinkedList&lt;/code> 的方法！&lt;/p>
&lt;p>再看 &lt;code>ArrayDeque&lt;/code>，其初始化方式类似于 &lt;code>ArrayList&lt;/code>：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">ArrayDeque&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">AbstractCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="kd">implements&lt;/span> &lt;span class="n">Deque&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">Cloneable&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Serializable&lt;/span>
&lt;span class="o">{&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">ArrayDeque&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">ArrayDeque&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">numElements&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// 数组的初始容量，默认 16
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="kd">public&lt;/span> &lt;span class="nf">ArrayDeque&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collection&lt;/span>&lt;span class="o">&amp;lt;?&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>然后是 &lt;code>LinkedList&lt;/code>：&lt;/p>
&lt;p>【未完待续】&lt;/p>
&lt;h3 id="优先队列-priorityqueue">优先队列 &lt;code>PriorityQueue&lt;/code>&lt;/h3>
&lt;p>优先队列的底层数据结构是堆，堆其实是用数组表示的一棵完全二叉树。当元素数量增加的时候，用于表示堆的数组也是需要动态扩容的。&lt;/p>
&lt;p>先看初始化方法，默认使用&lt;strong>小顶堆&lt;/strong>：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">PriorityQueue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">AbstractQueue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="kd">implements&lt;/span> &lt;span class="n">java&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">Serializable&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">PriorityQueue&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">PriorityQueue&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">initialCapacity&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// 不传入 Comparator 则会使用类的默认比较方法
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="kd">public&lt;/span> &lt;span class="nf">PriorityQueue&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Comparator&lt;/span>&lt;span class="o">&amp;lt;?&lt;/span> &lt;span class="kd">super&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">comparator&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">PriorityQueue&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">initialCapacity&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">Comparator&lt;/span>&lt;span class="o">&amp;lt;?&lt;/span> &lt;span class="kd">super&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">comparator&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">PriorityQueue&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collection&lt;/span>&lt;span class="o">&amp;lt;?&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">PriorityQueue&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">PriorityQueue&lt;/span>&lt;span class="o">&amp;lt;?&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">PriorityQueue&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">SortedSet&lt;/span>&lt;span class="o">&amp;lt;?&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>PriorityQueue&lt;/code> 完全支持队列的接口操作。使用的关键在于传入的 &lt;code>Comparator&lt;/code>，如果要使用最大堆的话，需要传入相应的 &lt;code>Comparator&lt;/code>（可以使用Java 8的lambda表达式）&lt;/p>
&lt;h3 id="集合-set">集合 &lt;code>Set&lt;/code>&lt;/h3>
&lt;p>&lt;code>Set&lt;/code> 接口继承了 &lt;code>Collection&lt;/code> 接口，并没有添加什么特殊的方法；一切与 &lt;code>Set&lt;/code> 相关的其它接口都直接或间接继承了 &lt;code>Set&lt;/code> 接口。&lt;/p>
&lt;p>&lt;code>TreeSet&lt;/code> 是基于平衡二叉查找树的集合类型，而 &lt;code>HashSet&lt;/code> 基于哈希表。还有一种可能听的少，叫做 &lt;code>LinkedHashSet&lt;/code>。&lt;/p>
&lt;p>&lt;code>HashSet&lt;/code> 的底层其实是 &lt;code>HashMap&lt;/code>，其初始化方式也跟 &lt;code>HashMap&lt;/code> 非常的相似！&lt;/p>
&lt;p>&lt;code>TreeSet&lt;/code> 是一个非常重要的数据结构，顾名思义，要认识 &lt;code>TreeSet&lt;/code>，必须先了解 Java 里面几个与集合紧密相关的几个接口！&lt;/p>
&lt;p>先是 &lt;code>SortedSet&lt;/code>，它实现了 &lt;code>Set&lt;/code> 接口，内部有一个 &lt;code>Comparator&amp;lt;E&amp;gt;&lt;/code>，提供以下几种常用方法：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">SortedSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">subSet&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">fromElement&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="n">toElement&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// 包含 fromElement, 不包含toElement
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">SortedSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">headSet&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">toElement&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// 包含所有严格小于 toElement 的元素
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">SortedSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">tailSet&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">fromElement&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// 包含所有大于等于 fromElement 的元素
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="nf">first&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="n">E&lt;/span> &lt;span class="nf">last&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="c1">// 返回最小、最大的元素
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>NavigableSet&lt;/code> 是 &lt;code>SortedSet&lt;/code> 的“升级版”！提供了更多与“比较”有关系的函数。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">E&lt;/span> &lt;span class="nf">lower&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// 返回最大的、严格小于 e 的元素，如果没有，返回 null
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="nf">floor&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// 返回最大的、小于等于 e 的元素，如果没有，返回 null
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="nf">ceiling&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// 返回最小的、大于等于 e 的元素，如果没有，返回 null
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="nf">higher&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// 返回最小的、严格大于 e 的元素，如果没有，返回 null
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="nf">pollFirst&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="c1">// 删除并返回最小的元素 (first)
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="nf">pollLast&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="c1">// 删除并返回最大的元素 (last)
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">NavigableSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">descendingSet&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="c1">// 返回一个相反的 NavigableSet (相反的 Comparator&amp;lt;E&amp;gt;)
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">Iterator&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">descendingIterator&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">NavigableSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">subSet&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">fromElement&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="kt">boolean&lt;/span> &lt;span class="n">fromInclusive&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">E&lt;/span> &lt;span class="n">toElement&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="kt">boolean&lt;/span> &lt;span class="n">toInclusive&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">NavigableSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">headSet&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">toElement&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="kt">boolean&lt;/span> &lt;span class="n">inclusive&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">NavigableSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">tailSet&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">E&lt;/span> &lt;span class="n">fromElement&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="kt">boolean&lt;/span> &lt;span class="n">inclusive&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="c1">// 以上的那几个函数与 SortedSet 的相关函数类似
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>TreeSet&lt;/code> 的实现基于 &lt;code>TreeMap&lt;/code>，有如下几种初始化方式：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">TreeSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">AbstractSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="kd">implements&lt;/span> &lt;span class="n">NavigableSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">Cloneable&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">java&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">Serializable&lt;/span>
&lt;span class="o">{&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">TreeSet&lt;/span>&lt;span class="o">();&lt;/span> &lt;span class="c1">// 使用自然顺序，插入的元素的超类必须实现 Comparable 接口
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="kd">public&lt;/span> &lt;span class="nf">TreeSet&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Comparator&lt;/span>&lt;span class="o">&amp;lt;?&lt;/span> &lt;span class="kd">super&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">comparator&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">TreeSet&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collection&lt;/span>&lt;span class="o">&amp;lt;?&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="nf">TreeSet&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">SortedSet&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">E&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>TreeSet&lt;/code> 的类方法大致就是 &lt;code>NavigableSet&lt;/code> 和 &lt;code>SortedSet&lt;/code> 的结合体。&lt;/p>
&lt;p>&lt;code>HashSet&lt;/code> 相对来说就没有那么多复制的操作了。默认的初始化容量是16，装填因子是0.75。&lt;/p>
&lt;h3 id="映射字典-map">映射/字典 &lt;code>Map&lt;/code>&lt;/h3>
&lt;p>&lt;code>Map&lt;/code> 类型是 Java 中非常重要的类型，类似于 &lt;code>Set&lt;/code>，&lt;code>TreeMap&lt;/code> 使用基于&lt;strong>键&lt;/strong>的平衡二叉查找树，而 &lt;code>HashMap&lt;/code> 使用哈希表。&lt;code>TreeMap&lt;/code>、&lt;code>HashMap&lt;/code> 都实现了&lt;code>AbstractMap&lt;/code> 抽象类，映射的实现，相当于 Python 中的 &lt;code>dict&lt;/code> 字典类型。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="n">V&lt;/span> &lt;span class="nf">get&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">object&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">containsKey&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">object&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">V&lt;/span> &lt;span class="nf">put&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">K&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="n">value&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">V&lt;/span> &lt;span class="nf">remove&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">object&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">containsValue&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">object&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">keySet&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="n">values&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="n">getOrDefault&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">object&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="n">defaultValue&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="n">putIfAbsent&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">K&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="n">value&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="n">entrySet&lt;/span>&lt;span class="o">()&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>HashMap&lt;/code> 的初始化方法有多种，常见的有这三种：&lt;/p>
&lt;ol>
&lt;li>无参数&lt;/li>
&lt;li>整数（表示初始容量）&lt;/li>
&lt;li>整数，浮点数（表示初始容量和装填因子）&lt;/li>
&lt;/ol>
&lt;p>【未完待续】&lt;/p></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/tags/java/" term="Java" label="Java"/></entry><entry><title type="text">Java 流的妙用</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/java-stream/"/><id>https://allenz-me.github.io/posts/coding/java-stream/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2019-11-15T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">求数组的最大值、最小值、和... 等 在Java中，基本类型流只支持 int、long、d……</summary><content type="html">&lt;ol>
&lt;li>求数组的最大值、最小值、和... 等&lt;/li>
&lt;/ol>
&lt;p>在Java中，基本类型流只支持 &lt;code>int、long、double&lt;/code> 三种类型。一是可以通过 &lt;code>IntSream.range&lt;/code> 方法创建；二是通过 &lt;code>Arrays.stream( )&lt;/code> 中传入一个基本类型的数组进行创建&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kt">int&lt;/span>&lt;span class="o">[]&lt;/span> &lt;span class="n">nums&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">{&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">2&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">3&lt;/span>&lt;span class="o">};&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="n">max&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Arrays&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">stream&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">max&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getAsInt&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="n">sum&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Arrays&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">stream&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">sum&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="c1">// 类似于python的reduce函数，提供一个二元运算符
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">reduce&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Arrays&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">stream&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">reduce&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">);&lt;/span> &lt;span class="c1">// 6，所有数之积
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">orelse&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Arrays&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">stream&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">reduce&lt;/span>&lt;span class="o">((&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">orElse&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="o">);&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>Arrays.stream(nums).max()&lt;/code> 返回的是一个 &lt;code>Optinal&amp;lt;int&amp;gt;&lt;/code> 对象，因为流可能是空的，所以 &lt;code>getAsInt&lt;/code> 方法可能会报错，因此 &lt;code>Optinal&amp;lt;int&amp;gt;&lt;/code> 对象提供了一个 &lt;code>orElse(int default)&lt;/code> 方法去处理这种情况。类似于 Python 的 &lt;code>max(nums, default=0)&lt;/code>。而流是空的时候 &lt;code>sum&lt;/code> 方法不会出现问题。&lt;/p>
&lt;p>&lt;strong>如果 &lt;code>Optional&lt;/code> 装载的泛型类不是基本类型，那么就通过 &lt;code>get&lt;/code> 方法去获得变量的值。&lt;/strong>&lt;/p>
&lt;ol start="2">
&lt;li>列表筛选&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">ss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ArrayList&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;(&lt;/span>&lt;span class="n">Arrays&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">asList&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;ab&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="s">&amp;#34;bb&amp;#34;&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="n">ss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ss&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">stream&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">filter&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">s&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">startsWith&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;a&amp;#34;&lt;/span>&lt;span class="o">)).&lt;/span>&lt;span class="na">collect&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collectors&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">toList&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Boolean&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">tt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ss&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">stream&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">map&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">::&lt;/span>&lt;span class="n">isEmpty&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">collect&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collectors&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">toList&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="n">ist&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">ll&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ss&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">stream&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">map&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">::&lt;/span>&lt;span class="n">length&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">collect&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Collectors&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">toList&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="c1">// collect() 方法接受stream数据并转换类型
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/tags/java/" term="Java" label="Java"/></entry><entry><title type="text">给定自然数 N，如何快速分解质因数？</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/primers/"/><id>https://allenz-me.github.io/posts/coding/primers/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2019-10-27T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">$120=2^3 \times 3 \times 5$ 如上，对一个数分解质因数就是分解成若干个质数的乘积，如果这个数是质数，那么……</summary><content type="html">&lt;blockquote>
&lt;p>$120=2^3 \times 3 \times 5$&lt;/p>
&lt;/blockquote>
&lt;p>如上，对一个数分解质因数就是分解成若干个质数的乘积，如果这个数是质数，那么对它分解质因数就是这个数本身。&lt;/p>
&lt;p>一个很朴素的想法是：&lt;/p>
&lt;p>对于从 $2\sim N$ 的每个自然数 $p$，去判断它是否是质数，如果 $p$ 整除 $N$，那么就可以求出 $p$ 在 $N$ 的质因数分解式里的指数&lt;/p>
&lt;p>一个非常简单是实现是：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">def&lt;/span> &lt;span class="nf">isprime&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kc">True&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kc">False&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kc">False&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kc">True&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">factors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">dict&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{}&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">isprime&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">continue&lt;/span>
&lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">k&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>
&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">factors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">120&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="c1"># {2: 3, 3: 1, 5: 1}&lt;/span>
&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">factors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">97&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="c1"># {97: 1}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>由于一个 &lt;code>int&lt;/code> 表示的整数一般不会有超过30个不同的质因数（并且该质因数的指数也一般不超过30，想想这是为什么），因而可以认为空间复杂度是 $O(1)$。&lt;/p>
&lt;p>接下来我们计算一下这种方法的时间复杂度。对于一个自然数 $p$，仅判断它是否是质数需要遍历从3到 $\sqrt p$的所有奇数，用时 $\sqrt p$ ，对于从 2 到 $N$ 的每个奇数都去判断一遍，共计 $O(n \sqrt n)$。&lt;/p>
&lt;p>显然这样做不是最优方法。因为一个数 $N$ 不可能有超过两个大于 $\sqrt N$ 的质因数！！！&lt;/p>
&lt;p>So, 我们只需要把 $N$ 在 $ \sqrt N$ 内做质因数分解，剩下还没有分解掉的就是剩下的那个质因数了。（想想这个数为什么一定是质数）&lt;/p>
&lt;p>还是贴python代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">def&lt;/span> &lt;span class="nf">isprime&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="c1"># 跟上面一样，判断是否是质数&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kc">True&lt;/span>
&lt;span class="k">elif&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kc">False&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kc">False&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="kc">True&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">factors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">dict&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{}&lt;/span>
&lt;span class="n">sqrt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="mf">0.5&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sqrt&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">isprime&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">continue&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="n">N&lt;/span> &lt;span class="o">//=&lt;/span> &lt;span class="n">i&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>However，细心的你可能会发现，好像根本不需要判断质数这一步&lt;/p>
&lt;p>因为有 &lt;code>while&lt;/code> 循环，所以结果中根本不可能分解出非质数（想想这又是为什么）&lt;/p>
&lt;p>所以代码可以简化到：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="k">def&lt;/span> &lt;span class="nf">factors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">dict&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{}&lt;/span>
&lt;span class="n">sqrt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="mf">0.5&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sqrt&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="n">N&lt;/span> &lt;span class="o">//=&lt;/span> &lt;span class="n">i&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>while&lt;/code> 循环总的执行次数是有限的，因此总的时间复杂度就是 &lt;code>for&lt;/code> 循环带来的 $O(\sqrt n)$ 。&lt;/p>
&lt;p>到这里，我也再想不出什么可以优化这个 &lt;code>for&lt;/code> 循环的方法了。&lt;/p>
&lt;p>最后贴一下C++的实现：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-cpp" data-lang="cpp">&lt;span class="cp">#include&lt;/span> &lt;span class="cpf">&amp;lt;iostream&amp;gt;&lt;/span>&lt;span class="cp">
&lt;/span>&lt;span class="cp">#include&lt;/span> &lt;span class="cpf">&amp;lt;vector&amp;gt;&lt;/span>&lt;span class="cp">
&lt;/span>&lt;span class="cp">#include&lt;/span> &lt;span class="cpf">&amp;lt;cmath&amp;gt;&lt;/span>&lt;span class="cp">
&lt;/span>&lt;span class="cp">#include&lt;/span> &lt;span class="cpf">&amp;lt;unordered_map&amp;gt;&lt;/span>&lt;span class="cp">
&lt;/span>&lt;span class="cp">&lt;/span>
&lt;span class="k">using&lt;/span> &lt;span class="k">namespace&lt;/span> &lt;span class="n">std&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">unordered_map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">factors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="n">unordered_map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">const&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">sq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">sq&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">N&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="nf">main&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="n">unordered_map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">factors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">120&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">auto&lt;/span> &lt;span class="n">it&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="n">it&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="n">it&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="n">cout&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="n">it&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">first&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="s">&amp;#34;: &amp;#34;&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="n">it&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">second&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="n">endl&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="c1">// 结果：
&lt;/span>&lt;span class="c1">// 5: 1
&lt;/span>&lt;span class="c1">// 3: 1
&lt;/span>&lt;span class="c1">// 2: 3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/></entry><entry><title type="text">Java BigInteger</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/java-bigint/"/><id>https://allenz-me.github.io/posts/coding/java-bigint/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2019-10-26T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">在 C++ 中，没有超过 long long 以上的大整数类了，而 Java 就有大整数类 普通的 int 类型能表示的整数的绝对……</summary><content type="html">&lt;p>在 C++ 中，没有超过 &lt;code>long long&lt;/code> 以上的大整数类了，而 Java 就有大整数类&lt;/p>
&lt;p>普通的 &lt;code>int&lt;/code> 类型能表示的整数的绝对值上限是$2^{31} \simeq 10^{(\log_{10}2) \times 31} \simeq 10^{0.3 \times 31}$，其数量级在$10^{9}$&lt;/p>
&lt;p>&lt;code>long&lt;/code> 类型是 $2^{63} \simeq 10^{0.3\times 63}$，数量级不超过 $10^{19}$&lt;/p>
&lt;blockquote>
&lt;p>比如对于Fibonacci数列，其增长速度是 $(\frac{1+\sqrt 5}{2})^n \simeq 1.618^n$，在 $n\ge \frac{19}{\log_{10}{1.618}}\simeq 91$ 时是无法用 &lt;code>long&lt;/code> 类型来保存的&lt;/p>
&lt;/blockquote>
&lt;p>由此可见，学习 Java 的大整数类 &lt;code>java.math.BigInteger&lt;/code> 是非常有必要的！&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kn">import&lt;/span> &lt;span class="nn">java.util.BigInteger&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">java.util.Scanner&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">main&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">[]&lt;/span> &lt;span class="n">args&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">Scanner&lt;/span> &lt;span class="n">in&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Scanner&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">System&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">in&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">bi&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">in&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">nextBigInteger&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">v0&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">BigInteger&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">valueOf&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">v1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">BigInteger&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;2&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="c1">// 主要就是这两种构造方法，推荐使用字符串构造
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="o">}&lt;/span>
&lt;span class="c1">// 下面是一些BigInteger的常用方法
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">abs&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="n">返回大整数的绝对值&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回两个大整数的和&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">and&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回两个大整数的按位与的结果&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">andNot&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回两个大整数与非的结果&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">divide&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回两个大整数的商&lt;/span>
&lt;span class="kt">double&lt;/span> &lt;span class="nf">doubleValue&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="n">返回大整数的double类型的值&lt;/span>
&lt;span class="kt">float&lt;/span> &lt;span class="nf">floatValue&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="n">返回大整数的float类型的值&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">gcd&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回大整数的最大公约数&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="nf">intValue&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="n">返回大整数的整型值&lt;/span>
&lt;span class="kt">long&lt;/span> &lt;span class="nf">longValue&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="n">返回大整数的long型值&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">max&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回两个大整数的最大者&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">min&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回两个大整数的最小者&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">mod&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">用当前大整数对val求模&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">multiply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回两个大整数的积&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">negate&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="n">返回当前大整数的相反数&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">not&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="n">返回当前大整数的非&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">or&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回两个大整数的按位或&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">pow&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">exponent&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回当前大整数的exponent次方&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">remainder&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回当前大整数除以val的余数&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">leftShift&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">将当前大整数左移n位后返回&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">rightShift&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">将当前大整数右移n位后返回&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">subtract&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span>&lt;span class="n">返回两个大整数相减的结果&lt;/span>
&lt;span class="kt">byte&lt;/span>&lt;span class="o">[]&lt;/span> &lt;span class="nf">toByteArray&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span>&lt;span class="n">将大整数转换成二进制反码保存在byte数组中&lt;/span>
&lt;span class="n">String&lt;/span> &lt;span class="nf">toString&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="n">将当前大整数转换成十进制的字符串形式&lt;/span>
&lt;span class="n">BigInteger&lt;/span> &lt;span class="nf">xor&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigInteger&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">返回两个大整数的异或&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/><category scheme="https://allenz-me.github.io/tags/java/" term="Java" label="Java"/></entry><entry><title type="text">给定自然数 N，如何求出所有约数？</title><link rel="alternate" type="text/html" href="https://allenz-me.github.io/posts/coding/none/"/><id>https://allenz-me.github.io/posts/coding/none/</id><updated>2022-07-05T20:26:54+08:00</updated><published>2019-10-23T00:00:00+00:00</published><author><uri>https://io-oi.me/</uri><email>allenz.me@qq.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights><summary type="html">问题：给定自然数 N，如何求出 N 的所有约数？ 如 N = 198，它的约数有：[1, 2, 3, 6, 9, 11,……</summary><content type="html">&lt;p>&lt;strong>问题：给定自然数 N，如何求出 N 的所有约数？&lt;/strong>&lt;/p>
&lt;p>如 N = 198，它的约数有：[1, 2, 3, 6, 9, 11, 18, 22, 33, 66, 99, 198]&lt;/p>
&lt;p>一个很自然的想法是：对从 2 到 $\displaystyle\frac{N}{2}$ 的所有数进行遍历，看这个数是否能整除N。这样的时间复杂度是 $O(N)$&lt;/p>
&lt;p>有没有更快一点的呢？&lt;/p>
&lt;p>我们可以这样考虑：如果有非平方数 $N=a\times b$，其中 $a&amp;lt;b$，那么一定有 $a&amp;lt;\sqrt N, b &amp;gt; \sqrt N$，对于每一个小于 $\sqrt N$ 的 $N$ 的约数 $a$，都有一个对应的 $b&amp;gt;\sqrt N$，$b$ 也是 $N$ 的约数，因此我们只需要遍历 2 到 $\sqrt N$ 就可以了，每一次整除都对应了 $N$ 的两个约数，最后只需要再考虑一下 $N$ 是不是平方数就可以了。这样，时间复杂度就被优化到了$\sqrt N$，是个非常大的进步了。&lt;/p>
&lt;p>下面贴代码&lt;/p>
&lt;p>Python：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">List&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">divisors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;span class="s1">&amp;#39;&amp;#39;&amp;#39;return all the factors of N&amp;#39;&amp;#39;&amp;#39;&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">sqrt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sqrt&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">sqrt&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>
&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">sorted&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">divisors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">198&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;span class="c1"># [1, 2, 3, 6, 9, 11, 18, 22, 33, 66, 99, 198]&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Java：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kn">import&lt;/span> &lt;span class="nn">java.util.ArrayList&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">java.util.Arrays&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">java.util.List&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">Divisors&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">divisors&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ArrayList&lt;/span>&lt;span class="o">&amp;lt;&amp;gt;();&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">1&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">final&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">sqrt&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">sqrt&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">2&lt;/span>&lt;span class="o">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">sqrt&lt;/span>&lt;span class="o">;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">++)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">0&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">sqrt&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sqrt&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">main&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">[]&lt;/span> &lt;span class="n">args&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">List&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">divisors&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">198&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="c1">// stream是Java 8的新语法
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">System&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">println&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Arrays&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">toString&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">res&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">stream&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">sorted&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">toArray&lt;/span>&lt;span class="o">()));&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>C++&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="cp">#include&lt;/span> &lt;span class="cpf">&amp;lt;iostream&amp;gt;&lt;/span>&lt;span class="cp">
&lt;/span>&lt;span class="cp">#include&lt;/span> &lt;span class="cpf">&amp;lt;vector&amp;gt;&lt;/span>&lt;span class="cp">
&lt;/span>&lt;span class="cp">#include&lt;/span> &lt;span class="cpf">&amp;lt;cmath&amp;gt;&lt;/span>&lt;span class="cp">
&lt;/span>&lt;span class="cp">#include&lt;/span> &lt;span class="cpf">&amp;lt;algorithm&amp;gt;&lt;/span>&lt;span class="cp">
&lt;/span>&lt;span class="cp">&lt;/span>
&lt;span class="k">using&lt;/span> &lt;span class="k">namespace&lt;/span> &lt;span class="n">std&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">divisors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">};&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="k">const&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">sq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">sq&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="n">res&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">sq&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sq&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sq&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="nf">main&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">res&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">divisors&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">198&lt;/span>&lt;span class="p">);&lt;/span>
&lt;span class="n">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">res&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">auto&lt;/span> &lt;span class="nl">n&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">res&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="n">cout&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="s">&amp;#34; &amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></content><category scheme="https://allenz-me.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" term="算法与程序设计" label="算法与程序设计"/></entry></feed>